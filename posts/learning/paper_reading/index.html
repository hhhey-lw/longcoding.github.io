<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>深度恶习论文汇总 | LongCoding&#39;s Blog</title>
<meta name="keywords" content="DL">
<meta name="description" content="图示
卷积注意力

自注意力

学习
Norm

Loss
Cross Entropy

$$
Loss = -\sum_{i}^{C}y_ilog(p(x_{i})), where \ y_i\ is label,p(x_i)\ is\ predict.
$$
BCE Loss
$$
Loss = −\sum_{i}^{c}(y_ilog(p(x_i)&#43;(1−y_i)log(1−p(x_i)) \
where \ y_i \in [0, 1] \">
<meta name="author" content="LongWei">
<link rel="canonical" href="http://localhost:1313/posts/learning/paper_reading/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.488b476fe12f895e41e7473bc23e9831609c74c2bdb368da996800ef083721e1.css" integrity="sha256-SItHb&#43;EviV5B50c7wj6YMWCcdMK9s2jamWgA7wg3IeE=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/learning/paper_reading/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel="stylesheet">
 <script type="text/javascript"
         async
         src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
 MathJax.Hub.Config({
   tex2jax: {
     inlineMath: [['$','$'], ['\\(','\\)']],
     displayMath: [['$$','$$'], ['\[\[','\]\]']],
     processEscapes: true,
     processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
<meta property="og:url" content="http://localhost:1313/posts/learning/paper_reading/">
  <meta property="og:site_name" content="LongCoding&#39;s Blog">
  <meta property="og:title" content="深度恶习论文汇总">
  <meta property="og:description" content="图示 卷积注意力 自注意力 学习 Norm Loss Cross Entropy $$ Loss = -\sum_{i}^{C}y_ilog(p(x_{i})), where \ y_i\ is label,p(x_i)\ is\ predict. $$
BCE Loss $$ Loss = −\sum_{i}^{c}(y_ilog(p(x_i)&#43;(1−y_i)log(1−p(x_i)) \ where \ y_i \in [0, 1] \">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-19T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-04-19T00:00:00+00:00">
    <meta property="article:tag" content="DL">
      <meta property="og:see_also" content="http://localhost:1313/posts/learning/common_commands/">
      <meta property="og:see_also" content="http://localhost:1313/posts/learning/minibaseline_learning/">
      <meta property="og:see_also" content="http://localhost:1313/posts/learning/java_learning/">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度恶习论文汇总">
<meta name="twitter:description" content="图示
卷积注意力

自注意力

学习
Norm

Loss
Cross Entropy

$$
Loss = -\sum_{i}^{C}y_ilog(p(x_{i})), where \ y_i\ is label,p(x_i)\ is\ predict.
$$
BCE Loss
$$
Loss = −\sum_{i}^{c}(y_ilog(p(x_i)&#43;(1−y_i)log(1−p(x_i)) \
where \ y_i \in [0, 1] \">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "深度恶习论文汇总",
      "item": "http://localhost:1313/posts/learning/paper_reading/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "深度恶习论文汇总",
  "name": "深度恶习论文汇总",
  "description": "图示 卷积注意力 自注意力 学习 Norm Loss Cross Entropy $$ Loss = -\\sum_{i}^{C}y_ilog(p(x_{i})), where \\ y_i\\ is label,p(x_i)\\ is\\ predict. $$\nBCE Loss $$ Loss = −\\sum_{i}^{c}(y_ilog(p(x_i)+(1−y_i)log(1−p(x_i)) \\ where \\ y_i \\in [0, 1] \\\n",
  "keywords": [
    "DL"
  ],
  "articleBody": "图示 卷积注意力 自注意力 学习 Norm Loss Cross Entropy $$ Loss = -\\sum_{i}^{C}y_ilog(p(x_{i})), where \\ y_i\\ is label,p(x_i)\\ is\\ predict. $$\nBCE Loss $$ Loss = −\\sum_{i}^{c}(y_ilog(p(x_i)+(1−y_i)log(1−p(x_i)) \\ where \\ y_i \\in [0, 1] \\\npos_partition = -log(p(x_i))\\ neg_partition = -log(1-p(x_i)) $$\nFocal Loss $$ Loss = -α_t(1-p_t)^γlog(p_t)\\\n# Multi-Label:\\ 1.\\ pos_loss = α(1-p(x_i))^γ\\ \\ -log(p_t)\\ 2.\\ neg_loss = (1-α)p(x_i)^γ\\ \\ -log(1-p_t) $$\n论文 Mlp-mixer - NIPS 2021 title: Mlp-mixer: An all-mlp architecture for vision\n⭐ channel-mixing MLPs and token-mixing MLPs.\n① 融合Patches间的特征信息\n② 融合Patches内的特征信息\n❌ 易过拟合\n**Transformer ** - NIPS 2017 Transformer Encoder\nMulti-Head Self Attention ⭐每段用不一样的α权重\t– 分段融合Token间信息\n​\t多组注意力权重α\nToken分段，并行计算每段的权重α，多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。\n⭐⭐⭐ 并行多个头，增强表示能力，融合不同子空间(低纬空间)\nToken间信息融合，比例是相似度权重\nFeed-Forward Networks 卷积 深度可分离卷积计算成本比标准卷积小 8 到 9 倍，而精度仅略有降低 - MobileNetV2\n✨使用共享内核在局部区域内进行信息交互\n标准卷积 输入特征图和模板卷积核\n输出像素 - 融合局部空间通道信息\n瓶颈结构 - Resnet 减少参数数量\n逐点卷积降维 -\u003e 标准卷积 -\u003e 逐点卷积升维\n​\t残差连接\n压缩 - 卷积 - 扩张\n深度可分离卷积 倒残差结构 - MobileNet 扩充通道的原因，是为了能够提取到更多的信息\nReLU 激活函数可能会崩溃掉某些通道信息。然而，如果我们有很多通道，那么信息可能仍然保留在其他通道中。\n逐点卷积升维 -\u003e 分组卷积 -\u003e 逐点卷积降维\n​\t残差连接\n扩张- 卷积 - 压缩\nLinear 作用等同于1×1逐点卷积，实现线性映射\nViT - CVPR 2020/10 Vision Transformer - Patching 化为互不重叠的区域，输出通道数为Token的长度。⭐ 将image拆分为16×16×3的patch\n经过Conv2D提取Token，# Token中每元素均有局部宽高及通道信息\n所有标记之间的盲目相似性比较\nPVT - ICCV 2021 Pyramid Vision Transformer A Versatile Backbone for Dense Prediction without Convolutions\n创新点：在ViT（patch embed大小固定）基础上，仿照CNN网络的特征图变化\n⭐使用“渐进”收缩策略通过补丁嵌入层来控制特征图的尺度\nCNNs: 特征图变化 通道×2，宽高÷2\nViT: patch embed不变\nPVT： 渐进式缩小特征图，减少token数量\nPVT框架：\n①特征图 - patch_embed -\u003e token token map -\u003e token -\u003e token map\n​\t⭐proj = Conv2d(in_channel, dim_token[i], kernel_size[i], stride[i])\ndim_token = [64, 128, 256, 512]\nkernel_size = [4, 2, 2, 2]\nstride= [4, 2, 2, 2]\t@ 渐进式缩小特征图\t– 融合局部token的感觉\ntoken数量⬇，token维度⬆(信息更加丰富)\n多次Patch Embed\n②Transformer Encoder 对 token 进行信息提取 MHSA处进行调整：引入Spacial Reduction操作\nSpacial Reduction：将token折叠回特征图，使用Conv2d对特征图进行局部信息融合的处理，再保持通道维度不变的情况下，缩小宽高空间大小，再拆分成token，减少token数量。\n对Token Key和Value的部分进行SR处理，shape=(*num_token, dim_token)\nQuery shape=(num_token, dim_token)\nα = softmax(Q@SR(K).T / √d) shape=(num_token, *num_token)\nα@V\tshape=(num_token, dim_token)\n⭐图示操作：\n利用conv局部性，将相邻的Token进行合并 | 或者利用Pool进行窗口内Token合并\n计算量降低sr_ratio平方倍 – Conv2D中的stride\n③将token折叠回特征图\nCvT - ICCV 2021 CvT: Introducing Convolutions to Vision Transformers\n很像PVT - 前置论文Bottle neck Transformer @ 在resnet末尾将后三层Conv3×3替换为MHSA （Conv结构中引入Attension全局建模）\n目的：\n①将卷积神经网络 (CNN) 的理想特性引入 ViT 架构（即移位、尺度和失真不变性），同时保持 Transformer 的优点（即动态注意力、全局上下文和更好的泛化）\n②将具有图像域特定归纳偏差的卷积引入 Transformer 来实现两全其美\n③空间维度下采样，通道增加。Token数量少了，但每个Token变长了，增强了携带信息的表示的丰富性\n⭐删除位置嵌入：为每个 Transformer 块引入卷积投影，并结合卷积令牌嵌入，使我们能够通过网络对局部空间关系进行建模。使其具有适应需要可变输入分辨率的各种视觉任务的潜在优势。\n总体结构\rConvolutional Token Embedding 重叠式的嵌入，使用标准Conv2D\n并且在每个Stage最后，将Token集合折叠回Token Map\nConvolutional Projection – Conv生成QKV 而非通常的nn.Linear 利用 分组卷积 将K与V的Token Map下采样，性能换效率\n局部空间上下文的附加建模，相比于普通卷积(更复杂的设计和额外的计算成本)，DWConv性价比更高\n(b): 可参考MobileViT，先利用卷积局部融合信息，再执行窗口级注意力\nSwin - ICCV 2021 Swin transformer: Hierarchical vision transformer using shifted windows\n层级式ViT\n创新点：\n①patch embed尺寸逐步增大，检测小目标更好\n总体结构\n窗口级的Token特征融合操作： Swin Transformer Block 数量为偶数 ①窗口内的Token信息融合\nMHSA限制在窗口内进行 – 降低计算复杂度\n②窗口间的Token信息融合 – 滑动窗口策略 – 间接看到全局信息\n蓝线表示窗口内Token信息融合，红线表示窗口间信息融合\n❗❗❗⭐⭐⭐MHSA依然限制在窗口内进行\n❗为了规整统一窗口大小，便于批量计算\n进行一次循环位移，使窗口大小统一\n⭐使用window-mask将本该不进行自注意计算的部分遮挡掉，在计算注意力α时，在其中softmax中，将mask的值设为-100，则e^{-100} / Σ e^{i} 为 0\n滑动窗口Mask示例：\n给窗口编号，再使用torch.roll 滑动 （ window_size // 2 ） 个像素\n$$ \\text{softmax}(z)i = \\frac{e^{z_i}}{\\sum{j=1}^{K} e^{z_j}}, \\quad \\text{for } i = 1, 2, \\ldots, K \\\\e^{-100} ≈ 0 $$\n相对位置编码： 因为计算Window-MHSA是并行的，没有位置顺序信息\n⭐注意力权重 加上 相对位置偏置\n✨相对位置信息 – 压缩\u0026复用\n① relative position bias table 自学习的位置标量偏置 – 压缩\n② relative position bias index 索引 – 不同位置的token使用相同的相对位置偏置 – 复用\n给定窗口大小，index是固定值，table是可学习的位置信息\n一维情况：\nPatch Mergeing 目的： 宽高减半，通道翻倍\n整幅特征图分4部分，合并不同部分，相同位置的Token\nMobilevit - CVPR 2021 title: Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer\n在资源受限设备上运行ViT – 混合CNN和Transformer\nMV2: MobileNetv2 中的倒残差块 ⬇2： 下采样2倍，控制stride步幅\nMobileViT Block:\t– 间接融合 用二维卷积局部融合(重叠区域)，输出的特征图中每个像素都会看到 MobileViT 块中的所有其他像素\n红色像素使用变压器关注蓝色像素\n由于蓝色像素已经使用卷积对有关相邻像素的信息进行了编码，因此红色像素可以对图像中所有像素的信息进行编码\nunfold和fold 将特征图转为Token集合，改排序，不等于patch embed(用conv2d)\n原论文 最后的通道维度融合，使用的Conv3x3卷积。 # 模块对称。\nCrossViT - ICCV 2021 CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification\n双分支 - 多尺度patch嵌入\n互换分支的CLS Token 实现分支间的信息通信\n总体结构\r交互示例\r详细\rDeiT - CVPR 2021 Training data-efficient image transformers \u0026 distillation through attention\n⭐纯Transformer版本的知识蒸馏\n引入distillation token来进行知识蒸馏，执行的目标不同。一个与label计算loss，一个与Teacher label计算蒸馏损失。反向传播优化时，梯度不一样\nT2T - ICCV 2021 Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\n重叠嵌入，信息更全面。融合Token，删除冗余\nsoft split\t重叠融合(图示是堆叠在通道维度)\nBiFormer - CVPR 2023 BiFormer: Vision Transformer with Bi-Level Routing Attention\n一种动态的、查询感知的稀疏注意力机制。 关键思想是在粗糙区域级别过滤掉大部分不相关的键值对，以便只保留一小部分路由区域，再进行细粒度的注意力计算\n注意力区域对比\r路由方式建立跨窗口信息交互\n通过化为window，各Token均值为代表整window的Token，计算区域相似性关系图，借此为窗口间链接通信\n将窗口串起来\nBi-Routing Self-Attention\r总体结构\rDWConv 3×3 作用：\n​\t在开始时使用 3×3 深度卷积来隐式编码相对位置信息。\nSMT - ICCV 2023 Scale-Aware Modulation Meet Transformer\n“模拟随着网络变得更深而从捕获局部依赖关系到全局依赖关系的转变“（串行渐变结构） =联想到=\u003e 浅层网络捕获形态特征，深层部分捕获高级节律特征\nSMT1D 生硬的训练ECG 中等效果，但计算量小！\n层级式ViT结构\n总体结构\rEvolutionary Hybrid Network 结构取名为“进化混合网络” Evolutionary Hybrid Network - 局部到全局建模的过渡\n两种融合模块的堆叠方式\r性能对比\rSAM Block scale-aware modulation\rMulti-Head Mixed Convolution 将通道分割为多个头(组)，每个头使用不同大小的卷积核（目的：捕捉多个尺度的各种空间特征。增强网络的建模能力“局部远程依赖”）\n例：\nScale-Aware Aggregation 倒残差结构 - 融合通道\n在MHMC结构中，多头由于卷积核大小不同，更大的卷积核看到的感受野更大。 将多头各个卷积依次打包成组，进行组间通道融合，每组既有小感受野的信息，又有大感受野信息，多样 multi-scale\n可视化模块效果 – 画的卷积调制权重 深度可分离卷积作为注意力权重\n① 每个不同的卷积特征图都学习以自适应方式关注不同的粒度特征\n②多头更准确地描绘前景和目标对象\n③网络变深，多头仍然可以呈现目标物体的整体形状。与细节相关的信息在单头卷积下会丢失\n⭐⭐表明 MHMC 在浅层阶段有能力比单头更好地捕获局部细节，同时随着网络变得更深，保持目标对象的详细和语义信息。\n增强了语义相关的低频信号，精确地聚焦于目标对象最重要的部分。\n更好的捕获和表示视觉识别任务的基本特征的能力。\nConv2Former - CVPR 2022 Conv2Former: A Simple Transformer-Style ConvNet for Visual Recognition\n创新点： 用卷积操作 模拟近似 注意力 – 更低的计算代价 略低的性能下降\nHadamard product == 矩阵按元素相乘\n使得每个空间位置(h,w)能够与以(h,w)为中心的k×k正方形区域内的所有像素相关。通道之间的信息交互可以通过线性层来实现。\n※重点： ConvNeXt中表明，卷积核不是越大越好，超过7x7，计算代价远远大于性能收益\n⭐本文实验提出，将卷积特征作为权重，能比传统方式更有效地利用大内核\nConvNeXt - CVPR 2022 ConvNeXt 将CNN搭建成SwinT的风格，性能超越SwinT\nInceptionNext 多尺度版本的ConvNeXt\nShunted SA - CVPR 2022 Shunted Self-Attention via Multi-Scale Token Aggregation\nPVT 和类似的模型往往会在这种空间缩减中合并太多的标记，使得小物体的细粒度信息与背景混合并损害模型的性能；\n同时保留粗粒度和细粒度的细节，同时保持对图像标记的全局依赖性建模；\n观察到PVT和ViT在同一个Encoder中token尺度是固定的，创新结合二者。用Conv将Token Map局部融合成多幅Token Map，使每个汇聚的Token代表着不同区域，不仅观察到小物体，也能看到大物体。 ViT：小区域\nPVT： 大区域\nShunted : 结合大小区域\n灵感源于：\n做法：\n​\t不同头的长度不同，以捕获不同粒度的信息。\n原始特征图x，卷积局部融合得到x1， x2 (HW均变小)\n❗注意：Q的线性映射维度不变，而不同尺度的KV通过线性映射时维度缩减率为SR特征图的数量（QKV维度一致）\n两种做法\n细粒度 - pixel token || 粗粒度 - patch token\nCAM - CVPR 2015 Learning Deep Features for Discriminative Localization\n弱监督对象定位 - 仅提供Image level label\n期望：每个单元被其感受野内的某种视觉模式激活。因此 fk （表示空间位置 (x, y) 处最后一个卷积层中单元 k 的激活//输出特征图的一个像素）是该视觉模式存在的地图。类激活图只是这些视觉模式在不同空间位置的存在的加权线性和\n计算卷积特征图对于特定输出单元的重要性来实现的\n⭐⭐⭐网络可以保留其卓越的定位能力，直到最后一层 =\u003e 深层特征的定位能力\n❗❗❗尽管接受了图像级标签的训练，CNN 仍具有出色的对象定位能力\n缺陷：卷积特征图→全局平均池化→softmax层 // 特定网络结构\n做法图示\r数学公式\r在卷积特征图上执行全局平均池化，并将它们用作全连接层的特征，产生所需的输出分类;\n❗❗❗将输出层的权重投影回卷积特征图来识别图像区域的重要性\nGrad-CAM - ICCV 2017 适用CNN模型\n但论文提到在CNN+LSTM的也能定位有区别的图像区域\nα 捕获特征图 k 对于目标类 c 的重要性 // 与CAM的分类线性层权重作用一致\nReLU的作用，只对对感兴趣的类别有积极影响的特征感兴趣。负像素可能属于图像中的其他类别\n上述操作 =\u003e 具有类别区分性并且可以很好地定位相关图像区域 - 最后特征图比较小!\n​\t但缺乏显示细粒度重要性的能力 （能区分猫狗，但对为什么识别为猫，不够精确）\n通过点乘法融合 引导反向传播 和 Grad-CAM =\u003e 可视化\nGrad-CAM : 类别区分性\nGuided Backprop： 细节纹理重要程序。 做法：将梯度值小于等于零的部分置为零，保留梯度值大于零的部分 =\u003e 以突出输入图像中对预测结果有积极影响的区域，来实现对神经网络中每个像素对最终预测结果的影响进行可视化和解释\n浅层卷积关注纹理特征，深层网络关注本质的那种特征？\nDETR - ECCV 2020 ⭐End-to-End⭐ Object Detection with Transformers\n传统：设置锚框 + 非极大值抑制(去除多余的框)\n创新：集合预测(预测分类 + 锚框)\n前向流程\r模型框架\rCNN backbone - local information fusion\nTransformer encoder - global information fusion\nobject queries - learnable information vector 作用： =\u003e anchor\nFFN: 1. classification =\u003e output class vector 2. box =\u003e output 4 number [center_x, center_y, width, hight]\nobject queries: 作用就是锚框，并且一次性生成100个 » 图片检测的物体数\n如何将object queries 与 groundtrue一一对应？\n匈牙利算法 寻找最佳匹配\n匹配loss = 分类loss(分类正确率) + 框loss(框的重叠度)\n=\u003e 匈牙利算法 =\u003e 哪些框与GT最佳匹配(预测框与GT一一对应)\n最终回传梯度优化参数LOSS = 分类loss(分类正确率) + [框loss + 与框大小无关的iou loss]\n因为框也是生成的，且Transformer容易出大框(全局建模)\nBert - 2018 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - Computation and Language\nBidirectional Transformers 意思是\nTransformer Encoder中的自注意力计算是全局的，每个Token能观测到其余的Token序列； 而\nTransformer Decoder中由于进行的是Masked Multi Head Self Attention，所以序列只能观测到自己与之前的语境；\n这对于文本上下文语境建模是有弊端的！\n(生成式无监督学习)\n总体结构\n[CLS] 分类Token，凝聚全局语义信息\n[SEP] 分割符，划分句子范围\n用的某个语料库进行词嵌入\n⭐训练方法\n完型填空：使某个词随机被 [Mask] 字符遮挡；为防止模型对[Mask]字符敏感，遮挡时使用概率遮挡 =\u003e 1. 仍替换为[Mask]字符 2.随机替换为其他字符 3. 保持不变 =\u003e 迫使模型学习上下文语境 [句子内信息建模]\n预测下一句： [句子间信息建模]\n丰富的上下文信息：通过考虑单词的左右上下文，BERT 能够更好地理解词义和句法结构，这对于理解语言的复杂性至关重要。\nGPT generative pretrain transformer\n初略版\n模型图：(Transformer Decoder -仅Masked Attention版)\n⭐ input: word =\u003e index(查找词汇表) =\u003e embedding + position embedding\n[batch, sequence_length, embedding_dim]\t# sequence_length 可由多个句子组成，可以使用标识句子结束，用来填充，使得sequence_length在batch内长度一致\n⭐ train：\nx1 =\u003e x2\nx1, x2 =\u003e x3\nx1, x2, x3 =\u003e x4\n因为每个词都要预测下一个词，故使用masked attention （mask-softmax），防止答案泄漏\nCLIP - 2021 Learning Transferable Visual Models From Natural Language Supervision\n实现zero-shot，上游大数据集预训练好，下游任务迁移学习无需样本微调\n多模态模型的总体目标就是：训练一个模型，一方面能统一特征表达，另一方面又能让不同模态特征间学到相关性\n(判别式无监督学习)\n工作目的 痛点：\n在特点数据集上进行标签训练 =\u003e 输入没见过的类别，那么模型就不能输出正确的结果\n数据出现分布偏移，动物图片与卡通动物图片 =\u003e 识别不出来\n图片 - 文字描述 ， 模型学习配对关系\n一个对象的不同视角表示：图片和文本描述\n对比学习方法 （Train） Text Encoder (resnet/vit) 学习文本描述的 深度特征 - 单模态内特征 // T_i == 一个文本特征 Image Encoder(transformer) 学习图片的 深度特征 - 单模态内特征 // I_i == 一个图像特征 将多模态特征投影到跨模态空间 // 矩阵映射，(特征向量)到同一纬度 计算余弦相似度，很明显，正确配对的位置为对角线 计算Loss： （相似度logit作为预测分数） 按行计算Loss，在每一行范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一张图片，我们都希望找到和它最相似的文字 按列计算Loss，在每一列的范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一段文字，我们都希望找到和它最相似的图片 最后将这两个Loss相加取平均，代表我们在模型优化过程中考虑了“图片-\u003e文字”和“文字-\u003e图片”的双向关系 encoder均从头开始训练\n预测 创建一个标签全集， [f’A photo of a {object}’ for object in dataset_labels] Text Encoder 学习上述 模板文字描述 的 深度特征 Image Encoder 学习 待预测图片 的 深度特征 计算余弦相似度 =\u003e 取最高分作为预测目标 ​\n缺点 每次预测需要构建标签全集描述 对抽象任务，性能较差 Two-Stream - 2014 Two-Stream Convolutional Networks for Action Recognition in Videos\n⭐先前工作中通过使用堆叠视频帧作为网络的输入来解决此任务，但结果明显比最好的手工制作的浅层表示差。 =\u003e 这可能表明学习的时空特征不能很好地捕捉运动（简单堆叠 =\u003e 让模型从庞大的数据中学习❌很难） =\u003e 模型很难识别该类特征 =\u003e 预先处理成模型擅长的数据形式\n❗❗❗虽然多帧信息很重要，但以适当的方式将其呈现给 ConvNet 也很重要（饱和）\n信息显式建模 =\u003e 可以简化学习过程\n创新点\n空间流从静止视频帧执行动作识别 时间流以识别密集光流形式的运动动作 =\u003e 贴合人类视觉：识别物体 + 识别运动 =\u003e ⭐[结果角度]表明两个识别流是互补的\n光流：帧帧之间像素变化(描述运动变化的数据形式) | 水平方向和垂直方向 \u003c= 有用的线索 (与魔改模型不一样的思路)\n⭐⭐⭐此类输入明确描述了视频帧之间的运动 =\u003e 这使得识别更容易 =\u003e 因为网络不需要隐式估计运动 （显示建模）\n可以从光流数据 =\u003e 时空局部特征 || 运动学特征 || 运动是使用光流位移场明确表示的\n模型框架（双模态）\n提高模型迁移能力 类似于CLIP的目的\n考虑将两个数据集合并为一个，由于类集之间的交叉，这并不简单\n=\u003e 多任务学习 =\u003e 最后生成多个分类头，对应两个数据集分类。混合多个数据集进行实验\nGC-ViT - ICML 2023 Global Context Vision Transformers\n与SwinT的区别：\n不通过滑动窗口实现全局信息建模 =\u003e 直接生成全局信息Token，使用这组携带全局信息的TokenSet来实现全局信息建模 SwinT 层数堆不够的话，全局信息建模不强 与SwinT的共同点：\n都属于window-wise attention\n位置编码都采用相对位置偏置bias\n基本单元：局部窗口内计算 + 全局信息建模；（窗口内 + 窗口间）\n思路：\nGlobal query tokens generation\nMBConv - FeatureExtract\n代码分析\nX: [batch, num_token, dim_token] x-window: [batch*num_windows, num_token//num_windows, dim_token] x-window-head:[batch*num_windows, num_heads, num_token//num_windows, dim_token//num_heads] Query: [batch × num_window, num_heads, num_tokens, dim_head] Key-Global: [batch, repeat , num_heads, num_tokens, dim_head] 图示最后的复制，是让每个窗口内的token与全局token表示进行信息融合 （复制窗口维度的大小）\nfunny: 公司{多部门} =\u003e [部门内部成员进行讨论] =\u003e [部门之间领导进行讨论]\nToken Sparsification - CVPR 2023 Making Vision Transformers Efficient from A Token Sparsification View\n动机：\n​\t观察到注意力后期，仅部分核心Token起着主要作用\n对Token进行瘦身 - 与利用CLS Token进行级联删除不用的策略\n方法：\tSparse Token Vision Transformer\nSemantic Token Generation Module 1️⃣ Base Module 学习浅层特征\n2️⃣ 进行Spatial Pooling聚合区域，生产空间簇中心TokenSet\n​\tConv( GELU( LayerNorm( DWConv( X )))) - 深度可分离\n​\t创新：Intra and inter-window spatial pooling\n​\t目的：聚合窗口信息(代表) + 疏远窗口间信息(不可被替代)\n3️⃣ 融合生成Semantic Token Set\t- Global Initial Center G 是可学习的参数 $$ \\overline{S^{1}} = MHA(P, X, X) + P,\\ \\ \\ S^{1} = FFN(\\overline{S^{1}}) + \\overline{S^{1}} \\\\ \\overline{S^{2}} = MHA(S^{1} + G, Concat(S^{1}, X), Concat(S^{1}, X)) + P,\\ \\ \\ S^{2} = FFN(\\overline{S^{2}}) + \\overline{S^{2}} \\ S^{2} = Semantic\\ \\ Token $$\nRecovery Module $$ \\overline{X} = MHA(X, S, S) + P,\\ \\ \\ X = FFN(\\overline{X}) + \\overline{X} $$ 融合操作的逆过程\n从语义级TokenSet中恢复空间信息\nFirst layer attention maps\nMOCO - CVPR 2020 Momentum Contrast for Unsupervised Visual Representation Learning\n(判别式无监督学习)\n目标：\n​\t1️⃣ 构建一个足够大的动态词典，包含足够多的负样本，使模型真能够学到判别式的特征; 在海量数据中学到真正的样本分布\n​\t2️⃣ 因为词典是动态变化的，为了使词典中的负样本特征尽可能的保持一致性(模型参数不同，时间维度上，得到的特征向量存在不一致性)，提出动量更新 =\u003e 动量模型的缓慢更新确保了字典中的特征相对稳定，从而提供更一致的负样本，提升对比学习的效果。 $$ θ_{k} ←mθ_{k} + (1 −m)θ_{q} $$ m ∈ [0, 1) 是动量系数。论文中Query Encoder 和Key Encoder是一样配置架构的编码器。 目的，缓慢的更新Key Encoder，构建一个又大又一致的动态词典。\n框架伪代码\npretext task ： 实例判别任务。目标拉近正样本对在特征空间的距离，并使负样本对尽可能的远离。\n# f_q, f_k: encoder networks for query and key # queue: dictionary as a queue of K keys (CxK) # m: momentum # t: temperature f_k.params = f_q.params # initialize for x in loader: # load a minibatch x with N samples x_q = aug(x) # a randomly augmented version x_k = aug(x) # another randomly augmented version q = f_q.forward(x_q) # queries: NxC k = f_k.forward(x_k) # keys: NxC k = k.detach() # no gradient to keys # positive logits: Nx1 l_pos = bmm(q.view(N,1,C), k.view(N,C,1)) # negative logits: NxK l_neg = mm(q.view(N,C), queue.view(C,K)) # logits: Nx(1+K) logits = cat([l_pos, l_neg], dim=1) # contrastive loss, Eqn.(1) labels = zeros(N) # positives are the 0-th loss = CrossEntropyLoss(logits/t, labels) # SGD update: query network loss.backward() update(f_q.params) # momentum update: key network f_k.params = m*f_k.params+(1-m)*f_q.params # update dictionary enqueue(queue, k) # enqueue the current minibatch dequeue(queue) # dequeue the earliest minibatch 两个不同视角的同一张图片为正样本对，不同图片为负样本对。\n抽特征：q, k； 最大化q k相似且与负样本远离\n更新Q_Encoder, 使用Q_Encoder参数更新K_Encoder，动量缓慢更新\n无监督预训练后好，取Q_Encoder作为抽取特征骨干网络，冻结其参数，微调分类头，在进行泛化测试\nMAE - CVPR2022 Masked Autoencoders Are Scalable Vision Learners\n非对称掩码自动编码器；Encoder和Decoder架构可以不同\n(生成式无监督学习)\nCV领域的Bert\n⭐NLP与CV的不同：\n信息密度不同\nNLP：句子信息语义很高，信息密度也高。（人类语言-事先浓缩过的信息）\n视觉：信息很冗余，也没高级的语义（自然界），像素可以被相邻的重建恢复\n恢复难度不一致\n恢复高级语义单词和恢复像素级(低级语义)图片，难度也不一样。 框架：\nEncoder：位置编码所有Patch都加上。但仅输入未被Mask的Patch。并且Mask比例很高(论文mask75%patch)，迫使模型学到高维的特征，而非捷径。\nDecoder：Mask Patch是自学习的向量，并且和Encoded Patch在位置上一致，再次添加位置信息。\n简单实现（shuffle，截取前面的作为Encoder输入，后面Patch被Mask；再shuffle逆操作，将Encoded Patch和learnabel patch位置对齐组合好进行Decoder。再重建像素）\n通过预测每个屏蔽补丁的像素值来重建输入。解码器的最后一层是线性投影，其输出通道的数量等于补丁中像素值的数量。\n损失函数计算像素空间中重建图像和原始图像之间的均方误差。\n问题：\n​\t预训练的输入中具有很大一部分掩码标记，而这在下游任务未损坏的图像中不存在。这种差距可能会降低部署的准确性。\n自监督学习方法通常侧重于预训练的不同借口任务。\n对比学习对两个或多个视图之间的图像相似性和相异性进行建模。\nwav2vec 2.0 - NeurIPS 2020 wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\n时序信号版本的Bert\n[自监督]学习通用特征 =\u003e 再微调任务头\n(判别式无监督学习)\n总体框架：\n1️⃣ 原始信号[X] =CNN=\u003e 浅在特征表示 [Z]\n2️⃣ 浅在特征表示 [Z] =Transformer Encoder=\u003e 全局上下文特征表示 [C]\n3️⃣ 浅在特征表示 [Z] =量化器=\u003e 对比目标[Q]\n把原来连续的特征空间假设是d维，拆分成G个子空间（codebook），每个子空间维度是d/G。 然后分别在每个子空间里面聚类（K-mean什么的），一共获得V个中心和其中心特征。 每个类别的特征用其中心特征代替。 量化qt和对应ct\n❌ Quantization module 部分不理解\nMask\n随机起点，遮挡后面t个时间步\n对比损失\n​\t包括 qt 和 K 个干扰项\n❗❗❗理解不太清晰\nWhisper - 2022 OpenAI Robust Speech Recognition via Large-Scale Weak Supervision\n大力出奇迹\n模型结构\n多任务训练\n[英文口语=\u003e 英文] 语音识别\n[多语言口语 =\u003e 英文] 语音识别 +翻译\n[多语言口语 =\u003e 对应语言文字] 语音识别\n识别背景音(无内容声音)\n⭐⭐⭐信号 =\u003e Log-Mel Spectrogram(频谱图)\n音素\n⭐⭐⭐数据集\n680k小时，超大数据集。 在此基础上预训练，并且0样本迁移(无需特定任务微调)\nCPC - Machine Learning 2018 Representation Learning with Contrastive Predictive Coding\nContrastive Predictive Coding - 无监督学习\n通过预测未来，学习特征表示 (学习对(高维)信号不同部分之间的底层共享信息进行编码的表示 - 局部平滑度)\n不预测原始信号，而是对高维嵌入依赖建模\n(判别式无监督学习)\nCPC框架\n$$ g_{enc}: local\\ feature\\ learning\\ g_{ar}: global\\ context\\ learning $$\n对比学习\n# 序列： [x1, x2, x3, x4, x5, x6, x7, x8] # positive sample # x1, x2, x3, x4 =\u003e c \u003c=\u003e x5, x6, x7, x8 # negative sample # x1, x2, x3, x4 =\u003e c \u003c=\u003e xa, xb, xc, xd (同batch中其他的) 迫使模型学习序列间的high level feature，学习这种内部的顺序逻辑关系\n互信息公式 - 衡量两个随机变量之间的相互依赖程度 $$ I(x;c)=\\sum_{x,c}p(x,c)\\log\\frac{p(x|c)}{p(x)}. \\ I(x;c): x 与 c 的互信息\\ p(x,c): x 和 c 同时发生的联合概率分布\\ p(x|c): 给定 c 的条件下，x 发生的条件概率分布\\ p(x): x 的边缘概率分布 $$ InfoNCE Loss\n最小化CPC定义的损失 =\u003e 实际上最大化 context c_t 和待预测正样本 X_t 之间的互信息\n优化目标：最大化似然概率\n正相关\n近似计算互信息\nLoss\nX = {x1, x2, …, xN} N个负样本，从batch中其他数据中采样\nE/X 表示似然概率\n最大化互信息 =\u003e 最小化Loss (互信息下界)\n完整InfoNCE-Loss $$ \\text{InfoNCELoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\text{exp}(s(x_i, x_i^+))}{\\text{exp}(s(x_i, x_i^+)) + \\sum_{j=1}^{K} \\text{exp}(s(x_i, x_j^-))} $$\n不太了解这个最大化互信息的Loss\nDALL·E 2 - 2022.3 OpenAI Hierarchical Text-Conditional Image Generation with CLIP Latents\n层级式图生文\n模型架构\n描述：\n虚线上是CLIP架构(文本和图像的联合表示空间)，学习图文对的关联信息 虚线下是生成框架，prior模型根据Text Embedding生成出CLIP对应的Image Embedding， decoder(diffussion model)根据Image Embedding进行重建 ⭐分步训练\nprior\n生成CLIP image Embedding (diffusion model)\ndecoder\n是根据image Embedding生成图片 – 并且这个decoder是多个堆叠，先生成低分辨率，再高清化 - (diffusion model)\n简洁表示：\ny - 文字\nx - 图片\nzi - 图片嵌入 （显式生成图像表示可以提高图像多样性，同时将照片真实性和标题相似度的损失降至最低）\ndiffusion model - NeurIPS 2020 Denoising Diffusion Probabilistic Models\n框架：\n加噪 + 去噪(还原)\n正向过程：\n1️⃣ Xt 是 前一张图片加噪生成的，Z1是服从正太分布的噪声\nβt是超参数=范围为[0.0001,0.02]递增，则αt 是随时间减少， 表示公式一中原图信息越来越少，噪声越来越重\n2️⃣ 递推带入一下\n最后可得···\nZt_hat 是一个服从正太分布的随机噪声，at_hat = at*at-1*···*a1, 可由X0直接产生任意时间步的加噪图片\n反向去噪过程：\n核心基础\n1️⃣ 用Xt生成Xt-1 ，按贝叶斯公式转换\n2️⃣ q(Xt|Xt-1) == q(Xt|Xt-1, X0)， 而q(Xt-1) == q(Xt-1|X0) 任意步加噪图可由原图直接产生\n3️⃣ 反解公式7， X0可由Xt进行估计\n4️⃣ 带入并整理\n··· 因此可根据Xt =\u003e Xt-1， 下式为最终的去噪公式 $$ x = \\frac{1}{\\sqrt{\\alpha}} \\left( x - \\frac{1 - \\alpha}{\\sqrt{1 - \\alpha_{\\text{hat}}}} \\cdot \\text{predicted_noise} \\right) + \\sqrt{\\beta} \\cdot \\text{noise} $$ β noise 保证多样性\nƐθ表示预测模型\n代码逻辑\n训练：\nfor i, images in enumerate(pbar): images = images.to(device) t = diffusion.sample_timesteps(images.shape[0]).to(device)\t# 采样几个时间步进行训练 x_t, noise = diffusion.noise_images(images, t)\t# @ 公式-7 predicted_noise = model(x_t, t)\t# Unet loss = mse(noise, predicted_noise) optimizer.zero_grad() loss.backward() optimizer.step() sampled_images = diffusion.sample(model, n=images.shape[0]) # @ 去噪公式 预测模型\nUnet 带时间点嵌入(用余弦-位置嵌入实现的)\ndef forward(self, x, t): t = t.unsqueeze(-1).type(torch.float) t = self.pos_encoding(t, self.time_dim)\t# 嵌入时间步顺序 x1 = self.inc(x)\t# cnn x2 = self.down1(x1, t) # pooling x2 = self.sa1(x2)\t# self attention x3 = self.down2(x2, t) x3 = self.sa2(x3) x4 = self.down3(x3, t) x4 = self.sa3(x4) x4 = self.bot1(x4)\tx4 = self.bot2(x4) x4 = self.bot3(x4) x = self.up1(x4, x3, t)\t# 插值上采样，再拼接skip connect x = self.sa4(x) x = self.up2(x, x2, t) x = self.sa5(x) x = self.up3(x, x1, t) x = self.sa6(x) output = self.outc(x) return output p(Xt-2|Xt-1, t, y) t是时间步嵌入，y是条件嵌入\n最简单的融合方法就是相加\n？？？ 疑惑点 - 反向过程求的t时刻的均值方差用在哪了？\n2025/1/14 更新：\n// X =\u003e X_noise_t 可以一步生成，可以时间步t可以直接生成对应t时间步的噪声图。\n// 使用U-net预测时间步t的噪声 使用MSE-Loss进行训练\n# 训练循环 for epoch in range(num_epochs): for x_0 in dataloader: # x_0 是原始数据 # 1. 随机选择一个时间步 t t = torch.randint(0, T, (x_0.size(0),)) # 为每个样本随机选择一个时间步 # 2. 前向过程：添加噪声 epsilon = torch.randn_like(x_0) # 采样噪声 alpha_bar_t_t = alpha_bar_t[t].view(-1, 1, 1, 1) # 选择对应时间步的 alpha_bar_t x_t = torch.sqrt(alpha_bar_t_t) * x_0 + torch.sqrt(1 - alpha_bar_t_t) * epsilon # 添加噪声 # 3. 反向过程：预测噪声 predicted_epsilon = model(x_t, t) # 模型预测噪声 # 4. 计算损失函数 loss = F.mse_loss(predicted_epsilon, epsilon) # 均方误差损失 # 5. 反向传播和优化 optimizer.zero_grad() loss.backward() optimizer.step() // 串行，一步一步去噪\n# 生成过程 def generate_samples(model, num_samples, T, alpha_bar_t): x_T = torch.randn(num_samples, ...) # 从标准正态分布中采样初始噪声 for t in range(T-1, 0, -1): # 从 T-1 到 1 # 预测噪声 epsilon = model(x_T, t) # 计算去噪后的数据 \u003c= 消除噪声 alpha_t = alpha_bar_t[t] / alpha_bar_t[t-1] x_T = (x_T - torch.sqrt(1 - alpha_t) * epsilon) / torch.sqrt(alpha_t) return x_T Time-Frequency Consistency - NeurIPS 2022 Self-Supervised Contrastive Pre-Training for Time Series via Time-Frequency Consistency\n期望同一示例的基于时间和基于频率的表示在时频空间中靠近在一起\n(判别式无监督学习)\npretext task：实例判别 (一对正样本，其余负样本)\n框架\n时域增强：基于时间特性从 xi 扩充，包括抖动、缩放、时移和邻域分段；\n频域增强：频谱特征扰动 xFi 的增强，添加或删除频率分量来扰动频谱 (确保扰动时间序列仍然与原始样本在频域和时域仍相似)\nLoss：\n余弦相似度：衡量两个向量之间相似性，范围[-1, 1]\n// 补充 2025/1/14\n样本越相似 =\u003e sim(i, j) 越大 =\u003e exp(sim) 越大 =\u003e exp(sim)/\\sum(exp) 越接近于1 =\u003e log(·)越接近于0\n-log(·) 将图像倒置，样本越不相似 =\u003e exp(sim)/\\sum(exp) 越接近于0 =\u003e loss 越大\nCOMET - NeurIPS 2023 Contrast Everything A Hierarchical Contrastive Framework for Medical Time-Series\n(判别式无监督学习)\n我们的方法旨在弥合标记数据的有限可用性与医疗时间序列分析中稳健且可概括的模型的需求之间的差距 对比表示学习背后的关键思想是通过将相似的数据靠近在一起并将不相似的数据进一步分开来挖掘数据一致性 关键是要利用所有可用的信息；除了样本标签之外，数据集是否还有其他信息？(补充信息)\n患者间、患者内进行测试(定义打乱规则)\n⭐⭐⭐多级信息\n多级对比学习框架\n代码分析总结\nX_train.size == [Batch, channels, segment] # segment length 330, sampling_rate = 250Hz y_train # 心肌梗塞二分类标签， patient_id， segment_id 细分为样本级别(心跳)，故图示Encoder不同级别对比学习可复用\n关键，计算不同级别的对比Loss\nPatient-Level Loss\nx = [Batch, channels, segment] out1 = net(x) out2 = net(x) # ? 模拟数据增强后的不同视角或版本，以便在对比学习中生成有效的正例和负例 # 根据y_train 病人id，构建mask矩阵 pid1, pid2 = np.meshgrid(str_pid, str_pid)\tpid_matrix = pid1 + '-' + pid2\t# 每项为 id_x - id_y\t# 目标位置 id_x - id_x pids_of_interest = np.unique(str_pid + '-' + str_pid) # unique combinations of pids of interest i.e. matching bool_matrix_of_interest = np.zeros((len(str_pid), len(str_pid))) for pid in pids_of_interest: bool_matrix_of_interest += pid_matrix == pid # 上三角和下三角目标row，col rows1, cols1 = np.where(np.triu(bool_matrix_of_interest, 1)) # upper triangle same patient combs rows2, cols2 = np.where(np.tril(bool_matrix_of_interest, -1)) # down triangle same patient combs out1, out2 =\u003e sim_matrix_exp # 余弦相似度矩阵 # @@ 对比学习：不考虑对角线元素的原因主要是因为对角线元素表示的是特征向量与其自身的相似度 # @@ 我们关注的是如何使具有相同标签的不同特征向量之间更相近，而使不同标签的特征向量之间更疏远 # Loss 分母， 某样本与其他样本相似度之和， @分上三角和下三角 triu_sum = torch.sum(sim_matrix_exp, 1) # add column tril_sum = torch.sum(sim_matrix_exp, 0) # add row loss_terms = 0 # 取平均 if len(rows1) \u003e 0: triu_elements = sim_matrix_exp[rows1, cols1] # row and column for upper triangle same patient combinations loss_triu = -torch.mean(torch.log(triu_elements / triu_sum[rows1])) loss += loss_triu # technically need to add 1 more term for symmetry loss_terms += 1 ... loss = loss/loss_terms Trial-Level Loss\n同上，只不过patient_id =\u003e segment_id\n❌❌❌Sample-Level Loss \u0026 Observation-Level Loss\n看不懂源码~ 😎😭\nTS2Vec - AAAI 22 TS2Vec: Towards Universal Representation of Time Series\n(判别式无监督学习)\n现存工作局限：\n它们都没有以不同尺度的时间序列为特征来捕获尺度不变的信息，而这对于时间序列任务的成功至关重要。多尺度特征可以提供不同级别的语义并提高学习表示的泛化能力 粗粒度表示 - 整个时间序列，可能没那么细致 之前工作的问题\n正样本对会误判\n当存在水平偏移时，子系列一致性很容易受到攻击 (左图) 当出现异常时，时间一致性可能会引入误报对 (右图) ​\n创新：\nTS2Vec 中的对比目标基于增强上下文视图，即相同子系列在两个增强上下文中的表示应该是一致的 (上下文语境下) 层级式对比Loss，由细粒度=\u003e粗粒度，局部到全局 # 学习各种语义级别的任意子系列的上下文表示，灵活且通用的表示。 顶级语义级别的对比使模型能够学习实例(样本)级表示 框架\n流程：\n实例(一段信号)，分两个重叠的子序列, [batch, channel, dim_feature] 投影，[batch, channel, dim_feature] =\u003e [batch, channel, dim_hidden] 随机Mask掉部分信号 CNN-Encoder ⭐⭐⭐ Hierarchical Contrasting Loss\ninstance \u0026 temporal contrastive loss ​\t​\t用 -F.log_softmax(x, dim=-1) 实现\nz1 = F.max_pool1d(z1.transpose(1, 2), kernel_size=2).transpose(1, 2) z2 = F.max_pool1d(z2.transpose(1, 2), kernel_size=2).transpose(1, 2)\n编码特征浓缩，多尺度的Contrast loss\n图示Loss过程\n子序列是从原始信号中裁剪下来的，并且有重叠部分\nTimesURL - AAAI 2024 TimesURL: Self-Supervised Contrastive Learning for Universal Time Series Representation Learning\n代码在TS2Vec上修改\n=\u003e 在这里，我们必须提到，重要的时间变化信息，例如趋势和季节，在多次最大池化操作后会丢失，因此顶层对比实际上无法为下游任务捕获足够的实例级信息\n=\u003e 掩码重建进行学习实例级信息\n(判别式+生成式 混合 无监督学习)\n观察\n**简单负样本：**大多数时间序列片段可以被视为容易负样本。 这些片段往往表现出与锚点的语义差异，并且仅贡献较小的梯度，因此无法提供有用的区分信息\n硬负样本： 硬负样本就是离正样本很近，并且模型很难区别的\n正样本-硬负样本-负样本：，这些样本如果让其远离正样本可以大大提高模型性能。 其有效性被大量的简单负样本所掩盖。（现存框架没有特点显示的指出）\n由于时间序列中的局部平滑性和马尔可夫特性，大多数负样本很容易不足以捕获时间信息，因为它们从根本上缺乏驱动对比学习所需的学习信号。 作为图 2 中真实示例，对于每个正锚点（红色方块），相应的负样本（灰色标记）包含许多简单的负样本和很少的困难负样本，即许多负片太远，无法造成对比损失。\n对比学习的一个关键组成部分是选择适当的增强，这些增强可以施加一些先验来构建可行的正样本，以便编码器可以被训练来学习鲁棒和有区别的表示\n频率混合用于通过将通过快速傅里叶变换（FFT）运算计算出的一个训练实例 xi 中的一定比例的频率分量替换为同一批次中的另一个随机训练实例 xk 的相同频率分量来生成新的上下文视图 （保持病理相同）\n随机裁剪。 重叠两个子序列 - 随机裁剪是上下文一致性策略的关键步骤。 它可以保持时间序列的重要时间关系和语义一致性\n创新点：\n提出双Universum概念，就是利用Mix-up增强F(x)，追加硬负样本。 对比学习+自监督掩码重建，联合优化，来捕获段级和实例级信息， 实现通用表示 ⭐ 第一类包括预测、异常检测和插补，它们更多地依赖于在分段级别捕获的细粒度信息，因为这些任务需要推断特定的时间戳或子序列。细粒度(局部)\n⭐ 第二类包括分类和聚类优先考虑实例级信息（即粗粒度信息），旨在推断整个系列的目标。粗粒度(全局)\n实现 : 分段(对比学习，学习片段)，整句(掩码重建，学习整体)\n框架：\nAUG：\n​\t频率增强，x =\u003e fft() =mask+fusion=\u003e ifft() =\u003e y，①该篇论文fusion是融合同batch其他信号的某些部分 =\u003e 领域创新 （fusion的信号应该和这个信号相同病理，扩张数据分布）\nDualConv：\n​\t原始信号的两个增强子视图，z1 = Encoder(x1), z1’ = Encoder(x1’), =\u003e mix-up option =\u003e z1_mix = α × z1 + (1-α) × z1[torch.randperm(z1.shape[0])]，z1’_mix。\n​\t[z1, z1’, z1_mix, z1’mix] @ [z1, z1’, z1_mix, z1’mix].T =\u003e sim =\u003e -F.log_softmax(sim)\n​\tloss: 俩视图对应段为正样本(分子)\n负样本在母分，x_mix做负样本增强。\nMixup - 2017 Machine Learning mixup: BEYOND EMPIRICAL RISK MINIMIZATION\n一种简单并且不需要专业领域知识的数据增强\n现存问题讨论：\n过拟合(大模型，直接记忆Train data，走捷径) - overfitting 精心设计样本(对抗性例子，人难以察觉，但模型会给出错误的答案) - generalize 👇\nERM(经验风险最小化原则)问题\n一方面，即使存在强正则化，ERM 也允许大型神经网络记忆（而不是归纳）训练数据 另一方面，使用 ERM 训练的神经网络在对训练分布之外的示例进行评估时会极大地改变其预测。 CV: 图像的邻近区域定义为其水平反射、轻微旋转和轻微缩放的集合\n=\u003e 数据增强始终可以提高泛化能力, 但该过程依赖于数据集，因此需要使用专家知识 （不同领域增强不一定通用）\n数据增强假设邻近的示例共享同一类，并且不会对不同类的示例之间的邻近关系进行建模。(聚类)\n=\u003e 邻近风险最小化 (VRM) 原则\n⭐=\u003e 从训练样例的邻近分布中提取额外的虚拟样例，以扩大训练分布的支持度\n方法：\n框架伪代码：\n图例：\n虚拟数据，让数据边界过渡； 当不在Train数据的分布出现时，降低不确定性。 稍微清晰化边界\nYOLO-v1 - CVPR 2016 You Only Look Once: Unified, Real-Time Object Detection\n⭐将目标检测视作回归问题 =\u003e 预测出来\n前向推理\nModel:\ninput: [3, 448, 448] =\u003e output: [30, 7, 7]\n置信度(confidence): 这个值代表了模型认为预测的边界框内存在对象的概率\n框的中心坐标 + 宽高\n框中的物体是什么类\n非极大值抑制 （最佳：每个类别独立执行非极大值抑制，从而更精确地处理多类别情况）\n置信度排序：首先将所有的预测边界框按照它们的置信度（confidence scores）进行降序排序。 选择最高置信度边界框：从排序后的列表中选择置信度最高的边界框作为参考框（reference box）。 计算IOU：计算选中的参考框与列表中其他所有边界框的交并比（IOU）。交并比是两个边界框的交集面积与它们的并集面积的比值。 抑制：如果参考框与任何其他边界框的IOU超过预先设定的阈值（通常设置为0.5），那么这些边界框会被认为是多余的，并从列表中删除。 重复步骤：从剩余的边界框列表中再次选择置信度最高的边界框，重复上述过程，直到所有的边界框都被处理完毕。 最终结果：经过非极大值抑制后，剩余的边界框被认为是对目标位置的最佳预测，它们将被用于最终的目标检测输出。 训练：\nλcoord = 5, λnoobj = 0.5, 调整各个部分的重要性 $$ 1_{ij}^{obj}: 表示第ij个格子有对象 \\ 1_{ij}^{noobj}: 表示第ij个格子没有对象\\ S^{2}: 图片划分格子 \\ B: 每个格子预测多少个框 $$ bounding box loss : 中心点 + 框宽高\nconfidence: 格子是否有对象\nclasses：格子分类是否正确\nSemi-Supervised Hybrid Loss - Machine Learning 2023 Semi-Supervised End-To-End Contrastive Learning For Time Series Classification\n1️⃣无标签数据对比学习(增强视图一致性)，2️⃣有标签对比学习(相同种类一致性)，3️⃣有标签分类监督学习\n(判别式无监督学习 + 有监督学习)\n框架对比\n⭐ End to End\n框架\nUnlabeled Sample : 使用两个增强视图作为positive pair，与其他sample为negative pair (标准的对比学习)\nLabeled Sample：1️⃣ 同类型的sample为positive pair，不同类型的sample为negative pair. 2️⃣过分类头，计算分类Loss\n❤️ 混合上述三个Loss，联合优化Encoder\nSimCLR - 2020 A Simple Framework for Contrastive Learning of Visual Representations\n贡献：\n数据增强对于对比学习至关重要 (裁剪缩放，翻转，颜色紊乱，旋转， 掩盖， 高斯噪声， 高斯模糊，Sobel 滤波) - 裁剪缩放+颜色紊乱 比较好\n在经过Resnet编码器后，追加MLP能增强模型性能\n样本x，增强视图xi和xj(正样本)，batch size =N，一共2N的增强视图，对于某个样本x，xi和xj为正样本，和batch中剩余的样本的增强为负样本\n(大batchsize, 性能更好， 全局BN)\n样本自成一类，来尽可能地让编码器找到图像中最重要的特征\n框架\n共享参数 shared weight\nLoss\n上面是正样本对，下面是负样本对 -log_softmax() =\u003e 挑选出需要的值\n算法\nViLT - 2021 ViLT Vision-and-Language Transformer Without Convolution or Region Supervision\n极简结构的图片文多模态融合\n速度限制-问题分析\n归纳总结：\n(a) vision embedding 参数量 \u003e Text Embedding \u003e Modality Interaction # 缺点，视觉嵌入太重(比重太大)，并且融合非常简单即点乘算相似度\n(b) vision embedding和Text embedding 占比差不多 \u003e Modality Interaction # 模态融合之前，工作太繁杂，而且前抽取特征不好，限制后面融合，并且不重视后面的模态融合操作。\n(c) 重视visual Embed和后期的modality interaction，# text 和 vision不均等，重要性不平衡\n=\u003e 简单框架，Text词嵌入(bert中的BertEmbeddings加载训练后的权重)，vision用patch projection，都很快\n⭐ 1. 图像和文本前期嵌入应该有相似均匀的表达能力 2. 这两种模态是否在深层网络中相互作用。\n模型框架：\n初始化参数-ViT，而不是bert\n优化目标(主要)：\nImage Text Matching：0.5概率将图片替换为与文本不匹配的图片，预测一致性(二分类问题) Masked Language Modeling：预测被遮掩的词 text cls: 预测图文是否一致，二分类\ntext token set： 全局上下文=\u003e预被掩词\ntext token set 和 visual token set：进行对齐Loss\nBEIT-v3 2022 Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks\n统一Vision 和 NLP\n⭐ 核心思想是图像可以被建模为一门外语，这样我们就可以对图像、文本和图文对进行统一的掩码“语言”建模。\n结果非常好\n基础块\n共享注意力矩阵(都是一个物体的不同视角)，但是最后的FFN各个模态专享\n拓展到不同的模态：\n任务：\n图像字幕任务：采用了特殊的自注意力掩模。 图像标记（即图像块）只能在图像序列内双向相互关注。 标题的标记可以关注图像标记、它们的左侧标题标记以及它们本身。 在微调过程中，我们随机屏蔽一定比例的标题标记。 该模型经过训练，可以根据图像的线索及其左侧标题上下文来恢复这些标记。 我们还屏蔽了特殊的边界标记 [SEP]，以帮助模型学习终止生成\n视觉问答： 将任务表述为分类问题。 该模型经过训练，可以从训练集中 3129 个最常见的候选答案中预测答案。我们将给定问题和图像的嵌入连接起来，然后将输入嵌入输入多路转换器以联合编码图像-问题对。 最终的池化输出被输入到分类器层来预测答案。\n**图像文本检索任务：**是测量图像和文本之间的相似度。 根据检索目标的模态，有两个方向：图像到文本检索和文本到图像检索。 双编码器模型分别对图像和文本进行编码以获得它们的表示。 然后我们计算这些表示的余弦相似度分数。\n图像分类： 将该任务制定为图像到文本检索任务。 我们使用类别名称作为文本来构建图像-文本对。 BEIT-3 被训练为双编码器，以找到图像最相关的标签。 在推理过程中，我们首先计算可能的类名的特征嵌入和图像的特征嵌入。 然后计算它们的余弦相似度分数以预测每个图像最可能的标签。\nALBEF - 2021 Align before Fuse: Vision and Language Representation Learning with Momentum Distillation\n现存问题：大多数现有方法采用基于变压器的多模态编码器来联合建模-视觉Token（基于区域的图像特征）和文本Token。 由于视觉标记和单词标记未对齐，因此多模态编码器学习图像文本交互具有挑战性。\n（1）图像特征和文本符号映射仍然停留在他们自己的空间，使得多模态编码器很难学习建模他们之间的交互；\n（2）物体检测器 — 标注费钱，使用费算力 — 在预训练阶段需要标注矩形框，在推理阶段高分辨率图像，如 600*1000，速度较慢；\n（3）广泛使用的 image-text 数据集均是从网上搜集的带有严重噪声的数据，现有的预训练目标，如 MLM 可能过拟合到文本数据，降低了模型的泛化性能。\n框架：\nimage encoder: ViT(ImageNet预训练参数) - CLS token\ntext encoder: Bert(预训练参数) - CLS token\nmultimodal encoder: Bert(预训练参数) + cross-attention\n⭐ **在传入multi-modal encoder前，使用ITC迫使模型进行对齐 ** align before fuse的align\n对齐图像特征和文本特征，使多模态融合编码器更容易执行跨模态学习 改进了单模态编码器，以更好地理解图像和文本的语义 它学习一个共同的低维空间来嵌入图像和文本，这使得图像文本匹配目标能够通过我们的对比硬负挖掘找到更多信息样本。 Image-Text Contrastive Loss：\n正样本对：配对的Image-Text\n负样本对：Queue存储着的样本表示\nImage =\u003e 匹配Text-Queue(Momentum)\nText =\u003e 匹配Image-Queue(Momentum)\n(代码中利用了Momentum Distillation …)\nImage-Text Matching:\n有了multi-modal encoder输出的 embed token (正确样本的表征) =\u003e 即喂入Multi-modal encoder的 text embed = (positive), Image embed = (positive), 拿融合的CLS作为最终表征，过MLP =\u003e 二分类预测是否匹配；这部分Targets=(1, 1, …, 1)\n从同batch中，按相似性大小随机挑选一个(hard)负样本，\n然后，text embed = (positive, …, negetive, …) , Image embed = (negetive, …, positive)\nmulti-modal encoder =\u003e CLS =\u003e MLP =\u003e two probability\nTargets = (0, 0, …, 0)\n// 重新梳理如下：\n# 图像i-文本j =\u003e 多模态编码器 =\u003e 是否匹配； 图像 1-文本 1 : 匹配对 图像 2-文本 2 : 匹配对 图像 1-文本 2 : 不匹配 // 这里使用余弦相似度选取最困难的样本 图像 2-文本 1 : 不匹配 // 这里使用余弦相似度选取最困难的样本 Masked Language Modeling:\n屏蔽掉一些词，通过从图片模态信息中预测掉被屏蔽的词(多分类Loss)\n这里也借助了图像的信息去更好的恢复被mask掉的单词\n【这里只对匹配的对计算 掩码Loss】\n目的：缓解noisy web data的不足，真正的label不一定有momentum的好\n真实label不一定比momentum model给出的predict label好，=\u003e 使用KL散度进行约束 一致性\n最大化互信息视角解释：\n在自监督学习中，a 和 b 是同一图像的两个增强。 在视觉语言表示学习中，我们将 a 和 b 视为捕获其语义的图像文本对的不同变体。 我们的目标是学习对观点变化不变的表征。\n最小化Loss =\u003e 最大化互信息的下限(最大化了图像-文本对的**不同“视图”**之间的互信息（MI）的下限)\nInfoNCE Loss\nImage-Text Contrastive Loss\n最大化Text和Image中的互信息， ITC 将两个单独的模态（即 I 和 T）视为图像-文本对的两个视图\nMLM:\nMLM 将图像-文本对的两个视图视为：(1) 随机选择的单词标记，以及 (2) 图像 + 带有该单词屏蔽的上下文文本。\nInstance discrimination - 2018 Unsupervised Feature Learning via Non-Parametric Instance Discrimination\n首次提出个体判别任务！\n观察：\n在监督学习中。在预测’花豹’时，预测概率除了’花豹’，剩余预测得分比较高的是’美洲虎’、‘猎豹’； 最不相似的是’救生艇’、‘购物车’、‘书柜’;\n最高响应的类都是视觉相关的\n⭐ 并不是语义标签，而是数据本身的明显相似性使某些类比其他类更接近；\n❗❗❗ 个体判别：将类监督发挥到了极致，并学习了区分各个实例的特征表示。\n这些观察结果表明，典型的判别学习方法可以自动发现语义类别之间的明显相似性，而无需明确指导这样做。\n我们能否通过纯粹的判别学习来学习反映实例之间明显相似性的有意义的度量？ 图像本身就是独特的，并且每个图像都可能与同一语义类别中的其他图像显着不同。如果我们学会在没有任何语义类别概念的情况下区分各个实例，我们最终可能会得到一个捕获实例之间明显相似性的表示，就像类明智的监督学习如何仍然保留类之间的明显相似性一样。\n目标:\n​\t在没有监督的情况下学习嵌入函数 v = fθ(x)。 fθ 是一个具有参数 θ 的深度神经网络，将图像 x 映射到特征 v。这种嵌入将在图像空间上产生一个度量，对于实例 x 和 y, dθ(x, y) = |fθ(x) − fθ(y)|。 良好的嵌入应该将视觉上相似的图像映射得彼此更接近。 我们新颖的无监督特征学习方法是实例级区分。 我们将每个图像实例视为其自己的不同类，并训练分类器来区分各个实例类。\n方法：\n用一个memory bank存储4096个样本embed feature(128-dimention) 随着网络更新, 目的是让特征在嵌入空间中远离(每一个样本都是一个类)，学习那种有监督时类和类之间相似聚集的现象。\nBYOL - 2020/6 Bootstrap Your Own Latent A New Approach to Self-Supervised Learning\n无监督学习： { 判别式：从增强视图的表示中，他们学会区分同一图像的另一个增强视图的表示和不同图像的增强视图的表示 =\u003e 这种判别方法通常需要将增强视图的每个表示与许多反例进行比较。 生成式：通过预测同一图像的不同视图（例如，不同的随机裁剪）来学习表示 =\u003e 图像的增强视图的表示应该能够预测同一图像的另一个增强视图的表示。 } 方法：\n在线网络θ + 目标网络γ(提供回归目标，γ = α×γ + (1-α)×θ ，指数移动平均 )\nyθ是目标编码器，其余的训练好后丢掉\n⭐一张图片的两个增强表示的相同的语义 =\u003e 在高维的嵌入表示中，应该可以预测对方(相近)\nyθ：Encoder zθ：Projection head qθ：Prediction head Loss: $$ 注意图像增强t(x)和t^{}x会对等的传给online\\ net 和 target\\ net\\\\\rLoss: 1/2 × ( || q_{θ}(z_{θ}) - z^{}{ξ}||^{2} + || q{θ}(z_{θ}) - z^{`}_{ξ}||^{2} ) $$ Train: $$ θ：online\\ net\\ parameters\\ ξ：target\\ net\\ parameters\\ θ \u003c- optimizer(θ),\\ \\ \\ ξ \u003c- αξ + (1-α)θ $$\nDINO - 2021 Emerging Properties in Self-Supervised Vision Transformers\nViT最后的CLS注意力图示⭐⭐⭐\n探讨：质疑自监督学习是否为 Vision Transformer (ViT) 提供了比卷积网络 (convnets) 更突出的新属性？\n框架：(借鉴BYOL)\n教师是在训练过程中动态构建的。知识蒸馏就不再被用作自监督预训练的后处理步骤，而是直接作为自监督目标。\n其中学生和教师具有相同的架构并在训练期间使用蒸馏。\n教师在我们工作中用学生的动量平均值进行更新。\n增强策略：\n1. 多裁剪策略构建图像的不同扭曲视图或裁剪。 更准确地说，根据给定的图像，我们生成一组 V 的不同视图。 该集合包含两个全局视图 xg 1 和 xg 2 以及几个分辨率较小的局部视图。\r1. 所有的裁剪都通过学生传递，而只有全局观点通过老师传递，因此鼓励“局部到全局”的对应。\r伪代码：\n防止模型坍塌：\n1. *对动量教师输出进行居中和锐化，以避免模型崩溃。*\r2. **居中（Centering）**：对动量教师的输出进行居中操作是为了减少批次之间的偏差，增加输出的稳定性。具体做法是从每个输出中减去其均值，确保输出围绕零分布，这有助于避免网络输出在特征空间内偏向某一方向，从而降低了模型坍塌的风险。\r3. **锐化（Sharpening）**：锐化是通过增加输出分布的峰值来实现的，目的是使模型的输出更加区分明显，即使不同类别之间的区别更加清晰。这通常通过提高输出概率分布的熵来实现，比如可以采用温度调整（temperature scaling）等方法来调整概率分布，使得主要的概率值更加突出，而其他的概率值则相对降低。\r图示：\n不同颜色是不同的注意力头\n无监督注意力更能学到本质！\nSimSiam - CVPR 2021 Exploring Simple Siamese Representation Learning\n简单设计的Siamese(孪生)网络。 我们的极简主义方法的竞争力表明\n1️⃣ “没有动量编码器的 BYOL”\n2️⃣ “没有负样本的 SimCLR“ + stop-grad(⭐这个非常关键， 这个对防止模型坍塌很关键)\n方法：\n一幅图像的两个增强视图由同一编码器网络 f（主干网络加投影 MLP）处理。 然后在一侧应用预测 MLP-h，在另一侧应用停止梯度操作。 该模型最大化了双方之间的相似性。 它既不使用负对也不使用动量编码器。\n伪代码：\n消融\nLoss:\n负的余弦相似度 和 交叉熵\n相似度：\n交叉熵：\nBatchNorm的影响：\nBatchSize的影响：\n预测头的影响：\nLoss的对称性：\nsym对称；asym非对称；asym. 2×(每个图像采样两对来粗略地补偿对称性)\nHiLo Attention - NeurIPS 2022 Fast Vision Transformers with HiLo Attention\n图像中的高频捕获局部精细细节，低频关注全局结构，而多头自注意力层忽略了不同频率的特征。 因此，我们建议通过将头部分为两组来解开注意力层中的高频/低频模式，其中一组通过每个局部窗口内的自注意力对高频进行编码，另一组通过在每个局部窗口内执行全局注意力来对低频进行编码，输入特征图中每个窗口和每个查询位置的平均池化低频键和值。\n创新点：\n将自注意力分为高频和低频 高频捕捉局部精细细节（轮廓、边缘） 低频捕获整体结构or趋势 high部分是window级别的attn low部分是space reduce(经过pool)的粗糙级别的attn 图示：\n伪代码：\nHiLoAttention(): def high(x): # window-partition [batch, num_token, dim_token] =\u003e [batch, num_window, window_size, dim_token] # multi-head [batch, num_window, window_size, dim_token] =\u003e [batch, num_window, num_head, window_size, dim_head] # do attention # reshape to restore def low(x): B, L, C = x.shape x_ = x.transpose(-2, -1)\t# [batch, channel, length] x_ = self.sr_cnn(x_) # [batch, channel, _length] reduce length || avgpool or dwconv q = self.q(x).reshape() =\u003e [batch, num_head, num_token, dim_head] k, v = self.kv(x).reshape()[...] =\u003e [batch, num_head, *num_token, dim_head] # do attention # reshape to restore def forward(x): x1 = high(x) x2 = low(x) x = torch.cat([x1, x2], dim=-1) x = self.proj(x) CMT - CVPR 2022 CMT: Convolutional Neural Networks Meet Vision Transformers\n注重点：与之前基于 CNN 和基于 Transformer 的模型相比，在准确性和效率方面获得了更好的权衡。\n⭐ 由于 patch 大小固定，Transformer 很难显式地提取低分辨率和多尺度特征 =\u003e 图像是二维的（即具有宽度和高度），并且在图像中的每个像素位置都与其周围的像素有关。这种空间局部信息非常重要，例如，边缘检测、纹理分析等都依赖于这种局部关系。=\u003e Patchfiy 后削弱了pixel之间的关系，只补充了Patch间的位置信息。\nCNN、Transformer 、CNN\u0026Transformer\n在每个阶段，产生层次表示 – 金字塔结构\n定制的Stem Block内部混合CNN和MHSA ❤️ [DWConv(Skip-con) + SR-MHSA(Skip-con) + IRFFN(Skip-Conv)] Conformer - 2020 Conformer: Convolution-augmented Transformer for Speech Recognition\n集成了 CNN 和 Transformer 组件以进行端到端识别的架构\n分析：\n虽然 Transformer 擅长对远程全局上下文进行建模，但它们提取细粒度局部特征模式的能力较差； 卷积神经网络（CNN）利用局部信息，在本地窗口上学习共享的基于位置的内核，能够捕获边缘和形状等特征。使用本地连接的限制之一是需要更多的层或参数来捕获全局信息。 ⭐ 将卷积与自注意力有机结合起来。 我们假设全局和局部交互对于参数效率都很重要。 为了实现这一目标，我们提出了一种自注意力和卷积的新颖组合，将实现两全其美\narchitecture\nConvolution Module\nFeed Forward Module\nPathFormer - ICLR 2024 PATHFORMER: MULTI-SCALE TRANSFORMERS WITH ADAPTIVE PATHWAYS FOR TIME SERIES FORECASTING\narchitecture\nDual-Attention\nMobile-Former - CVPR 2022 Mobile-Former: Bridging MobileNet and Transformer\n动机：\n如何设计高效的网络来有效地编码本地处理和全局交互？\n最近工作：串联组合卷积和视觉变换器的好处，无论是在开始时使用卷积还是将卷积交织到每个变换器块中\n视觉变换器（ViT）[10,34]展示了全局处理的优势，并实现了比 CNN 显着的性能提升，如何在计算资源或者参数量有限的情况下充分挖掘结合两者的优势，=\u003e parameters efficient\n贡献：\n并行设计 + 双路桥接； 利用了 MobileNet 在本地处理和 Transformer 在全局交互方面的优势；实现局部和全局特征的双向融合 全局Token只有初始化为0的很少的Token，利用Cross Attention 进行交互 ⭐大概就是在MobileNet为主干的基础上添加ViT全局Token的信息注入 architecture\nInteraction\npseudo code\nq = FC(token).view(), k, v = x.view()\t# shape =\u003e batch, num_token, dim_token; do Local2Global-CrossAttn() token =\u003e MHSA() x = MobileNetBlock() q = x.view(), k, v = FC(token).view() do Global2Local-CrossAttn() ViT Adapter - ICLR 2023 VISION TRANSFORMER ADAPTER FOR DENSE PREDICTIONS\n动机\n在不改变原有ViT的基础上(利用大规模预训练参数)，添加辅助器帮助Transformer学习弱项；【使用现成的预训练 ViT 适应密集的预测任务】 ViT 单尺度和低分辨率表示的弱点 =\u003e 注入一些多尺度特征(CNN)给单尺度的ViT ​\t…表明卷积可以帮助 Transformer 更好地捕获局部空间信息，对与补丁嵌入层并行的图像的局部空间上下文进行建模，以免改变 ViT 的原始架构。\n贡献\nViT Adapter: [Spatial Prior Module, Spatial Feature Injector, Multi-Scale Feature Extractor] paradigm compare\narchitecture\n（c）用于根据输入图像对局部空间上下文进行建模的空间先验模块， Adapter: Spatial feature token set; ViT: origin feature map; （d）用于将空间先验引入ViT的空间特征注入器 （e）用于从单个图像重新组织多尺度特征的多尺度特征提取器 -ViT 的尺度特征 采用稀疏注意力来降低计算成本\npseudo code\n# Spatial prior module X_vit = ResnetBlock(x) x1 = PointConv(Conv(X_vit))\t# down-sample: HW/8^2 and project channel to D dimension x2 = PointConv(Conv(x2))\t# down-sample: HW/16^2\tto D dimension x3 = PointConv(Conv(x3))\t# down-sample: HW/32^2\tto D dimension X_vit = torch.cat([x1, x2, x3], dim=num_token) # injector // spatial feature to ViT q = FC(X_vit).view(...), k, v = FC(X_spm).view(...)\t# spm: spatial prior module do Cross-Attn() # Multi-Scale Feature Extractor q = FC(X_spm).view(...), k, v = FC(X_vit).view(...)\t# analyze：\n⭐ 研究表明，ViT 呈现出学习低频全局信号的特征(整体、模糊和粗糙)，而 CNN 则倾向于提取高频信息（例如局部边缘和纹理） visualize\n傅里叶变换特征图的傅里叶频谱和相对对数幅度（超过 100 张图像的平均值） =\u003e 表明 ViT-Adapter 比 ViT 捕获更多的高频信号\n我们还可视化了图5（b）（c）中的stride-8特征图，这表明ViT的特征是模糊和粗糙的\nInception Transformer - NeurIPS 2022 Inception Transformer\n动机：\nTransformer 具有很强的建立远程依赖关系的能力，但无法捕获主要传达局部信息的高频；\nViT 及其变体非常有能力捕获视觉数据中的低频，主要包括场景或对象的全局形状和结构，但对于学习高频（主要包括局部边缘和纹理）不是很强大(CNNs很擅长，它们通过感受野内的局部卷积覆盖更多的局部信息，从而有效地提取高频表示)；\n最近的研究[21-25]考虑到CNN和ViT的互补优势，将它们集成起来。 一些方法[21,22,24,25]以串行方式堆叠卷积层和注意力层，以将局部信息注入全局上下文中。 不幸的是，这种串行方式仅在一层中对一种类型的依赖关系（全局或局部）进行建模，并且在局部性建模期间丢弃全局信息，反之亦然。❤️ 每个模块都不够全面=\u003e模型要么只有局部感知能力，要么只有全局建模能力 =\u003e 在ECG中，有些疾病不仅仅是局部或全局的病理特征，而且是节律异常伴随着波形形态异常；从这一角度出发，我们希望能够充分的利用Transformer的全局依赖感知能力和CNN的强大的局部感知能力，交互融合有力结合两者优势； 【就像在人类视觉系统中一样，高频分量的细节有助于较低层捕获视觉基本特征，并逐渐收集局部信息以对输入有全局的理解】\n层级式网络，多尺度分辨率特征图，每部分均能全局+局部感知；并且设计频率斜坡结构 =\u003e 底层更注重高频信息(细节信息，局部模式、纹理边缘)；高层更注重低频信息(整体轮廓，全局)\n创新点：\nTransformer中的Multi-Head Self-Attention =\u003e Inception Mixer ; 按channel分两组：1. 低频组；2. 高频组； 低频组 池化稀疏注意力，但仅最低两块用； 高频组 [MaxPool, DWConv]； 频率斜坡结构: 高频组\u003e低频组 =\u003e 高频组\u003c低频组 底层在捕获高频细节方面发挥更多作用，而顶层在建模低频全局信息方面发挥更多作用 architecture\nInception Mixer\npseudo code\n# x : [batch, channel, width, hight] x_h, x_l = torch.chunk(x, chunks=2, dim=1) x_h1, x_h2 = torch.chunk(x_h, chunks=2, dim=1) y_h1 = FC(MaxPool(x_h1)) y_h2 = DWConv(FC(x_h2)) y_l = MSA(AvePooling (X_l)) Y = X + ITM(LN(X)) # ITM : Inception Mixer H = Y + FFN(LN(Y)) 局部|高频 \u0026 全局|低频 - 傅里叶谱\nTransNeXt - CVPR 2024 TransNeXt: Robust Foveal Visual Perception for Vision Transformers\nanalysis\n目前的稀疏Attention： Local Attention[限制计算量，n×固定窗口计算量]: window-level attention, =\u003e cross-window attn information exchange 需要堆叠很深才能实现全局感知 patially downsamples【降低计算的Token数量】: pool or dwconv =\u003e 信息丢失问题； 【细粒度(丢失)=\u003e 粗粒度】 motivation\n⭐ 观察：生物视觉对视觉焦点周围的特征具有较高的敏锐度，而对远处的特征具有较低的敏锐度。 结合window-level attn \u0026 spatial downsample attn，临近的token执行pixel-level attn，稍远的区域执行pool-level attn, 实现视觉仿生聚焦attn contribution\nfocus attn 【局部细粒度，全局粗粒度】\nfocus attn 升级 aggregated attention 【QKV 注意力、LKV 注意力和 QLV 注意力统一】\nQLV 与传统的 QKV 注意力机制不同，它破坏了键和值之间的一对一对应关系，从而为当前查询学习更多隐含的相对位置信息。 LKV：增强表达能力，通过引入可学习的键和值，模型可以学习到更多有用的特征，增强了对复杂关系的建模能力 length-scaled cosine attention 【双路注意力concat经过同一个softmax】\nconvolutional GLU替换MLP\narchitecture\nleft figure: focus attn; right figure: aggregated attn(add QKV-attn、LKV-attn、QLV-attn)\n共享同一个Softmax（作用可能是这里进行多注意力的制约交互）\nConvGLU: 卷积 GLU (ConvGLU) 中的每个标记都拥有一个独特的门控信号，基于其最接近的细粒度特征。 这解决了SE机制中全局平均池化过于粗粒度的缺点。 它还满足了一些没有位置编码设计、需要深度卷积提供位置信息的ViT模型的需求。\nEfficientViT - CVPR 2023 EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention\n权衡性能和代价\nmotivation\n发现现有 Transformer 模型的速度通常受到内存低效操作的限制，尤其是 MHSA 中的张量整形和逐元素函数; 虽然Transformer性能很好，但是代价很高，不适合实时应用; =\u003e优化;\n发现注意力图在头部之间具有高度相似性，导致计算冗余。// 显式分解每个头的计算可以缓解这个问题，同时提高计算效率\ncontribution\n设计了一种具有三明治布局的新构建块，即在高效 FFN 层之间使用单个内存绑定的 MHSA，从而提高内存效率，同时增强通道通信; 提出了一个级联的组注意力模块，为注意力头提供完整特征的不同分割，这不仅节省了计算成本，而且提高了注意力多样性。 architecture\nToken Interaction：DWConv，增强局部交互能力，引入局部结构信息的归纳偏差来增强模型能力\n三明治结构中，局部建模和全局Attn, 即[Token Interaction, FFN][Cascaded Group Attention][Token Interaction, FFN]堆叠不是1:1:1, 而是N:1:N, Why? because Attention 计算量太大了，能少用就少用。\nCascaded Group Attention pseudo code\nfeature_group = x.chunk(len(self.qkv_group), dim=1) # split head feature = feature_group[0]\t# first head feature_out = [] for i, qkv in enumerate(self.qkv_group): if i \u003e 0: feature = feature + feature_group[i] # Cascaded Group Q, K, V = qkv(feature).view().permute() # shape: B, H, N, C/H Q = DWConv[i](Q) # enhance Query Token Set out = (Q@K.transpose(-2, -1) × scale)@V feature_out.append(out) out = torch.cat(feature_out, 1) FC(out) 不同于传统Attn，这里先分头再线性映射，头的信息会越来越丰富。 并且实现中(Cascaded Group Attention in Window-level Attention )\nEMO - ICCV 2023 Rethinking Mobile Block for Efficient Attention-based Models\n目标：轻量级 CNN 设计高效的混合模型，并在权衡精度、参数和 FLOP 的情况下获得比基于 CNN 的模型更好的性能\n出发点：我们能否为仅使用基本算子的基于注意力的模型构建一个轻量级的类似 IRB 的基础设施？\n基本算子结构对比\nMulti-head self attention: 线性映射qkv，MHSA, 投影回来\nFeed Forward Network: Linear-Linear\nInverted Residual Block: Conv1x1-DWConv-Conv1x1\n=\u003e 综合考量 提出基本算子Meta Mobile Block\nMeta Former Block vs Inverted Residual Block\n更加细致的抽象\niRMB（Inverted Residual Mobile Block）\n！！！ 由于 MHSA 更适合对更深层的语义特征进行建模，因此我们仅在之前的工作之后的 stage-3/4 中打开它 。\nConv:\nBN+SiLU与DWConv结合； W-MHSA(window-level attention 更加高效):\nLN+GeLU与EW-MHSA结合。 解释EW-MHSA 因为iRMB，会先升维，导致MHSA计算量变高， Q,K维度不变，而V的维度变长了，拿attn-score加权求和时应用的是扩展V ​\n深度设计，灵活的设计\nFocal Attention - NeurIPS 2021 Focal Attention for Long-Range Interactions in Vision Transformers\n观察：\n图 1：左：DeiT-Tiny 模型 [55] 第一层中给定查询块（蓝色）处三个头的注意力图的可视化。 右图：焦点注意力机制的说明性描述。 使用三个粒度级别来组成蓝色查询的注意区域。\n创新点：\nClose =\u003e Far Fine =\u003e Coarse 图示：\n伪代码：\n1. 使用torch.roll 再按窗口划分 =\u003e 收集细粒度周边Token, 再使用mask掩码掉多余不需要的Token 2. 先分窗口，执行窗口内Pool，生成超粗粒度Token Q: 窗口内Token K, V: 窗口内Token + 周边细粒度Token + Pool-Token CloFormer - CVPR 2023 Rethinking Local Perception in Lightweight Vision Transformer\narchitecture\n局部+全局 感知并行\n局部感知，类似于CNN中的卷积注意力用在自注意力分支中\nFFN 内追加局部感知增强模块\nMetaFormer - 2023 我们并不试图引入新颖的令牌混合器，而只是将令牌混合器指定为最基本或常用的运算符来探测 MetaFormer 的能力\n⭐探索Meta Block的潜力！\nX = X + TokenMixer(Norm(X)) X = X + ChannelMixer(Norm(X)) MaxViT - ECCV 2022 MaxViT: Multi-Axis Vision Transformer\n动机：解决Self-Attention平方复杂度问题\n框架：\n1️⃣ MobileNetV2中的倒残差块 =\u003e 提供增强局部感知 \u0026 隐式编码位置信息\n2️⃣ Block-Attention =\u003e window-level attention ❌ 限制感受野 ⭐ 降低计算量\n3️⃣ Grid-Attention =\u003e 感受野遍及全局的扩张卷积做法 1. 分窗口 2. 收集每个窗口相同次序的Token成组. 3. 组内计算注意力\nAttention illustration\nSegment Anything 即时分割\n模型组件\n模型框架：\nprompt encoder：\n​\tSparse prompts: point: point =\u003e 256 dimensional vectorial embedding. 这个使用index去索引位置嵌入像Swin-T， foreground or backgroud embedding（自学习）. to add together. box: 左上角位置编码 + 左上角的学习嵌入；左上角位置编码+“右下角”的学习嵌入 text: clip的text encoder. dense prompts: mask: CNN =\u003e 256 特征向量。有则加mask，没有就加可学习的表示无mask的学习嵌入 mask decoder：\nCiT - ICCV 2021 Incorporating Convolution Designs into Visual Transformers\n局部增强\n就是Inverted Resiual Block [Conv1×1 =\u003e Depth-wise Conv3×3 =\u003e Conv1×1] 对Token进行局部信息增强\n框架：\n⭐利用了每个Stage中的Class Token，这样可以有层级式信息，而且梯度会通过这个CLS Token直接传递给前面部分\nBi-Interaction Light-ViT Lightweight Vision Transformer with Bidirectional Interaction\n图示：\n框架：\n⭐想法很超前⭐\n局部与全局的相互调制\nUniRepLKNet UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition\n⭐超大卷积核 Conv winwinwin\n当我们向小内核 ConvNet 添加 3×3 卷积时，我们期望它同时产生三种效果\n使感受野更大， 增加空间模式的抽象层次（例如，从角度和纹理到物体的形状） 通过使其更深，引入更多可学习的参数和非线性来提高模型的一般表示能力 相比之下，我们认为大内核架构中的这三种影响应该是解耦的，因为模型应该利用大内核的强大优势——能够看到广泛而不深入的能力\n由于在扩大 ERF(感受野) 方面，增加内核大小比堆叠更多层更有效，因此可以使用少量大内核层构建足够的 ERF，从而可以节省计算预算以用于其他高效结构 在增加空间模式的抽象层次或总体上增加深度方面更有效。\n框架：\n重参数化\n块设计\nEdgeViTs - ECCV 2022 EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers\n架构：\n# 类似MobileViT\nX = LocalAgg(Norm(X)) + X # Y = FFN(Norm(X)) + X Z = LocalProp(GlobalSparseAttn(Norm(Y))) + Y Xout = FFN(Norm(Z)) + Z 图示： （选举 =\u003e 精英 =\u003e 分发）\nKNN(K-Nearest Neighbors) K-近邻算法\n核心思想：相似的样本具有相似的输出。\n=\u003e KNN通过计算输入样本与训练数据集中所有样本的距离，找到距离最近的K个样本，然后根据这些样本的类别来决定输入样本的类别\n主要步骤:\n选择K值：选择一个正整数K，代表你要比较的邻居数量。\n计算距离：对每个待分类样本，计算它与训练数据集中所有样本的距离。常用的距离度量有欧氏距离、曼哈顿距离和余弦相似度等。\n$$ \\textbf{欧氏距离}: d(x, x_i) = \\sqrt{\\sum_{j=1}^{m}(x_j-x_{ij})^2} $$ 选择最近的K个邻居：根据计算得到的距离，从训练数据集中选择距离待分类样本最近的K个样本\n投票或加权：在分类任务中，K个邻居中最多的类别即为待分类样本的预测类别。在回归任务中，可以对K个邻居的数值进行平均或者加权平均。\n输出结果：输出投票或加权后的结果作为待分类样本的预测结果。\nShuffleNet - CVPR 2018 出发点：构建高效模型\n⭐V1⭐\n缺点发现：\n=\u003e Conv1x1 _ Norm+ReLU # Point-wise =\u003e DWConv3x3 _ Norm # Depth-wise =\u003e Conv1x1 _ Norm+ReLU # Point-wise # 为了高效 =\u003e 只是将Conv3x3 =\u003e Conv1x1 # =\u003e 但是Conv1x1占据了93.4%的乘法-加法 # =\u003e 目标，优化PWConv 操作图示：\n卷积分组减少计算量 （⭐优化组间通信⭐）\nShuffleNet Basic Block\n(a) ResNet bottleneck unit \u003c= DWConv (b) 优化PWConv（Group Conv），并且Channel Shuffle，执行Group communication # 无下采样，输入输出shape一致 (c) 恒等映射占据一半的Channel，另一半精修过的Feature Map ⭐V2⭐\n=\u003e 分析各个类型操作占据的计算成本\n除了FLOPS指标，吞吐量和速度更为直接直观，符合真实贴切\n(使用FLOP作为计算复杂性的唯一度量是不够的，并且可能导致次优设计)\n(具有相同FLOP的操作可能具有不同的运行时间)\n访存时间 \u003c= 并行度 \u003c= (a) basic shuffle net block -v1 (b) basic shuffle net block with downsample -v1 (c) v2 (d) v2 with downsample # shape equivalence x.shape = (B, 64, H, W) x1, x2 = channel-split(x) # =\u003e (B, 32, H, W), (B, 32, H, W) # x1 作为恒等映射，残差连接？ 特征复用？ out = concat(x1, block(x2)) out = channel-shuffle(out) # with downsample x.shape = (B, 64, H, W) x1, x2 = x, x # =\u003e (B, 64, H, W), (B, 64, H, W) out = concat(branch1(x1), branch2(x2)) out = channel-shuffle(out) 特征复用示意图：\n$$ l1-norm = \\sum_{i=1}^n{|v_i|} $$ 相邻层更高效\n准确率参数贡献✔️\nRepVGG - ReParams - CVPR 2021 结构分析\n内存分析：\n=\u003e 权衡：性能和计算内存成本\nTrain：多分支结构，性能好\nTest： 单分支结构，速度快，内存少\n⭐⭐⭐重参化：\n细节：\n举例第一个卷积后的元素\nAgent Attention - ECCV 2024 Attn图示：\n做法：\ncode：\nq, k, v = qkv[0], qkv[1], qkv[2] agent_token = pool(q) agent_attn = softmax(agent_token * scale @ k.T + position_bias) agent_v = agent_attn @ v q_attn = self.softmax((q * self.scale) @ agent_tokens.T + agent_bias) x = q_attn @ agent_v x = x + self.dwc(v) # 复杂度 o(N * K) + o(N * K) 享受 =\u003e 高表达性和低计算复杂度的优势\nPatchTST - ICLR 2023 Patchify .\n有监督 =\u003e 可以重叠 自监督 =\u003e 不可以重叠，避免网络可以从重叠区域走捷径学习 多变量独立：\n每个时间序列将有自己的潜在表示，通过共享权重机制交叉学习 ？？？\n共享Encoder权重，不同通道使用相同的模型参数。这种方法允许模型在不同的任务之间共享知识\nOverview\nx = [batch, channel, length] x = [batch, channel, num_token, len_token] x = [batch*channel, num_sample, dim_hidden] =\u003e Transformer Encoder but residual attn # =\u003e Linear head =\u003e CrossFormer - ICLR 2023 TRANSFORMER UTILIZING CROSSDIMENSION DEPENDENCY FOR MULTIVARIATE TIME SERIES FORECASTING\n创新点：\n显示建模时间依赖关系 + 通道依赖关系 两阶段注意力 （时间：MHSA，通道：Router MHSA） 嵌入方式 and 依据：\n自注意力呈现小局部一致性，一坨而不是一个。\n保持通道独立 ✔️ 注意力优化 ✔️ TwoStageAttention\n# Step 1. Time Dependency x = [batch, channel, length] x = [batch, channel, num_patch, dim_patch] # \u003c= DSW (Dimension-Segment-Wise) x = [batch*channel, num_patch, dim_patch] y = TransformerEncoer(x, x, x) # \u003c= capture time dependency # Step 2. Channel Dependency y = [batch, channel, num_patch, dim_patch] y = [batch*num_patch, channel, dim_patch] router = [1, num_router, dim_patch] # \u003c= router router =\u003e repeat =\u003e [*, num_router, dim_patch] z = TransformerEncoder(router, y, y) # \u003c= capture channel dependency z = TransformerEncoder(y, router, router) # save per stage output =\u003e Unet Decoder =\u003e to predict ECG与多变量的异同：\n相似点 不同通道贡献不同 =\u003e DSW-patch, 能够更加细粒度编码局部波形 == 多变量(通道) patch化的成功！！！ 不同点 由于是对心脏电活动的同一时间不同角度的观察 =\u003e 病理位置相同 =\u003e是否能够通过共享策略 降低计算成本🤔❓ Informer - AAAI 2021 Best – TopK-Q\n观察：(Q\u0026K 是等价的)\n注意力呈现长尾分布：\nQuery分为活跃于惰性Token\n衡量指标：\n注意力优化：\nQ, K, V # probe K = [batch, num_head, len_token, dim_head] K_sample = [batch, num_head, random_len, dim_head] Q_K_sample = [batch, num_head, len_token, random_len] M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K) # 衡量指标 M_top = M.topk(n_top, sorted=False)[1] Q_reduce = Q[:, :, M_top, :] attn_active = softmax(torch.matmul(Q_reduce, K.transpose(-2, -1))*scale) contex = V.sum(-2).expand() =\u003e [batch, num_head, len_token, dim_head] # 均匀分布的就直接取V的均值 contex[:, :, M_top, :] = attn_active@V 结构优化：\n# Encoder: for num_layer ...: x = attn(x) x = conv_layer(x) # \u003c- maxpool(act(norm(conv()))) return enc_out # Decoder: cross = enc_out x = # 预测引导 for num_layer ...: x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask) # Note：Masked MHSA-ProbeAttn 框架逻辑\nClass Exp_Basic(Object): def __init__(): def _build_model(): def _acquire_device(): def _get_data(): def train(): def vali(): def test(self): Class Exp_Model(Exp_Basic): def _select_optimizer(): def _select_criterion(): def _process_on_batch(): # data_loader.py Class Dataset_XXX(Dataset): def __init__(): def __read_data__(self): def __getitem__(self, index): def __len__(self): # main.py parser = argparse.ArgumentParser(description='[Model] Task') ... args = parser.parse_args() setting = ...args exp = Exp_Model(args) exp.train(setting) exp.test(setting) Adaptive Token Dictionary - CVPR2024 Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary\n扩展局部窗口的限制\nwindow-based self-attention token dictionary cross-attention =\u003e Attention(Q(XW),K(TW), V(TW)) 基于2的Attn，将token map排序分group(类)，进行group内部的Attention Architecture\nPoly Kernel Inception - CVPR 2024 Poly Kernel Inception Network for Remote Sensing Detection\n遥感图像中的目标检测面临着多种挑战，包括目标尺度变化大、测距环境多样等。现有的方法试图通过大核卷积或扩张卷积来扩展脊柱的空间感受野来解决这些挑战。然而，前者通常会引入相当大的背景噪声，而后者则有生成过度稀疏的特征表示的风险。本文提出了一种多核初始化网络（PKINet）来解决上述问题。PKINet采用无膨胀的多尺度卷积核来提取不同尺度的对象特征并捕获局部上下文。此外，一个上下文锚注意（CAA）模块并行引入捕获远程上下文信息。\n不同尺度-局部上下文 并行引入捕获远程上下文信息 # 十字架型汇聚 =\u003e 近似标准的DWConvKxK =\u003e 降低参数量 agg = Conv1x1(AvgPool(X)) agg = Conv1x1(DWConvKx1(DWConv1xK(agg))) attn = Sigmoid(agg) DANet - CVPR 2019 Dual Attention Network for Scene Segmentation\nAt the end of the model, we use the dual attention mechanism to explicitly capture position and channel dependencies.\nclass PAM_Module(Module): \"\"\" Position attention module\"\"\" #Ref from SAGAN def __init__(self, in_dim): super(PAM_Module, self).__init__() self.chanel_in = in_dim self.query_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1) self.key_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1) self.value_conv = Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1) self.gamma = Parameter(torch.zeros(1)) self.softmax = Softmax(dim=-1) def forward(self, x): \"\"\" inputs : x : input feature maps( B X C X H X W) returns : out : attention value + input feature attention: B X (HxW) X (HxW) \"\"\" m_batchsize, C, height, width = x.size() proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1) proj_key = self.key_conv(x).view(m_batchsize, -1, width*height) energy = torch.bmm(proj_query, proj_key) attention = self.softmax(energy) proj_value = self.value_conv(x).view(m_batchsize, -1, width*height) out = torch.bmm(proj_value, attention.permute(0, 2, 1)) out = out.view(m_batchsize, C, height, width) out = self.gamma*out + x return out class CAM_Module(Module): \"\"\" Channel attention module\"\"\" def __init__(self, in_dim): super(CAM_Module, self).__init__() self.chanel_in = in_dim self.gamma = Parameter(torch.zeros(1)) self.softmax = Softmax(dim=-1) def forward(self,x): \"\"\" inputs : x : input feature maps( B X C X H X W) returns : out : attention value + input feature attention: B X C X C \"\"\" m_batchsize, C, height, width = x.size() proj_query = x.view(m_batchsize, C, -1) proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1) energy = torch.bmm(proj_query, proj_key) energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy attention = self.softmax(energy_new) proj_value = x.view(m_batchsize, C, -1) out = torch.bmm(attention, proj_value) out = out.view(m_batchsize, C, height, width) out = self.gamma*out + x return out MixNet MixConv: Mixed Depthwise Convolutional Kernels\nconvulution =\u003e capture local pattern\nearly stages: edges later stages: objects⭐ 这项研究表明了单个内核大小的局限性：我们既需要大内核来捕获高分辨率模式，也需要小内核来捕获低分辨率模式，以获得更好的模型精度和效率\n在单一机制下实现多种效果，进行增强\nMultimodal Learning Multimodal Learning With Transformers: A Survey\n融合策略\nCF-ViT CF-ViT: A General Coarse-to-Fine Method for Vision Transformer\n对于分类任务 - 不需要那么精细的patch\n两步策略：\n粗粒度patch=\u003eViT =\u003e 预测得分 =若得分小于设定的置信度\u003e 将重要区域细分 =ViT\u003e 最终预测 特征复用\n不重要区域 =\u003e 大尺度粗略的Patch (可能有不相关的背景干扰) 重要区域 =\u003e 小尺度精细的patch(更多边缘细节) \u003c= 第一阶段粗略的Patch充当区域嵌入 利用ViT中[CLS] Token与其他Token的Attn累计区域的重要性\nGlobal-Attn = αAttn_l + (1-α)Attn_l+1\nTraining\nloss = CE(pf, y) + KL(pc,pf)\n训练时每次均进行Patch精细推理。使用精细模型指导粗略Patch推理\nDual Aggregation Transformer Dual Aggregation Transformer for Image Super-Resolution\nMotivation: 现有方法利用自我注意沿着不同的维度，空间或通道，并取得了令人印象深刻的性能。这启发我们将Transformer中的两个维度结合起来，以获得更强大的表示能力。\nDual Vision Transformer 研究全局语义和更精细的像素级特征之间的依赖关系 =\u003e pixel-level token \u0026 semantic token\n分解和集成的全局语义和本地功能\nConformer: ResNet + ViT 并行结构 =\u003e 同时保留局部和全局特征 (保持CNN和ViT架构的优势)\nTwins： [Local-Global] [LSA-FFN] =\u003e [GSA-FFN]\nwindow-self-attention =\u003e global-self-attention\nMSG-Transformer 架构\n有趣点 - Shuffle-Net ?\n局部信使 - 传递信息\nDilateFormer IEEE TRANSACTIONS ON MULTIMEDIA – sci-1\noverview\nnovel\n不同注意力头部，进行细微的调整\nScopeViT Pattern Recognition\nArchitecture\nnovel\n串行交叉：[多尺度, 多份KV]+[dilated Attention] $$ 𝐐 = 𝑋𝐖_𝑄,𝐊_𝑖 = 𝑃_𝑖𝐖^𝐾_𝑖, 𝐕_𝑖 = 𝑃_𝑖𝐖^V_𝑖 $$ 1 Query不变， KV通过多个不同内核大小的DWConv生成多尺度 KV (粗粒度)\n2 Stride Attention (细粒度)\nFastViT - ICCV overview\nStem:\nreparams-conv 前三阶段：\nx = x + BN(DWConv(X)) # re-params x = x + (DWConv-\u003eBN-\u003eConv1x1-\u003eGELU-\u003eConv1x1) # ConvFFN 最后阶段：\nx = x + DWConv(X) # CPE convolution position embedding x = x + BN(Attention(X)) # Attention x = x + (DWConv-\u003eBN-\u003eConv1x1-\u003eGELU-\u003eConv1x1) # ConvFFN Integration of CNN + Attention Revisiting the Integration of Convolution and Attention for Vision Backbone\nnovel\nConv-part: [Conv1x1-\u003eDWConv5x5-\u003eConv1x1] =\u003e X_conv # ConvFFN ? Attn-part: 1. [聚簇] Clustering：X -\u003e pooling -\u003e cluster 2. [提炼] cluster@X.T -\u003e score@X -\u003e cluster$ {这里用点积相似度举例} 3. [全局] cluster$ -\u003e MHSA -\u003e cluster$ 4. [分发] cluster$ -\u003e cluster@score.T =\u003e X_attn Y = X_conv + X_attn RepNeXt overview\nToken-Mixer: 1. nn.Identity() 2. DWConv3x3 + (DWConv1x3 + DWConv3x1) 3. DWConv7x7 + DWConv3x5 + DWConv5x3 + (DWConv1x5 -\u003e DWConv5x1) + (DWConv1x7 -\u003e DWConv7x1) 4. (DWConv1x11 -\u003e DWConv11x1) nn.Conv2d(in_channels, out_channels, kernel_size=(3, 5), padding=(1, 2), bias=bias, stride=stride) # Fusion =\u003e 重参数化融合 InceptionNeXt\nBlock: [Token-Mixer -\u003e Norm -\u003e FFN] 2025/1/14 Tidying up\nFish-Speech Tech-report Text-to-Speech End2End Model\n两阶段训练策略：\nStage 1:\n​\tAudio:Mel Spectrogram =\u003e 【Encoder】 =\u003e Embedding =\u003e Quantize Tokens =\u003e 【⭐Decoder⭐】=\u003e Audio\n​\t⭐重构目标⭐\nStage 2:\n​\tText:Quantize Tokens =\u003e 【✨AR Model✨】=\u003e Quantize Tokens\n​ ⭐Text:Audio一致性 + 自回归预测Next⭐\nInference:\n​\tText:Prompt-Tokens =\u003e 【✨AR Model✨】=\u003e Quantize Tokens =\u003e 【⭐Decoder⭐】=\u003e Audio\nVector Quantize Tech:\nExample：\n有一组连续的温度数据（如 20.3°C, 21.7°C, 22.5°C, 19.8°C），你想将其离散化为几个类别: 1.低温: 15°C - 20°C 2.中温: 20°C - 25°C 3.高温: 25°C - 30°C =\u003e 20.3°C → 中温 21.7°C → 中温 22.5°C → 中温 19.8°C → 低温 假设编码本有 512 个向量 Shape:[512, dim] Encoder得到的Embedding Shape:[T, dim] 对于 Encoder 输出的每个时间步的特征向量，VQ 会找到编码本中与之最接近的向量，并用其索引表示。 最终输出是一个离散的索引序列，例如 [42, 123, 87, ...]，每个索引对应编码本中的一个向量。 编码本随机初始化，在训练过程中，编码本会通过梯度下降和优化算法（如 Adam）不断更新： 最近邻搜索 =\u003e 量化误差计算 =\u003e 梯度更新,BP 编码本的作用 =\u003e 降维与压缩 + 离散化表示 + 提升生成质量(通过离散化减少生成过程中的模糊性，提升生成语音的自然度s) Blip Image-2-Text 任务之一 Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation Architecture\n相同颜色共享参数\nStage 1：\nImage =\u003e 【Image Encoder:ViT】 =\u003e image Embedding\nText =\u003e 【Text Encoder:Bert】 =\u003e text Embedding\n目标：图文一致性, 训练Encoder\nStage 2：\nImage =\u003e 【Image Encoder:ViT:Freeze🥶】 =\u003e image Embedding\nText =\u003e 【Text Encoder:Bert:Freeze🥶 + Cross-Attention:image Embedding🥵】 =\u003e Linear:2class\n目标：图文是否匹配-2分类, 训练Cross-Attention部分\nStage 3:\nImage =\u003e 【Image Encoder:ViT:Freeze🥶】 =\u003e image Embedding\nText =\u003e 【Text Decoder:GPT🥵 + Cross-Attention:image Embedding:Freeze🥶】 =\u003e Linear:multi-class\n目标：Image Embedding + text 自回归预测Next\nInference\nImage =\u003e 【⭐Image Encoder⭐】 =\u003e image Embedding\nPrompt-Text =\u003e 【⭐Text Decoder:GPT⭐ + Cross-Attention:image Embedding:⭐】 =\u003e Linear:multi-class =\u003e Next-Token\n实现Image =\u003e Text\nViT with Deformable Attn Vision Transformer with Deformable Attention\n全部采样点如下：\n高得分key采样点如下\n# 1. 生成 query q = Conv1x1(X) offset = ConvKxK(q).conv1x1()=\u003e // 将通道映射为2，并且为了提高采样点的多样性，将通道分组，每组获取不一样的信息。 reference = 规整的网格 pos = reference + offset // 固定点 + 偏移 x_sampled = F.grid_sample(X, pos) k = Conv1x1(x_sampled) v = Conv1x1(x_sampled) MHSA(q, k, v) ⭐⭐⭐\nPVT下采样技术导致严重的信息丢失❗，而Swin-T的shiftwindow注意力导致感受野的增长要慢得多❗，这限制了对大型物体建模的潜力。Deformable DETR已经通过在每个尺度上设置Nk = 4的较低数量的键来减少这种开销，并且作为检测头工作良好，但是由于不可接受的信息丢失，在骨干网络中关注如此少的键是不好❗\n这不就是步幅Attention，添加了可变嘛\nBEVFormer BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers\n网络架构信息流思路：\n具体：\n一组可学习的BEV Queries，二维网格，模拟鸟瞰图； Spatial Cross Attention，每个视图经过backone提取，拿其中多个层级的输出，拼接为多尺度特征(多个层的特征图，校准通道)。然后每个位置的q，只查询对应几个视图的周边几个k； Temporal Attention，t时刻的BEV中的q，查询t-1时刻，相应位置周边的几个k。 这里的t-1时刻的BEV特征，需要通过一个角度还是啥校准空间对齐； Deformable DETR architecture figure：\nAttn figure：\n# 伪代码 - 单尺度的 import torch import torch.nn as nn import torch.nn.functional as F class DeformableAttention(nn.Module): def __init__(self, embed_dim, num_heads, num_points): super().__init__() self.embed_dim = embed_dim self.num_heads = num_heads self.num_points = num_points # 用于预测采样偏移的线性层 self.offset_proj = nn.Linear(embed_dim, num_heads * num_points * 2) # 用于计算注意力权重的线性层 self.attn_proj = nn.Linear(embed_dim, num_heads * num_points) # 输出投影层 self.out_proj = nn.Linear(embed_dim, embed_dim) # reference_points, 每个采样点初始，共用同一个reference_point，靠offset进行局部位置偏移 def forward(self, query, reference_points, value): B, N, C = query.shape H, W = value.shape[-2:] # 预测采样偏移 offsets = self.offset_proj(query).view(B, N, self.num_heads, self.num_points, 2) # 生成采样点 sampling_points = reference_points.unsqueeze(2) + offsets # 双线性插值采样特征值 sampled_value = F.grid_sample( value, sampling_points.view(B, -1, H, W, 2), mode='bilinear', align_corners=True ).view(B, N, self.num_heads, self.num_points, C // self.num_heads) # 计算注意力权重 attn_weights = self.attn_proj(query).view(B, N, self.num_heads, self.num_points) attn_weights = F.softmax(attn_weights, dim=-1) # 加权求和 output = (sampled_value * attn_weights.unsqueeze(-1)).sum(dim=3) output = output.view(B, N, C) # 输出投影 output = self.out_proj(output) return output ",
  "wordCount" : "29531",
  "inLanguage": "en",
  "datePublished": "2024-04-19T00:00:00Z",
  "dateModified": "2024-04-19T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "LongWei"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/learning/paper_reading/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "LongCoding's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="LongCoding&#39;s Blog (Alt + H)">LongCoding&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="LongCoding&#39;s Blog">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Timeline">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>Category</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About Me">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      深度恶习论文汇总
    </h1>
    <div class="post-meta"><span title='2024-04-19 00:00:00 +0000 UTC'>April 19, 2024</span>&nbsp;·&nbsp;59 min&nbsp;·&nbsp;LongWei

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%9b%be%e7%a4%ba" aria-label="图示">图示</a><ul>
                        
                <li>
                    <a href="#%e5%8d%b7%e7%a7%af%e6%b3%a8%e6%84%8f%e5%8a%9b" aria-label="卷积注意力">卷积注意力</a></li>
                <li>
                    <a href="#%e8%87%aa%e6%b3%a8%e6%84%8f%e5%8a%9b" aria-label="自注意力">自注意力</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%ad%a6%e4%b9%a0" aria-label="学习">学习</a><ul>
                        
                <li>
                    <a href="#norm" aria-label="Norm">Norm</a></li>
                <li>
                    <a href="#loss" aria-label="Loss">Loss</a><ul>
                        
                <li>
                    <a href="#cross-entropy" aria-label="Cross Entropy">Cross Entropy</a></li>
                <li>
                    <a href="#bce-loss" aria-label="BCE Loss">BCE Loss</a></li>
                <li>
                    <a href="#focal-loss" aria-label="Focal Loss">Focal Loss</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#%e8%ae%ba%e6%96%87" aria-label="论文">论文</a><ul>
                        
                <li>
                    <a href="#mlp-mixer---nips-2021" aria-label="Mlp-mixer - NIPS 2021">Mlp-mixer - NIPS 2021</a></li>
                <li>
                    <a href="#transformer----nips-2017" aria-label="**Transformer ** - NIPS 2017">**Transformer ** - NIPS 2017</a><ul>
                        
                <li>
                    <a href="#multi-head-self-attention" aria-label="Multi-Head Self Attention">Multi-Head Self Attention</a></li>
                <li>
                    <a href="#feed-forward-networks" aria-label="Feed-Forward Networks">Feed-Forward Networks</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%8d%b7%e7%a7%af" aria-label="卷积">卷积</a><ul>
                        
                <li>
                    <a href="#%e6%a0%87%e5%87%86%e5%8d%b7%e7%a7%af" aria-label="标准卷积">标准卷积</a><ul>
                        
                <li>
                    <a href="#%e7%93%b6%e9%a2%88%e7%bb%93%e6%9e%84---resnet" aria-label="瓶颈结构 - Resnet">瓶颈结构 - Resnet</a></li></ul>
                </li>
                <li>
                    <a href="#%e6%b7%b1%e5%ba%a6%e5%8f%af%e5%88%86%e7%a6%bb%e5%8d%b7%e7%a7%af" aria-label="深度可分离卷积">深度可分离卷积</a><ul>
                        
                <li>
                    <a href="#%e5%80%92%e6%ae%8b%e5%b7%ae%e7%bb%93%e6%9e%84---mobilenet" aria-label="倒残差结构 - MobileNet">倒残差结构 - MobileNet</a></li></ul>
                </li>
                <li>
                    <a href="#linear" aria-label="Linear">Linear</a></li></ul>
                </li>
                <li>
                    <a href="#vit---cvpr-202010" aria-label="ViT - CVPR 2020/10">ViT - CVPR 2020/10</a><ul>
                        
                <li>
                    <a href="#vision-transformer----patching" aria-label="Vision Transformer - Patching">Vision Transformer - Patching</a></li></ul>
                </li>
                <li>
                    <a href="#pvt---iccv-2021" aria-label="PVT - ICCV 2021">PVT - ICCV 2021</a><ul>
                        
                <li>
                    <a href="#%e7%89%b9%e5%be%81%e5%9b%be---patch_embed---token" aria-label="①特征图 - patch_embed -&gt; token">①特征图 - patch_embed -&gt; token</a></li>
                <li>
                    <a href="#transformer-encoder-%e5%af%b9-token-%e8%bf%9b%e8%a1%8c%e4%bf%a1%e6%81%af%e6%8f%90%e5%8f%96" aria-label="②Transformer Encoder 对 token 进行信息提取">②Transformer Encoder 对 token 进行信息提取</a></li></ul>
                </li>
                <li>
                    <a href="#cvt---iccv-2021" aria-label="CvT - ICCV 2021">CvT - ICCV 2021</a><ul>
                        
                <li>
                    <a href="#convolutional-token-embedding" aria-label="Convolutional Token Embedding">Convolutional Token Embedding</a></li>
                <li>
                    <a href="#convolutional-projection--conv%e7%94%9f%e6%88%90qkv-%e8%80%8c%e9%9d%9e%e9%80%9a%e5%b8%b8%e7%9a%84nnlinear" aria-label="Convolutional Projection &ndash; Conv生成QKV 而非通常的nn.Linear">Convolutional Projection &ndash; Conv生成QKV 而非通常的nn.Linear</a></li></ul>
                </li>
                <li>
                    <a href="#swin---iccv-2021" aria-label="Swin - ICCV 2021">Swin - ICCV 2021</a><ul>
                        
                <li>
                    <a href="#%e7%aa%97%e5%8f%a3%e7%ba%a7%e7%9a%84token%e7%89%b9%e5%be%81%e8%9e%8d%e5%90%88%e6%93%8d%e4%bd%9c--swin-transformer-block-%e6%95%b0%e9%87%8f%e4%b8%ba%e5%81%b6%e6%95%b0" aria-label="窗口级的Token特征融合操作： Swin Transformer Block 数量为偶数">窗口级的Token特征融合操作： Swin Transformer Block 数量为偶数</a></li>
                <li>
                    <a href="#%e7%9b%b8%e5%af%b9%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81" aria-label="相对位置编码：">相对位置编码：</a></li>
                <li>
                    <a href="#patch-mergeing" aria-label="Patch Mergeing">Patch Mergeing</a></li></ul>
                </li>
                <li>
                    <a href="#mobilevit---cvpr-2021" aria-label="Mobilevit - CVPR 2021">Mobilevit - CVPR 2021</a><ul>
                        
                <li>
                    <a href="#mv2-mobilenetv2-%e4%b8%ad%e7%9a%84%e5%80%92%e6%ae%8b%e5%b7%ae%e5%9d%97" aria-label="MV2: MobileNetv2 中的倒残差块">MV2: MobileNetv2 中的倒残差块</a></li>
                <li>
                    <a href="#mobilevit-block-%e9%97%b4%e6%8e%a5%e8%9e%8d%e5%90%88" aria-label="MobileViT Block:	&ndash; 间接融合">MobileViT Block:	&ndash; 间接融合</a></li></ul>
                </li>
                <li>
                    <a href="#crossvit----iccv-2021" aria-label="CrossViT  - ICCV 2021">CrossViT  - ICCV 2021</a></li>
                <li>
                    <a href="#deit----cvpr-2021" aria-label="DeiT  - CVPR 2021">DeiT  - CVPR 2021</a></li>
                <li>
                    <a href="#t2t----iccv-2021" aria-label="T2T  - ICCV 2021">T2T  - ICCV 2021</a></li>
                <li>
                    <a href="#biformer----cvpr-2023" aria-label="BiFormer  - CVPR 2023">BiFormer  - CVPR 2023</a></li>
                <li>
                    <a href="#smt---iccv-2023" aria-label="SMT - ICCV 2023">SMT - ICCV 2023</a><ul>
                        
                <li>
                    <a href="#evolutionary-hybrid-network" aria-label="Evolutionary Hybrid Network">Evolutionary Hybrid Network</a></li>
                <li>
                    <a href="#sam-block" aria-label="SAM Block">SAM Block</a><ul>
                        
                <li>
                    <a href="#multi-head-mixed-convolution" aria-label="Multi-Head Mixed Convolution">Multi-Head Mixed Convolution</a></li>
                <li>
                    <a href="#scale-aware-aggregation" aria-label="Scale-Aware Aggregation">Scale-Aware Aggregation</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%8f%af%e8%a7%86%e5%8c%96%e6%a8%a1%e5%9d%97%e6%95%88%e6%9e%9c---%e7%94%bb%e7%9a%84%e5%8d%b7%e7%a7%af%e8%b0%83%e5%88%b6%e6%9d%83%e9%87%8d" aria-label="可视化模块效果  &ndash; 画的卷积调制权重">可视化模块效果  &ndash; 画的卷积调制权重</a></li></ul>
                </li>
                <li>
                    <a href="#conv2former---cvpr-2022" aria-label="Conv2Former - CVPR 2022">Conv2Former - CVPR 2022</a></li>
                <li>
                    <a href="#convnext----cvpr-2022" aria-label="ConvNeXt  - CVPR 2022">ConvNeXt  - CVPR 2022</a><ul>
                        
                <li>
                    <a href="#convnext" aria-label="ConvNeXt">ConvNeXt</a></li>
                <li>
                    <a href="#inceptionnext" aria-label="InceptionNext">InceptionNext</a></li></ul>
                </li>
                <li>
                    <a href="#shunted-sa----cvpr-2022" aria-label="Shunted SA  - CVPR 2022">Shunted SA  - CVPR 2022</a></li>
                <li>
                    <a href="#cam----cvpr-2015" aria-label="CAM - CVPR 2015">CAM - CVPR 2015</a></li>
                <li>
                    <a href="#grad-cam----iccv-2017" aria-label="Grad-CAM  - ICCV 2017">Grad-CAM  - ICCV 2017</a></li>
                <li>
                    <a href="#detr----eccv-2020" aria-label="DETR  - ECCV 2020">DETR  - ECCV 2020</a></li>
                <li>
                    <a href="#bert-----2018" aria-label="Bert  -  2018">Bert  -  2018</a></li>
                <li>
                    <a href="#gpt" aria-label="GPT">GPT</a></li>
                <li>
                    <a href="#clip----2021" aria-label="CLIP  - 2021">CLIP  - 2021</a><ul>
                        
                <li>
                    <a href="#%e5%b7%a5%e4%bd%9c%e7%9b%ae%e7%9a%84" aria-label="工作目的">工作目的</a></li>
                <li>
                    <a href="#%e5%af%b9%e6%af%94%e5%ad%a6%e4%b9%a0%e6%96%b9%e6%b3%95-train" aria-label="对比学习方法 （Train）">对比学习方法 （Train）</a></li>
                <li>
                    <a href="#%e9%a2%84%e6%b5%8b" aria-label="预测">预测</a></li>
                <li>
                    <a href="#%e7%bc%ba%e7%82%b9" aria-label="缺点">缺点</a></li></ul>
                </li>
                <li>
                    <a href="#two-stream----2014" aria-label="Two-Stream  - 2014">Two-Stream  - 2014</a></li>
                <li>
                    <a href="#gc-vit----icml-2023" aria-label="GC-ViT - ICML 2023">GC-ViT - ICML 2023</a></li>
                <li>
                    <a href="#token-sparsification----cvpr-2023" aria-label="Token Sparsification  - CVPR 2023">Token Sparsification  - CVPR 2023</a><ul>
                        
                <li>
                    <a href="#semantic-token-generation-module" aria-label="Semantic Token Generation Module">Semantic Token Generation Module</a></li>
                <li>
                    <a href="#recovery-module" aria-label="Recovery Module">Recovery Module</a></li></ul>
                </li>
                <li>
                    <a href="#moco----cvpr-2020" aria-label="MOCO  - CVPR 2020">MOCO  - CVPR 2020</a></li>
                <li>
                    <a href="#mae----cvpr2022" aria-label="MAE  - CVPR2022">MAE  - CVPR2022</a></li>
                <li>
                    <a href="#wav2vec-20----neurips-2020" aria-label="wav2vec 2.0  - NeurIPS 2020">wav2vec 2.0  - NeurIPS 2020</a></li>
                <li>
                    <a href="#whisper----2022-openai" aria-label="Whisper - 2022 OpenAI">Whisper - 2022 OpenAI</a></li>
                <li>
                    <a href="#cpc----machine-learning-2018" aria-label="CPC  - Machine Learning 2018">CPC  - Machine Learning 2018</a></li>
                <li>
                    <a href="#dalle-2----20223-openai" aria-label="DALL·E 2 - 2022.3 OpenAI">DALL·E 2 - 2022.3 OpenAI</a></li>
                <li>
                    <a href="#diffusion-model----neurips-2020" aria-label="diffusion model  - NeurIPS 2020">diffusion model  - NeurIPS 2020</a></li>
                <li>
                    <a href="#time-frequency-consistency----neurips-2022" aria-label="Time-Frequency Consistency  - NeurIPS 2022">Time-Frequency Consistency  - NeurIPS 2022</a></li>
                <li>
                    <a href="#comet----neurips--2023" aria-label="COMET  - NeurIPS  2023">COMET  - NeurIPS  2023</a></li>
                <li>
                    <a href="#ts2vec----aaai-22" aria-label="TS2Vec  - AAAI 22">TS2Vec  - AAAI 22</a></li>
                <li>
                    <a href="#timesurl----aaai-2024" aria-label="TimesURL  - AAAI 2024">TimesURL  - AAAI 2024</a></li>
                <li>
                    <a href="#mixup----2017-machine-learning" aria-label="Mixup  - 2017 Machine Learning">Mixup  - 2017 Machine Learning</a></li>
                <li>
                    <a href="#yolo-v1----cvpr-2016" aria-label="YOLO-v1  - CVPR 2016">YOLO-v1  - CVPR 2016</a></li>
                <li>
                    <a href="#semi-supervised-hybrid-loss-----machine-learning-2023" aria-label="Semi-Supervised Hybrid Loss  -  Machine Learning 2023">Semi-Supervised Hybrid Loss  -  Machine Learning 2023</a></li>
                <li>
                    <a href="#simclr----2020" aria-label="SimCLR  - 2020">SimCLR  - 2020</a></li>
                <li>
                    <a href="#vilt----2021" aria-label="ViLT  - 2021">ViLT  - 2021</a></li>
                <li>
                    <a href="#beit-v3-2022" aria-label="BEIT-v3 2022">BEIT-v3 2022</a></li>
                <li>
                    <a href="#albef----2021" aria-label="ALBEF  - 2021">ALBEF  - 2021</a></li>
                <li>
                    <a href="#instance-discrimination----2018" aria-label="Instance discrimination  - 2018">Instance discrimination  - 2018</a></li>
                <li>
                    <a href="#byol----20206" aria-label="BYOL  - 2020/6">BYOL  - 2020/6</a></li>
                <li>
                    <a href="#dino---2021" aria-label="DINO - 2021">DINO - 2021</a></li>
                <li>
                    <a href="#simsiam---cvpr-2021" aria-label="SimSiam - CVPR 2021">SimSiam - CVPR 2021</a></li>
                <li>
                    <a href="#hilo-attention----neurips-2022" aria-label="HiLo Attention  - NeurIPS 2022">HiLo Attention  - NeurIPS 2022</a></li>
                <li>
                    <a href="#cmt----cvpr-2022" aria-label="CMT  - CVPR 2022">CMT  - CVPR 2022</a></li>
                <li>
                    <a href="#conformer---2020" aria-label="Conformer - 2020">Conformer - 2020</a></li>
                <li>
                    <a href="#pathformer----iclr-2024" aria-label="PathFormer  - ICLR 2024">PathFormer  - ICLR 2024</a></li>
                <li>
                    <a href="#mobile-former-----cvpr-2022" aria-label="Mobile-Former  -  CVPR 2022">Mobile-Former  -  CVPR 2022</a></li>
                <li>
                    <a href="#vit-adapter----iclr-2023" aria-label="ViT Adapter  - ICLR 2023">ViT Adapter  - ICLR 2023</a></li>
                <li>
                    <a href="#inception-transformer----neurips-2022" aria-label="Inception Transformer  - NeurIPS 2022">Inception Transformer  - NeurIPS 2022</a></li>
                <li>
                    <a href="#transnext----cvpr-2024" aria-label="TransNeXt  - CVPR 2024">TransNeXt  - CVPR 2024</a></li>
                <li>
                    <a href="#efficientvit----cvpr-2023" aria-label="EfficientViT  - CVPR 2023">EfficientViT  - CVPR 2023</a></li>
                <li>
                    <a href="#emo----iccv-2023" aria-label="EMO  - ICCV 2023">EMO  - ICCV 2023</a></li>
                <li>
                    <a href="#focal-attention----neurips-2021" aria-label="Focal Attention  - NeurIPS 2021">Focal Attention  - NeurIPS 2021</a></li>
                <li>
                    <a href="#cloformer----cvpr-2023" aria-label="CloFormer  - CVPR 2023">CloFormer  - CVPR 2023</a></li>
                <li>
                    <a href="#metaformer----2023" aria-label="MetaFormer  - 2023">MetaFormer  - 2023</a></li>
                <li>
                    <a href="#maxvit----eccv-2022" aria-label="MaxViT  - ECCV 2022">MaxViT  - ECCV 2022</a></li>
                <li>
                    <a href="#segment-anything" aria-label="Segment Anything">Segment Anything</a></li>
                <li>
                    <a href="#cit----iccv-2021" aria-label="CiT -  ICCV 2021">CiT -  ICCV 2021</a></li>
                <li>
                    <a href="#bi-interaction-light-vit" aria-label="Bi-Interaction Light-ViT">Bi-Interaction Light-ViT</a></li>
                <li>
                    <a href="#unireplknet" aria-label="UniRepLKNet">UniRepLKNet</a></li>
                <li>
                    <a href="#edgevits----eccv-2022" aria-label="EdgeViTs  - ECCV 2022">EdgeViTs  - ECCV 2022</a></li>
                <li>
                    <a href="#knnk-nearest-neighbors" aria-label="KNN(K-Nearest Neighbors)">KNN(K-Nearest Neighbors)</a></li>
                <li>
                    <a href="#shufflenet----cvpr-2018" aria-label="ShuffleNet  - CVPR 2018">ShuffleNet  - CVPR 2018</a></li>
                <li>
                    <a href="#repvgg---reparams---cvpr-2021" aria-label="RepVGG - ReParams - CVPR 2021">RepVGG - ReParams - CVPR 2021</a></li>
                <li>
                    <a href="#agent-attention---eccv-2024" aria-label="Agent Attention - ECCV 2024">Agent Attention - ECCV 2024</a></li>
                <li>
                    <a href="#patchtst---iclr-2023" aria-label="PatchTST - ICLR 2023">PatchTST - ICLR 2023</a></li>
                <li>
                    <a href="#crossformer---iclr-2023" aria-label="CrossFormer - ICLR 2023">CrossFormer - ICLR 2023</a></li>
                <li>
                    <a href="#informer---aaai-2021-best" aria-label="Informer - AAAI 2021 Best">Informer - AAAI 2021 Best</a></li>
                <li>
                    <a href="#adaptive-token-dictionary----cvpr2024" aria-label="Adaptive Token Dictionary  - CVPR2024">Adaptive Token Dictionary  - CVPR2024</a></li>
                <li>
                    <a href="#poly-kernel-inception----cvpr-2024" aria-label="Poly Kernel Inception  - CVPR 2024">Poly Kernel Inception  - CVPR 2024</a></li>
                <li>
                    <a href="#danet----cvpr-2019" aria-label="DANet  - CVPR 2019">DANet  - CVPR 2019</a></li>
                <li>
                    <a href="#mixnet" aria-label="MixNet">MixNet</a></li>
                <li>
                    <a href="#multimodal-learning" aria-label="Multimodal Learning">Multimodal Learning</a></li>
                <li>
                    <a href="#cf-vit" aria-label="CF-ViT">CF-ViT</a></li>
                <li>
                    <a href="#dual-aggregation-transformer" aria-label="Dual Aggregation Transformer">Dual Aggregation Transformer</a></li>
                <li>
                    <a href="#dual-vision-transformer" aria-label="Dual Vision Transformer">Dual Vision Transformer</a></li>
                <li>
                    <a href="#conformer-resnet--vit" aria-label="Conformer: ResNet &#43; ViT">Conformer: ResNet + ViT</a></li>
                <li>
                    <a href="#twins-local-global" aria-label="Twins： [Local-Global]">Twins： [Local-Global]</a></li>
                <li>
                    <a href="#msg-transformer" aria-label="MSG-Transformer">MSG-Transformer</a></li>
                <li>
                    <a href="#dilateformer" aria-label="DilateFormer">DilateFormer</a></li>
                <li>
                    <a href="#scopevit" aria-label="ScopeViT">ScopeViT</a></li>
                <li>
                    <a href="#fastvit---iccv" aria-label="FastViT - ICCV">FastViT - ICCV</a></li>
                <li>
                    <a href="#integration-of-cnn--attention" aria-label="Integration of CNN &#43; Attention">Integration of CNN + Attention</a></li>
                <li>
                    <a href="#repnext" aria-label="RepNeXt">RepNeXt</a></li>
                <li>
                    <a href="#fish-speech-tech-report" aria-label="Fish-Speech Tech-report">Fish-Speech Tech-report</a></li>
                <li>
                    <a href="#blip" aria-label="Blip">Blip</a></li>
                <li>
                    <a href="#vit-with-deformable-attn" aria-label="ViT with Deformable Attn">ViT with Deformable Attn</a></li>
                <li>
                    <a href="#bevformer" aria-label="BEVFormer">BEVFormer</a></li>
                <li>
                    <a href="#deformable-detr" aria-label="Deformable DETR">Deformable DETR</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="图示">图示<a hidden class="anchor" aria-hidden="true" href="#图示">#</a></h2>
<h3 id="卷积注意力">卷积注意力<a hidden class="anchor" aria-hidden="true" href="#卷积注意力">#</a></h3>
<p><img alt="image-20240322202245497" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240322202245497.png"></p>
<h3 id="自注意力">自注意力<a hidden class="anchor" aria-hidden="true" href="#自注意力">#</a></h3>
<p><img alt="image-20240322202200571" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240322202200571.png"></p>
<h2 id="学习">学习<a hidden class="anchor" aria-hidden="true" href="#学习">#</a></h2>
<h3 id="norm">Norm<a hidden class="anchor" aria-hidden="true" href="#norm">#</a></h3>
<p><img alt="image-20240305205407153" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240305205407153.png"></p>
<h3 id="loss">Loss<a hidden class="anchor" aria-hidden="true" href="#loss">#</a></h3>
<h4 id="cross-entropy">Cross Entropy<a hidden class="anchor" aria-hidden="true" href="#cross-entropy">#</a></h4>
<p><img alt="image-20240302160358957" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240302160358957.png">
$$
Loss = -\sum_{i}^{C}y_ilog(p(x_{i})), where \ y_i\ is label,p(x_i)\ is\ predict.
$$</p>
<h4 id="bce-loss">BCE Loss<a hidden class="anchor" aria-hidden="true" href="#bce-loss">#</a></h4>
<p>$$
Loss = −\sum_{i}^{c}(y_ilog(p(x_i)+(1−y_i)log(1−p(x_i)) \
where \ y_i \in [0, 1] \</p>
<p>pos_partition = -log(p(x_i))\
neg_partition = -log(1-p(x_i))
$$</p>
<h4 id="focal-loss">Focal Loss<a hidden class="anchor" aria-hidden="true" href="#focal-loss">#</a></h4>
<p>$$
Loss = -α_t(1-p_t)^γlog(p_t)\</p>
<p># Multi-Label:\
1.\ pos_loss = α(1-p(x_i))^γ\ \ -log(p_t)\
2.\ neg_loss = (1-α)p(x_i)^γ\ \ -log(1-p_t)
$$</p>
<hr>
<h2 id="论文">论文<a hidden class="anchor" aria-hidden="true" href="#论文">#</a></h2>
<h3 id="mlp-mixer---nips-2021">Mlp-mixer - <em>NIPS 2021</em><a hidden class="anchor" aria-hidden="true" href="#mlp-mixer---nips-2021">#</a></h3>
<p><strong>title: Mlp-mixer: An all-mlp architecture for vision</strong></p>
<p><img alt="image-20240225170151423" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225170151423.png"></p>
<p>⭐ channel-mixing MLPs and token-mixing MLPs.</p>
<p>① 融合Patches<strong>间</strong>的特征信息</p>
<p>② 融合Patches<strong>内</strong>的特征信息</p>
<p>❌ 易过拟合</p>
<hr>
<h3 id="transformer----nips-2017">**Transformer ** - NIPS 2017<a hidden class="anchor" aria-hidden="true" href="#transformer----nips-2017">#</a></h3>
<p><strong>Transformer Encoder</strong></p>
<p><img alt="image-20240225132511994" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225132511994.png"></p>
<h4 id="multi-head-self-attention">Multi-Head Self Attention<a hidden class="anchor" aria-hidden="true" href="#multi-head-self-attention">#</a></h4>
<p>⭐每段用不一样的α权重	&ndash; 分段融合Token间信息</p>
<p><img alt="image-20240225132601290" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225132601290.png"></p>
<p>​																			多组注意力权重α</p>
<p>Token分段，<strong>并行</strong>计算每段的权重α，多头注意力允许模型共同关注来自<strong>不同位置</strong>的<strong>不同表示子空间</strong>的信息。</p>
<p>⭐⭐⭐ 并行多个头，增强表示能力，融合不同子空间(低纬空间)</p>
<p><em><strong>Token间信息融合，比例是相似度权重</strong></em></p>
<h4 id="feed-forward-networks">Feed-Forward Networks<a hidden class="anchor" aria-hidden="true" href="#feed-forward-networks">#</a></h4>
<p><img alt="image-20240225132646417" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225132646417.png"></p>
<hr>
<h3 id="卷积">卷积<a hidden class="anchor" aria-hidden="true" href="#卷积">#</a></h3>
<p>深度可分离卷积计算成本比标准卷积小 8 到 9 倍，而精度仅略有降低  <em>- MobileNetV2</em></p>
<p>✨使用共享内核在局部区域内进行信息交互</p>
<h4 id="标准卷积">标准卷积<a hidden class="anchor" aria-hidden="true" href="#标准卷积">#</a></h4>
<p>输入特征图和模板卷积核</p>
<p>输出像素 - <em><strong>融合局部空间通道信息</strong></em></p>
<p><img alt="image-20240225152209315" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225152209315.png"></p>
<h5 id="瓶颈结构---resnet">瓶颈结构 - Resnet<a hidden class="anchor" aria-hidden="true" href="#瓶颈结构---resnet">#</a></h5>
<p><em>减少参数数量</em></p>
<p><img alt="image-20240225153933503" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225153933503.png"></p>
<p>逐点卷积降维 -&gt; 标准卷积 -&gt; 逐点卷积升维</p>
<p>​							残差连接</p>
<p>压缩 - 卷积 - 扩张</p>
<h4 id="深度可分离卷积">深度可分离卷积<a hidden class="anchor" aria-hidden="true" href="#深度可分离卷积">#</a></h4>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240225152858171.png" alt="image-20240225152858171" style="zoom: 67%;" />
<h5 id="倒残差结构---mobilenet">倒残差结构 - MobileNet<a hidden class="anchor" aria-hidden="true" href="#倒残差结构---mobilenet">#</a></h5>
<p>扩充通道的原因，是为了能够提取到更多的信息</p>
<p>ReLU 激活函数可能会崩溃掉某些通道信息。然而，如果我们有很多通道，那么信息可能仍然保留在其他通道中。</p>
<p><img alt="image-20240225153953959" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225153953959.png"></p>
<p>逐点卷积升维 -&gt; 分组卷积 -&gt; 逐点卷积降维</p>
<p>​							残差连接</p>
<p>扩张- 卷积 - 压缩</p>
<h4 id="linear">Linear<a hidden class="anchor" aria-hidden="true" href="#linear">#</a></h4>
<p>作用等同于1×1逐点卷积，实现线性映射</p>
<p><img alt="image-20240302162505751" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240302162505751.png"></p>
<hr>
<h3 id="vit---cvpr-202010">ViT - CVPR <em>2020/10</em><a hidden class="anchor" aria-hidden="true" href="#vit---cvpr-202010">#</a></h3>
<h4 id="vision-transformer----patching"><strong>Vision Transformer  - Patching</strong><a hidden class="anchor" aria-hidden="true" href="#vision-transformer----patching">#</a></h4>
<p><img alt="image-20240228154217975" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240228154217975.png"></p>
<p>化为互<strong>不重叠</strong>的区域，输出通道数为Token的长度。⭐ 将image拆分为16×16×3的patch</p>
<p><strong>经过Conv2D提取Token</strong>，# Token中每元素均有局部宽高及通道信息</p>
<p>所有标记之间的盲目相似性比较</p>
<hr>
<h3 id="pvt---iccv-2021"><em>PVT</em> - <strong>ICCV 2021</strong><a hidden class="anchor" aria-hidden="true" href="#pvt---iccv-2021">#</a></h3>
<p><strong>Pyramid Vision Transformer A Versatile Backbone for Dense Prediction without Convolutions</strong></p>
<p>创新点：在ViT（patch embed大小固定）基础上，仿照CNN网络的特征图变化</p>
<p>⭐<em><strong>使用“渐进”收缩策略通过补丁嵌入层来控制特征图的尺度</strong></em></p>
<p><img alt="image-20240225184934745" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225184934745.png"></p>
<p>CNNs: 特征图变化 通道×2，宽高÷2</p>
<p>ViT: patch embed不变</p>
<p>PVT： <strong>渐进式缩小特征图，减少token数量</strong></p>
<p><strong>PVT框架：</strong></p>
<p><img alt="image-20240225171800485" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225171800485.png"></p>
<h4 id="特征图---patch_embed---token">①特征图 - patch_embed -&gt; token<a hidden class="anchor" aria-hidden="true" href="#特征图---patch_embed---token">#</a></h4>
<p><em><strong>token map -&gt; token -&gt; token map</strong></em></p>
<p>​		⭐proj = Conv2d(in_channel, dim_token[i], kernel_size[i], stride[i])</p>
<p>dim_token = [64, 128, 256, 512]</p>
<p>kernel_size = [4, 2, 2, 2]</p>
<p>stride= [4, 2, 2, 2]	@ <strong>渐进式缩小特征图</strong>	&ndash; 融合局部token的感觉</p>
<p>token数量⬇，token维度⬆(信息更加丰富)</p>
<p><em><strong>多次Patch Embed</strong></em></p>
<p><img alt="image-20240225200016420" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225200016420.png"></p>
<h4 id="transformer-encoder-对-token-进行信息提取">②Transformer Encoder 对 token 进行信息提取<a hidden class="anchor" aria-hidden="true" href="#transformer-encoder-对-token-进行信息提取">#</a></h4>
<ul>
<li>
<p>MHSA处进行调整：引入Spacial Reduction操作</p>
</li>
<li>
<p>Spacial Reduction：将token折叠回特征图，使用Conv2d对特征图进行局部信息融合的处理，再保持通道维度不变的情况下，缩小宽高空间大小，再拆分成token，减少token数量。</p>
<p>对Token Key和Value的部分进行SR处理，shape=(*num_token, dim_token)</p>
<p>Query                                                   shape=(num_token, dim_token)</p>
<p>α = softmax(Q@SR(K).T / √d)            shape=(num_token, *num_token)</p>
<p>α@V												      shape=(num_token, dim_token)</p>
<p>⭐图示操作：</p>
<p><img alt="image-20240225203343221" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225203343221.png"></p>
</li>
</ul>
<p>利用conv局部性，将相邻的Token进行合并  | 或者利用Pool进行窗口内Token合并</p>
<p><em><strong>计算量降低sr_ratio平方倍</strong></em> &ndash; Conv2D中的stride</p>
<p>③将token折叠回特征图</p>
<hr>
<h3 id="cvt---iccv-2021">CvT - ICCV 2021<a hidden class="anchor" aria-hidden="true" href="#cvt---iccv-2021">#</a></h3>
<p><em><strong>CvT: Introducing Convolutions to Vision Transformers</strong></em></p>
<p><em><strong>很像PVT</strong></em>   - 前置论文Bottle neck Transformer @ 在resnet末尾将后三层Conv3×3替换为MHSA （Conv结构中引入Attension全局建模）</p>
<p>目的：</p>
<p>①将卷积神经网络 (CNN) 的理想特性引入 ViT 架构（即移位、尺度和失真不变性），同时保持 Transformer 的优点（即动态注意力、全局上下文和更好的泛化）</p>
<p>②将具有图像域特定归纳偏差的卷积引入 Transformer 来实现两全其美</p>
<p>③空间维度下采样，通道增加。Token数量少了，但每个Token变长了，增强了携带信息的表示的丰富性</p>
<p>⭐<em><strong>删除位置嵌入</strong></em>：为每个 Transformer 块引入卷积投影，并结合卷积令牌嵌入，使我们能够通过网络对局部空间关系进行建模。使其具有适应需要可变输入分辨率的各种视觉任务的潜在优势。</p>
<p><img alt="image-20240302163300436" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240302163300436.png"></p>
<center style="color:red; font-weight:bold">总体结构</center>
<h4 id="convolutional-token-embedding">Convolutional Token Embedding<a hidden class="anchor" aria-hidden="true" href="#convolutional-token-embedding">#</a></h4>
<p>重叠式的嵌入，使用标准Conv2D</p>
<p>并且在每个Stage最后，将Token集合折叠回Token Map</p>
<h4 id="convolutional-projection--conv生成qkv-而非通常的nnlinear">Convolutional Projection &ndash; Conv生成QKV 而非通常的nn.Linear<a hidden class="anchor" aria-hidden="true" href="#convolutional-projection--conv生成qkv-而非通常的nnlinear">#</a></h4>
<p>利用 <strong>分组卷积</strong> 将K与V的Token Map下采样，性能换效率</p>
<p>局部空间上下文的附加建模，相比于普通卷积(更复杂的设计和额外的计算成本)，DWConv性价比更高</p>
<p><img alt="image-20240302163105692" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240302163105692.png"></p>
<p><img alt="image-20240302164030978" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240302164030978.png"></p>
<p>(b): 可参考MobileViT，先利用卷积局部融合信息，再执行窗口级注意力</p>
<hr>
<h3 id="swin---iccv-2021"><em><strong>Swin - ICCV 2021</strong></em><a hidden class="anchor" aria-hidden="true" href="#swin---iccv-2021">#</a></h3>
<p><strong>Swin transformer: Hierarchical vision transformer using shifted windows</strong></p>
<p>层级式ViT</p>
<p>创新点：</p>
<p>①patch embed尺寸逐步增大，检测小目标更好</p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240225204225011.png" alt="image-20240225204225011" style="zoom: 50%;" />
<p><strong>总体结构</strong></p>
<p><img alt="image-20240225204557231" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225204557231.png"></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240226205609760.png" alt="image-20240226205609760" style="zoom:150%;" />
<h4 id="窗口级的token特征融合操作--swin-transformer-block-数量为偶数"><strong>窗口级的Token特征融合操作：</strong>  Swin Transformer Block <em><strong>数量为偶数</strong></em><a hidden class="anchor" aria-hidden="true" href="#窗口级的token特征融合操作--swin-transformer-block-数量为偶数">#</a></h4>
<p><img alt="image-20240225204802120" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225204802120.png"></p>
<p>①窗口内的Token信息融合</p>
<p><img alt="image-20240225205738527" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225205738527.png"></p>
<p>MHSA限制在窗口内进行 &ndash; 降低计算复杂度</p>
<p>②窗口间的Token信息融合 &ndash; 滑动窗口策略   &ndash; <em><strong>间接看到全局信息</strong></em></p>
<p><img alt="image-20240225205812927" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225205812927.png"></p>
<p>蓝线表示窗口内Token信息融合，红线表示窗口间信息融合</p>
<p>❗❗❗⭐⭐⭐MHSA依然限制在窗口内进行</p>
<p>❗<strong>为了规整统一窗口大小，便于批量计算</strong></p>
<p>进行一次循环位移，使窗口大小统一</p>
<p>⭐使用window-mask将本该不进行自注意计算的部分遮挡掉，在计算注意力α时，在其中softmax中，将mask的值设为-100，则e^{-100} / Σ e^{i} 为 0</p>
<p><img alt="image-20240225210202551" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225210202551.png"></p>
<p><strong>滑动窗口Mask示例</strong>：</p>
<p>给窗口编号，再使用torch.roll 滑动 （ window_size // 2 ） 个像素</p>
<p><img alt="image-20240225221427333" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225221427333.png"></p>
<p><img alt="image-20240225221521648" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225221521648.png">
$$
\text{softmax}(z)<em>i = \frac{e^{z_i}}{\sum</em>{j=1}^{K} e^{z_j}}, \quad \text{for } i = 1, 2, \ldots, K
\\e^{-100} ≈ 0
$$</p>
<h4 id="相对位置编码"><strong>相对位置编码：</strong><a hidden class="anchor" aria-hidden="true" href="#相对位置编码">#</a></h4>
<p>因为计算Window-MHSA是并行的，没有位置顺序信息</p>
<p><img alt="image-20240226153825736" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240226153825736.png"></p>
<p>⭐注意力权重 加上 相对位置偏置</p>
<p><em><strong>✨相对位置信息</strong></em>  &ndash;  <em><strong>压缩&amp;复用</strong></em></p>
<p>① relative position bias table 自学习的位置标量偏置   &ndash; <em><strong>压缩</strong></em></p>
<p><img alt="image-20240226194023098" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240226194023098.png"></p>
<p>② relative position bias index 索引 &ndash; 不同位置的token使用相同的相对位置偏置  &ndash; <em><strong>复用</strong></em></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240226154244883.png" alt="image-20240226154244883" style="zoom:150%;" />
<p>给定窗口大小，index是固定值，table是可学习的位置信息</p>
<p><strong>一维情况：</strong></p>
<p><img alt="image-20240311185939016" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240311185939016.png"></p>
<h4 id="patch-mergeing">Patch Mergeing<a hidden class="anchor" aria-hidden="true" href="#patch-mergeing">#</a></h4>
<p>目的： 宽高减半，通道翻倍</p>
<p><em><strong>整幅特征图分4部分</strong></em>，合并不同部分，相同位置的Token</p>
<p><img alt="image-20240226204417422" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240226204417422.png"></p>
<hr>
<h3 id="mobilevit---cvpr-2021">Mobilevit - <em><strong>CVPR 2021</strong></em><a hidden class="anchor" aria-hidden="true" href="#mobilevit---cvpr-2021">#</a></h3>
<p><em><strong>title: Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer</strong></em></p>
<p>在资源受限设备上运行ViT  &ndash; 混合CNN和Transformer</p>
<p><img alt="image-20240226210140021" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240226210140021.png"></p>
<h4 id="mv2-mobilenetv2-中的倒残差块"><strong>MV2</strong>: MobileNetv2 中的倒残差块<a hidden class="anchor" aria-hidden="true" href="#mv2-mobilenetv2-中的倒残差块">#</a></h4>
<p><img alt="img" loading="lazy" src="https://img-blog.csdnimg.cn/20200808185634409.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70#pic_center"></p>
<p><img alt="image-20240225153953959" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240225153953959.png"></p>
<p>⬇2： 下采样2倍，<em>控制stride步幅</em></p>
<h4 id="mobilevit-block-间接融合">MobileViT Block:	&ndash; 间接融合<a hidden class="anchor" aria-hidden="true" href="#mobilevit-block-间接融合">#</a></h4>
<p><img alt="image-20240227134923752" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240227134923752.png"></p>
<p>用二维卷积局部融合(重叠区域)，输出的特征图中每个像素都会看到 MobileViT 块中的所有其他像素</p>
<p>红色像素使用变压器关注蓝色像素</p>
<p>由于蓝色像素已经使用卷积对有关相邻像素的信息进行了编码，因此红色像素可以对图像中所有像素的信息进行编码</p>
<p>unfold和fold 将特征图转为Token集合，改排序，不等于patch embed(用conv2d)</p>
<p><img alt="image-20240227132301148" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240227132301148.png"></p>
<p>原论文 最后的通道维度融合，使用的Conv3x3卷积。    # 模块对称。</p>
<hr>
<h3 id="crossvit----iccv-2021">CrossViT  - ICCV 2021<a hidden class="anchor" aria-hidden="true" href="#crossvit----iccv-2021">#</a></h3>
<p><em><strong>CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification</strong></em></p>
<p>双分支 - <em><strong>多尺度patch嵌入</strong></em></p>
<p>互换分支的CLS Token 实现分支间的<em><strong>信息通信</strong></em></p>
<p><img alt="image-20240312172116507" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240312172116507.png"></p>
<center><strong>总体结构</strong></center>
<p><img alt="image-20240312171834443" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240312171834443.png"></p>
<center><strong>交互示例</strong></center>
<p><img alt="image-20240312171813246" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240312171813246.png"></p>
<center><strong>详细</strong></center>
<hr>
<h3 id="deit----cvpr-2021">DeiT  - CVPR 2021<a hidden class="anchor" aria-hidden="true" href="#deit----cvpr-2021">#</a></h3>
<p><em>Training data-efficient image transformers &amp; distillation through attention</em></p>
<p>⭐纯Transformer版本的知识蒸馏</p>
<p><img alt="image-20240312173214037" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240312173214037.png"></p>
<p>引入distillation token来进行知识蒸馏，执行的目标不同。一个与label计算loss，一个与Teacher label计算蒸馏损失。反向传播优化时，梯度不一样</p>
<hr>
<h3 id="t2t----iccv-2021">T2T  - ICCV 2021<a hidden class="anchor" aria-hidden="true" href="#t2t----iccv-2021">#</a></h3>
<p><em>Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</em></p>
<p><img alt="image-20240312173843585" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240312173843585.png"></p>
<p>重叠嵌入，信息更全面。融合Token，删除冗余</p>
<p><img alt="image-20240312174043432" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240312174043432.png"></p>
<p><strong>soft split</strong>	重叠融合(图示是堆叠在通道维度)</p>
<hr>
<h3 id="biformer----cvpr-2023">BiFormer  - CVPR 2023<a hidden class="anchor" aria-hidden="true" href="#biformer----cvpr-2023">#</a></h3>
<p><em><strong>BiFormer: Vision Transformer with Bi-Level Routing Attention</strong></em></p>
<p>一种<em><strong>动态的、查询感知</strong></em>的稀疏注意力机制。 关键思想是在粗糙区域级别过滤掉大部分不相关的键值对，以便只保留一小部分路由区域，再进行细粒度的注意力计算</p>
<p><img alt="image-20240301191315274" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240301191315274.png"></p>
<center style="text-align: center; color: red; font-weight:bold; font-style: italic;">注意力区域对比</center>
<p><img alt="image-20240301191932243" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240301191932243.png"></p>
<p><em><strong>路由方式建立跨窗口信息交互</strong></em></p>
<p>通过化为window，各Token均值为代表整window的Token，计算区域相似性关系图，借此为窗口间链接通信</p>
<p>将窗口串起来</p>
<p><img alt="image-20240301200645397" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240301200645397.png"></p>
<center style="font-weight:bold;">Bi-Routing Self-Attention</center>
<p><img alt="image-20240301200800664" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240301200800664.png"></p>
<center style="font-weight:bold;">总体结构</center>
<p>DWConv 3×3 作用：</p>
<p>​	在开始时使用 3×3 深度卷积来<em><strong>隐式编码相对位置信息</strong></em>。</p>
<hr>
<h3 id="smt---iccv-2023">SMT - ICCV 2023<a hidden class="anchor" aria-hidden="true" href="#smt---iccv-2023">#</a></h3>
<p><em><strong>Scale-Aware Modulation Meet Transformer</strong></em></p>
<p>“模拟随着网络变得更深而从捕获<strong>局部</strong>依赖关系到<strong>全局</strong>依赖关系的转变“（串行渐变结构） =联想到=&gt;  浅层网络捕获<strong>形态</strong>特征，深层部分捕获高级<strong>节律</strong>特征</p>
<p>SMT1D 生硬的训练ECG 中等效果，但计算量小！</p>
<p>层级式ViT结构</p>
<p><img alt="image-20240303133748562" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240303133748562.png"></p>
<center style="color:red;font-weight:bolder">总体结构</center>
<h4 id="evolutionary-hybrid-network">Evolutionary Hybrid Network<a hidden class="anchor" aria-hidden="true" href="#evolutionary-hybrid-network">#</a></h4>
<p>结构取名为“进化混合网络” Evolutionary Hybrid Network   - 局部到全局建模的过渡</p>
<p><img alt="image-20240303134117897" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240303134117897.png"></p>
<center>两种融合模块的堆叠方式</center>
<p><img alt="image-20240303134540985" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240303134540985.png"></p>
<center>性能对比</center>
<h4 id="sam-block"><strong>SAM Block</strong><a hidden class="anchor" aria-hidden="true" href="#sam-block">#</a></h4>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240303140344518.png" alt="image-20240303140344518" style="zoom:80%;" />
<center style="font-weight:bold">scale-aware modulation</center>
<h5 id="multi-head-mixed-convolution">Multi-Head Mixed Convolution<a hidden class="anchor" aria-hidden="true" href="#multi-head-mixed-convolution">#</a></h5>
<p>将通道分割为多个头(组)，每个头使用不同大小的卷积核（目的：捕捉多个尺度的各种空间特征。增强网络的建模能力“局部远程依赖”）</p>
<p>例：</p>
<p><img alt="image-20240122210304883" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240122210304883.png"></p>
<h5 id="scale-aware-aggregation">Scale-Aware Aggregation<a hidden class="anchor" aria-hidden="true" href="#scale-aware-aggregation">#</a></h5>
<p>倒残差结构 - 融合通道</p>
<p>在MHMC结构中，多头由于卷积核大小不同，更大的卷积核看到的感受野更大。 将多头各个卷积依次打包成组，进行组间通道融合，每组既有小感受野的信息，又有大感受野信息，多样 multi-scale</p>
<h4 id="可视化模块效果---画的卷积调制权重">可视化模块效果  &ndash; 画的卷积调制权重<a hidden class="anchor" aria-hidden="true" href="#可视化模块效果---画的卷积调制权重">#</a></h4>
<p>深度可分离卷积作为注意力权重</p>
<p><img alt="image-20240304162401167" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240304162401167.png"></p>
<p>① 每个不同的卷积特征图都学习以自适应方式关注不同的粒度特征</p>
<p>②多头更准确地描绘前景和目标对象</p>
<p>③网络变深，多头仍然可以呈现目标物体的整体形状。与细节相关的信息在单头卷积下会丢失</p>
<p>⭐⭐表明 MHMC 在浅层阶段有能力比单头更好地捕获局部细节，同时随着网络变得更深，保持目标对象的详细和语义信息。</p>
<p><img alt="image-20240304162626670" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240304162626670.png"></p>
<p>增强了语义相关的低频信号，精确地聚焦于目标对象最重要的部分。</p>
<p>更好的捕获和表示视觉识别任务的基本特征的能力。</p>
<hr>
<h3 id="conv2former---cvpr-2022">Conv2Former - CVPR 2022<a hidden class="anchor" aria-hidden="true" href="#conv2former---cvpr-2022">#</a></h3>
<p>Conv2Former: A Simple Transformer-Style ConvNet for Visual Recognition</p>
<p><strong>创新点：</strong> <em><strong>用卷积操作 模拟近似 注意力</strong></em>        &ndash; 更低的计算代价 略低的性能下降</p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240304161241745.png" alt="image-20240304161241745"  />
<p>Hadamard product == 矩阵按元素相乘</p>
<p>使得每个空间位置(h,w)能够与以(h,w)为中心的k×k正方形区域内的所有像素相关。通道之间的信息交互可以通过线性层来实现。</p>
<p>※重点： ConvNeXt中表明，卷积核不是越大越好，超过7x7，计算代价远远大于性能收益</p>
<p>⭐本文实验提出，<em><strong>将卷积特征作为权重，能比传统方式更有效地利用大内核</strong></em></p>
<hr>
<h3 id="convnext----cvpr-2022">ConvNeXt  - CVPR 2022<a hidden class="anchor" aria-hidden="true" href="#convnext----cvpr-2022">#</a></h3>
<h4 id="convnext"><strong>ConvNeXt</strong><a hidden class="anchor" aria-hidden="true" href="#convnext">#</a></h4>
<p>将CNN搭建成SwinT的风格，性能超越SwinT</p>
<h4 id="inceptionnext"><strong>InceptionNext</strong><a hidden class="anchor" aria-hidden="true" href="#inceptionnext">#</a></h4>
<p>多尺度版本的ConvNeXt</p>
<p><img alt="image-20240312174928461" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240312174928461.png"></p>
<p><img alt="image-20240325140844728" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240325140844728.png"></p>
<hr>
<h3 id="shunted-sa----cvpr-2022">Shunted SA  - CVPR 2022<a hidden class="anchor" aria-hidden="true" href="#shunted-sa----cvpr-2022">#</a></h3>
<p><em><strong>Shunted Self-Attention</strong> via Multi-Scale Token Aggregation</em></p>
<p>PVT 和类似的模型往往会在这种空间缩减中合并太多的标记，使得小物体的细粒度信息与背景混合并损害模型的性能；</p>
<p>同时保留粗粒度和细粒度的细节，同时保持对图像标记的全局依赖性建模；</p>
<ul>
<li>观察到PVT和ViT在同一个Encoder中token尺度是固定的，<strong>创新</strong>结合二者。用Conv将Token Map局部融合成多幅Token Map，使每个汇聚的Token代表着不同区域，不仅观察到小物体，也能看到大物体。</li>
</ul>
<p><img alt="image-20240313161903398" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240313161903398.png"></p>
<p><img alt="image-20240313162357016" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240313162357016.png"></p>
<p>ViT：小区域</p>
<p>PVT： 大区域</p>
<p>Shunted : 结合大小区域</p>
<p><em><strong>灵感源于：</strong></em></p>
<p><img alt="image-20240326140819772" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240326140819772.png"></p>
<p><strong>做法：</strong></p>
<p>​	不同头的长度不同，以捕获不同粒度的信息。</p>
<p>原始特征图x，卷积局部融合得到x1， x2  (HW均变小)</p>
<p><img alt="image-20240313174204874" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240313174204874.png"></p>
<p>❗注意：Q的线性映射维度不变，而不同尺度的KV通过线性映射时维度<strong>缩减率</strong>为SR特征图的数量（QKV维度一致）</p>
<p><strong>两种做法</strong></p>
<p>细粒度 - pixel token ||   粗粒度 - patch token</p>
<p><img alt="image-20240326135013259" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240326135013259.png"></p>
<hr>
<p><img alt="image-20240326135047116" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240326135047116.png"></p>
<hr>
<h3 id="cam----cvpr-2015"><strong>CAM</strong>  - CVPR 2015<a hidden class="anchor" aria-hidden="true" href="#cam----cvpr-2015">#</a></h3>
<p><em>Learning Deep Features for Discriminative Localization</em></p>
<p><strong>弱监督对象定位</strong>  - 仅提供Image level label</p>
<p>期望：每个单元被其感受野内的某种视觉模式激活。因此 fk （表示空间位置 (x, y) 处最后一个卷积层中单元 k 的激活//输出特征图的一个像素）是该视觉模式存在的地图。类激活图只是这些视觉模式在不同空间位置的存在的加权线性和</p>
<p><strong>计算卷积特征图对于特定输出单元的重要性来实现的</strong></p>
<p>⭐⭐⭐网络可以保留其卓越的定位能力，直到最后一层   =&gt; 深层特征的定位能力</p>
<p>❗❗❗尽管接受了图像级标签的训练，CNN 仍具有出色的对象定位能力</p>
<p><strong>缺陷</strong>：卷积特征图→全局平均池化→softmax层  // 特定网络结构</p>
<p><img alt="image-20240322134603157" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240322134603157.png"></p>
<center style="color: red; font-weight: bold;">做法图示</center>
<p><img alt="image-20240322134441902" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240322134441902.png"></p>
<center style="color: red; font-weight: bold;">数学公式</center>
<p>在卷积特征图上执行全局平均池化，并将它们用作全连接层的特征，产生所需的输出分类;</p>
<p>❗❗❗将输出层的权重投影回卷积特征图来识别图像区域的重要性</p>
<hr>
<h3 id="grad-cam----iccv-2017">Grad-CAM  - ICCV 2017<a hidden class="anchor" aria-hidden="true" href="#grad-cam----iccv-2017">#</a></h3>
<p>适用CNN模型</p>
<p>但论文提到在CNN+LSTM的也能定位有区别的图像区域</p>
<p><img alt="image-20240322141249072" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240322141249072.png"></p>
<p><img alt="image-20240322141305661" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240322141305661.png"></p>
<p>α 捕获<strong>特征图</strong> k 对于目标<strong>类</strong> c 的<strong>重要性</strong>  // 与CAM的分类线性层权重作用一致</p>
<p><img alt="image-20240322141314248" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240322141314248.png"></p>
<p><strong>ReLU</strong>的作用，只对对感兴趣的类别有积极影响的特征感兴趣。负像素可能属于图像中的其他类别</p>
<p><em>上述操作 =&gt; 具有<strong>类别区分性</strong>并且可以很好地定位相关图像区域</em>   - 最后特征图比较小!</p>
<p>​					  但缺乏显示细粒度重要性的能力 （能区分猫狗，但对为什么识别为猫，不够精确）</p>
<p>通过点乘法融合 <strong>引导反向传播</strong> 和 <strong>Grad-CAM</strong> =&gt; 可视化</p>
<p>Grad-CAM : 类别区分性</p>
<p>Guided Backprop： 细节纹理重要程序。 做法：将梯度值小于等于零的部分置为零，保留梯度值大于零的部分 =&gt; 以突出输入图像中对预测结果有积极影响的区域，来实现对神经网络中每个像素对最终预测结果的影响进行可视化和解释</p>
<p><strong>浅层卷积关注纹理特征，深层网络关注本质的那种特征</strong>？</p>
<hr>
<h3 id="detr----eccv-2020">DETR  - ECCV 2020<a hidden class="anchor" aria-hidden="true" href="#detr----eccv-2020">#</a></h3>
<p><em><strong>⭐End-to-End⭐</strong></em> Object Detection with Transformers</p>
<p>传统：设置锚框 + 非极大值抑制(去除多余的框)</p>
<p>创新：集合预测(预测分类 + 锚框)</p>
<p><img alt="image-20240322202435323" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240322202435323.png"></p>
<center style="color: red; font-weight: bold;">前向流程</center>
<p><img alt="image-20240322202514460" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240322202514460.png"></p>
<center style="color: red; font-weight: bold;">模型框架</center>
<p>CNN backbone  - local information fusion</p>
<p>Transformer encoder  - global information fusion</p>
<p>object queries  - learnable information vector   作用： =&gt; anchor</p>
<p><em><strong>FFN:  1. classification =&gt; output class vector  2. box =&gt; output 4 number [center_x, center_y, width, hight]</strong></em></p>
<p>object queries: 作用就是锚框，并且一次性生成100个 &raquo; 图片检测的物体数</p>
<p>如何将object queries 与 groundtrue一一对应？</p>
<p>匈牙利算法 寻找最佳匹配</p>
<p>匹配loss = 分类loss(分类正确率) + 框loss(框的重叠度)</p>
<p>=&gt; 匈牙利算法 =&gt; 哪些框与GT最佳匹配(预测框与GT一一对应)</p>
<p>最终回传梯度优化参数LOSS = 分类loss(分类正确率) + [框loss + 与框大小无关的iou loss]</p>
<p>因为框也是生成的，且Transformer容易出大框(全局建模)</p>
<hr>
<h3 id="bert-----2018">Bert  -  2018<a hidden class="anchor" aria-hidden="true" href="#bert-----2018">#</a></h3>
<p><strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong>   <em>- Computation and Language</em></p>
<p>Bidirectional Transformers 意思是</p>
<p>Transformer Encoder中的自注意力计算是全局的，每个Token能观测到其余的Token序列； 而</p>
<p>Transformer Decoder中由于进行的是Masked Multi Head Self Attention，所以序列只能观测到自己与之前的语境；</p>
<p>这对于文本上下文语境建模是有弊端的！</p>
<p>(生成式无监督学习)</p>
<p><strong>总体结构</strong></p>
<p><img alt="image-20240327160149024" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240327160149024.png"></p>
<p>[CLS] 分类Token，凝聚全局语义信息</p>
<p>[SEP] 分割符，划分句子范围</p>
<p>用的某个语料库进行词嵌入</p>
<p><img alt="image-20240327202952064" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240327202952064.png"></p>
<p>⭐<strong>训练方法</strong></p>
<ul>
<li>
<p>完型填空：使某个词随机被 [Mask] 字符遮挡；为防止模型对[Mask]字符敏感，遮挡时使用概率遮挡 =&gt; 1. 仍替换为[Mask]字符 2.随机替换为其他字符 3. 保持不变 =&gt; 迫使模型学习上下文语境   <strong style='color:blue;'>[句子内信息建模]</strong></p>
</li>
<li>
<p>预测下一句：  <strong style='color:blue;'>[句子间信息建模]</strong></p>
</li>
</ul>
<p><strong>丰富的上下文信息</strong>：通过考虑单词的左右上下文，BERT 能够更好地理解词义和句法结构，这对于理解语言的复杂性至关重要。</p>
<hr>
<h3 id="gpt">GPT<a hidden class="anchor" aria-hidden="true" href="#gpt">#</a></h3>
<p>generative pretrain transformer</p>
<p><em>初略版</em></p>
<p>模型图：(Transformer Decoder -仅Masked Attention版)</p>
<p><img alt="image-20240505163406185" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240505163406185.png"></p>
<p>⭐ <em>input:</em>  word =&gt; index(查找词汇表) =&gt; embedding   +  position embedding</p>
<p>[batch, sequence_length, embedding_dim]	 # sequence_length 可由多个句子组成，可以使用<s>标识句子结束，<pad>用来填充，使得sequence_length在batch内长度一致</p>
<p>⭐ <em>train：</em></p>
<p>x1 =&gt; x2</p>
<p>x1, x2 =&gt; x3</p>
<p>x1, x2, x3 =&gt; x4</p>
<p>因为每个词都要预测下一个词，故使用masked attention （mask-softmax），防止答案泄漏</p>
<hr>
<h3 id="clip----2021">CLIP  - 2021<a hidden class="anchor" aria-hidden="true" href="#clip----2021">#</a></h3>
<p><strong>Learning Transferable Visual Models From Natural Language Supervision</strong></p>
<p><strong>实现zero-shot，上游大数据集预训练好，下游任务迁移学习无需样本微调</strong></p>
<p><strong>多模态模型的总体目标就是：训练一个模型，一方面能统一特征表达，另一方面又能让不同模态特征间学到相关性</strong></p>
<p>(判别式无监督学习)</p>
<h4 id="工作目的">工作目的<a hidden class="anchor" aria-hidden="true" href="#工作目的">#</a></h4>
<p>痛点：</p>
<ul>
<li>
<p>在特点数据集上进行标签训练 =&gt; 输入没见过的类别，那么模型就不能输出正确的结果</p>
</li>
<li>
<p>数据出现分布偏移，动物图片与卡通动物图片 =&gt; 识别不出来</p>
</li>
</ul>
<p>图片 - 文字描述 ， <strong>模型学习配对关系</strong></p>
<p><em>一个对象的不同视角表示：图片和文本描述</em></p>
<h4 id="对比学习方法-train"><strong>对比学习方法</strong> <strong>（Train）</strong><a hidden class="anchor" aria-hidden="true" href="#对比学习方法-train">#</a></h4>
<p><img alt="image-20240327200843127" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240327200843127.png"></p>
<ol>
<li>Text Encoder (resnet/vit) 学习文本描述的 深度特征  - 单模态内特征 // T_i == 一个文本特征</li>
<li>Image Encoder(transformer) 学习图片的 深度特征  - 单模态内特征  // I_i == 一个图像特征</li>
<li>将多模态特征投影到跨模态空间  // 矩阵映射，(特征向量)到同一纬度</li>
<li>计算余弦相似度，很明显，正确配对的位置为对角线</li>
<li>计算Loss： （相似度logit作为预测分数）</li>
<li><strong>按行计算Loss</strong>，在每一行范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一张图片，我们都希望找到和它最相似的文字</li>
<li><strong>按列计算Loss</strong>，在每一列的范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一段文字，我们都希望找到和它最相似的图片</li>
<li><strong>最后将这两个Loss相加取平均</strong>，代表我们在模型优化过程中<strong>考虑了“图片-&gt;文字”和“文字-&gt;图片”的双向关系</strong></li>
</ol>
<p>encoder均从头开始训练</p>
<h4 id="预测">预测<a hidden class="anchor" aria-hidden="true" href="#预测">#</a></h4>
<p><img alt="image-20240327201936442" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240327201936442.png"></p>
<ul>
<li>创建一个标签全集， [f&rsquo;A photo of a {object}&rsquo; for object in dataset_labels]</li>
<li>Text Encoder 学习上述 模板文字描述 的 深度特征</li>
<li>Image Encoder 学习 待预测图片 的 深度特征</li>
<li>计算余弦相似度 =&gt; 取最高分作为预测目标</li>
</ul>
<p>​</p>
<h4 id="缺点">缺点<a hidden class="anchor" aria-hidden="true" href="#缺点">#</a></h4>
<ul>
<li>每次预测需要构建标签全集描述</li>
<li>对抽象任务，性能较差</li>
</ul>
<hr>
<h3 id="two-stream----2014">Two-Stream  - 2014<a hidden class="anchor" aria-hidden="true" href="#two-stream----2014">#</a></h3>
<p><strong>Two-Stream Convolutional Networks for Action Recognition in Videos</strong></p>
<p>⭐先前工作中通过使用堆叠视频帧作为网络的输入来解决此任务，但结果明显比最好的手工制作的浅层表示差。 =&gt; 这可能表明学习的时空特征不能很好地捕捉运动（简单堆叠 =&gt; 让模型从庞大的数据中学习❌很难）  =&gt; 模型很难识别该类特征 =&gt; <strong>预先处理成模型擅长的数据形式</strong></p>
<p>❗❗❗虽然多帧信息很重要，但以<strong>适当的方式</strong>将其呈现给 ConvNet 也很重要（饱和）</p>
<p>信息显式建模  =&gt; 可以简化学习过程</p>
<p><em>创新点</em></p>
<ul>
<li>空间流从静止视频帧执行动作识别</li>
<li>时间流以识别密集光流形式的运动动作</li>
</ul>
<p>=&gt; 贴合人类视觉：识别物体  +  识别运动  =&gt; ⭐[结果角度]表明两个识别流是互补的</p>
<p>光流：帧帧之间像素变化(描述运动变化的数据形式) | 水平方向和垂直方向  <strong>&lt;= 有用的线索</strong>  (与魔改模型不一样的思路)</p>
<p>⭐⭐⭐此类输入<strong>明确描述</strong>了视频帧之间的<strong>运动</strong> =&gt; 这使得识别更容易 =&gt; 因为网络不需要隐式估计运动  <em><strong>（显示建模）</strong></em></p>
<p>可以从<strong>光流数据</strong> =&gt; <strong>时空局部</strong>特征 || <strong>运动学</strong>特征  || 运动是使用光流位移场<strong>明确表示</strong>的</p>
<p><strong>模型框架</strong>（双模态）</p>
<p><img alt="image-20240328154319036" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240328154319036.png"></p>
<p><strong>提高模型迁移能力</strong>   类似于CLIP的目的</p>
<p>考虑将两个数据集合并为一个，由于类集之间的交叉，这并不简单</p>
<p>=&gt;  多任务学习  =&gt; 最后生成多个分类头，对应两个数据集分类。混合多个数据集进行实验</p>
<hr>
<h3 id="gc-vit----icml-2023"><strong>GC-ViT  - ICML 2023</strong><a hidden class="anchor" aria-hidden="true" href="#gc-vit----icml-2023">#</a></h3>
<p><strong>Global Context Vision Transformers</strong></p>
<p>与SwinT的区别：</p>
<ul>
<li>不通过滑动窗口实现全局信息建模  =&gt;  直接生成全局信息Token，使用这组携带全局信息的TokenSet来实现全局信息建模</li>
<li>SwinT 层数堆不够的话，全局信息建模不强</li>
</ul>
<p>与SwinT的共同点：</p>
<ul>
<li>
<p>都属于window-wise attention</p>
</li>
<li>
<p>位置编码都采用相对位置偏置bias</p>
</li>
<li>
<p>基本单元：局部窗口内计算 + 全局信息建模；（窗口内 + 窗口间）</p>
</li>
</ul>
<p><img alt="image-20240409161629944" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240409161629944.png"></p>
<p>思路：</p>
<p><img alt="image-20240331194258110" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240331194258110.png"></p>
<p><strong>Global query tokens generation</strong></p>
<p><img alt="image-20240331195205866" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240331195205866.png"></p>
<p><em>MBConv  - FeatureExtract</em></p>
<p><img alt="image-20240331195153985" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240331195153985.png"></p>
<p><em>代码分析</em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">X</span><span class="p">:</span>            <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_token</span><span class="p">,</span> <span class="n">dim_token</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">-</span><span class="n">window</span><span class="p">:</span>     <span class="p">[</span><span class="n">batch</span><span class="o">*</span><span class="n">num_windows</span><span class="p">,</span> <span class="n">num_token</span><span class="o">//</span><span class="n">num_windows</span><span class="p">,</span> <span class="n">dim_token</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">-</span><span class="n">window</span><span class="o">-</span><span class="n">head</span><span class="p">:[</span><span class="n">batch</span><span class="o">*</span><span class="n">num_windows</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_token</span><span class="o">//</span><span class="n">num_windows</span><span class="p">,</span> <span class="n">dim_token</span><span class="o">//</span><span class="n">num_heads</span><span class="p">]</span>  
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Query</span><span class="p">:</span>      <span class="p">[</span><span class="n">batch</span> <span class="err">×</span> <span class="n">num_window</span><span class="p">,</span>  <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">Key</span><span class="o">-</span><span class="n">Global</span><span class="p">:</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span>    <span class="n">repeat</span>  <span class="p">,</span>  <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">]</span>
</span></span></code></pre></div><p>图示最后的复制，是让每个窗口内的token与全局token表示进行信息融合  （复制窗口维度的大小）</p>
<p><strong>funny:  公司{多部门} =&gt; [部门内部成员进行讨论] =&gt; [部门之间领导进行讨论]</strong></p>
<hr>
<h3 id="token-sparsification----cvpr-2023">Token Sparsification  - CVPR 2023<a hidden class="anchor" aria-hidden="true" href="#token-sparsification----cvpr-2023">#</a></h3>
<p><strong>Making Vision Transformers Efficient from A Token Sparsification View</strong></p>
<p><em><strong>动机：</strong></em></p>
<p>​	<em>观察到注意力后期，仅部分核心Token起着主要作用</em><img alt="image-20240407143751781" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240407143751781.png"></p>
<p>对Token进行瘦身 - 与利用CLS Token进行级联删除不用的策略</p>
<p><em><strong>方法：</strong></em>	<em><strong>Sparse Token Vision Transformer</strong></em></p>
<h4 id="semantic-token-generation-module"><em>Semantic Token Generation Module</em><a hidden class="anchor" aria-hidden="true" href="#semantic-token-generation-module">#</a></h4>
<p><img alt="image-20240407144042256" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240407144042256.png"></p>
<p>1️⃣ Base Module 学习浅层特征</p>
<p>2️⃣ 进行<strong>Spatial Pooling</strong>聚合区域，生产空间簇中心TokenSet</p>
<p>​	  <strong>Conv( GELU( LayerNorm( DWConv( X ))))</strong>  <em>- 深度可分离</em></p>
<p>​	  创新：<strong>Intra and inter-window</strong> spatial pooling</p>
<p>​	  目的：聚合窗口信息(代表) + 疏远窗口间信息(不可被替代)</p>
<p>3️⃣ 融合生成Semantic Token Set	- Global Initial Center G 是可学习的参数
$$
\overline{S^{1}} = MHA(P, X, X) + P,\ \ \  S^{1} = FFN(\overline{S^{1}}) + \overline{S^{1}}
\\
\overline{S^{2}} = MHA(S^{1} + G, Concat(S^{1}, X), Concat(S^{1}, X)) + P,\ \ \  S^{2} = FFN(\overline{S^{2}}) + \overline{S^{2}}
\
S^{2} = Semantic\ \ Token
$$</p>
<h4 id="recovery-module">Recovery Module<a hidden class="anchor" aria-hidden="true" href="#recovery-module">#</a></h4>
<p><img alt="image-20240407152546112" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240407152546112.png">
$$
\overline{X} = MHA(X, S, S) + P,\ \ \  X = FFN(\overline{X}) + \overline{X}
$$
融合操作的逆过程</p>
<p>从语义级TokenSet中恢复空间信息</p>
<p><em><strong>First layer</strong> attention maps</em></p>
<p><img alt="image-20240407152443412" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240407152443412.png"></p>
<hr>
<h3 id="moco----cvpr-2020">MOCO  - CVPR 2020<a hidden class="anchor" aria-hidden="true" href="#moco----cvpr-2020">#</a></h3>
<p><em><strong>Momentum Contrast for Unsupervised Visual Representation Learning</strong></em></p>
<p>(判别式无监督学习)</p>
<p><em><strong>目标：</strong></em></p>
<p>​	1️⃣ 构建一个足够大的动态词典，包含足够多的负样本，使模型真能够学到判别式的特征; 在海量数据中学到真正的样本分布</p>
<p>​	2️⃣ 因为词典是动态变化的，为了使词典中的负样本特征尽可能的保持一致性(模型参数不同，时间维度上，得到的特征向量存在不一致性)，提出动量更新 =&gt; 动量模型的缓慢更新确保了字典中的特征相对稳定，从而提供更一致的负样本，提升对比学习的效果。
$$
θ_{k} ←mθ_{k} + (1 −m)θ_{q}
$$
m ∈ [0, 1) 是动量系数。论文中Query Encoder 和Key Encoder是一样配置架构的编码器。  目的，缓慢的更新Key Encoder，构建一个又大又一致的动态词典。</p>
<p><img alt="image-20240408212751319" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240408212751319.png"></p>
<p><strong>框架伪代码</strong></p>
<p>pretext task ： 实例判别任务。目标拉近正样本对在特征空间的距离，并使负样本对尽可能的远离。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># f_q, f_k: encoder networks for query and key</span>
</span></span><span class="line"><span class="cl"><span class="c1"># queue: dictionary as a queue of K keys (CxK)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># m: momentum</span>
</span></span><span class="line"><span class="cl"><span class="c1"># t: temperature</span>
</span></span><span class="line"><span class="cl"><span class="n">f_k</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">f_q</span><span class="o">.</span><span class="n">params</span> <span class="c1"># initialize</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span> <span class="c1"># load a minibatch x with N samples</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_q</span> <span class="o">=</span> <span class="n">aug</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># a randomly augmented version</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_k</span> <span class="o">=</span> <span class="n">aug</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># another randomly augmented version</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">q</span> <span class="o">=</span> <span class="n">f_q</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x_q</span><span class="p">)</span> <span class="c1"># queries: NxC</span>
</span></span><span class="line"><span class="cl">    <span class="n">k</span> <span class="o">=</span> <span class="n">f_k</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x_k</span><span class="p">)</span> <span class="c1"># keys: NxC</span>
</span></span><span class="line"><span class="cl">    <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="c1"># no gradient to keys</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># positive logits: Nx1</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_pos</span> <span class="o">=</span> <span class="n">bmm</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">C</span><span class="p">),</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># negative logits: NxK</span>
</span></span><span class="line"><span class="cl">    <span class="n">l_neg</span> <span class="o">=</span> <span class="n">mm</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">C</span><span class="p">),</span> <span class="n">queue</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">C</span><span class="p">,</span><span class="n">K</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># logits: Nx(1+K)</span>
</span></span><span class="line"><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">cat</span><span class="p">([</span><span class="n">l_pos</span><span class="p">,</span> <span class="n">l_neg</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># contrastive loss, Eqn.(1)</span>
</span></span><span class="line"><span class="cl">    <span class="n">labels</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="c1"># positives are the 0-th</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">logits</span><span class="o">/</span><span class="n">t</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># SGD update: query network</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">update</span><span class="p">(</span><span class="n">f_q</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># momentum update: key network</span>
</span></span><span class="line"><span class="cl">    <span class="n">f_k</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">m</span><span class="o">*</span><span class="n">f_k</span><span class="o">.</span><span class="n">params</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">m</span><span class="p">)</span><span class="o">*</span><span class="n">f_q</span><span class="o">.</span><span class="n">params</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># update dictionary</span>
</span></span><span class="line"><span class="cl">    <span class="n">enqueue</span><span class="p">(</span><span class="n">queue</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="c1"># enqueue the current minibatch</span>
</span></span><span class="line"><span class="cl">    <span class="n">dequeue</span><span class="p">(</span><span class="n">queue</span><span class="p">)</span> <span class="c1"># dequeue the earliest minibatch</span>
</span></span></code></pre></div><p>两个不同视角的同一张图片为正样本对，不同图片为负样本对。</p>
<p>抽特征：q, k；   最大化q k相似且与负样本远离</p>
<p>更新Q_Encoder, 使用Q_Encoder参数更新K_Encoder，动量缓慢更新</p>
<p>无监督预训练后好，取Q_Encoder作为抽取特征骨干网络，冻结其参数，微调分类头，在进行泛化测试</p>
<hr>
<h3 id="mae----cvpr2022">MAE  - CVPR2022<a hidden class="anchor" aria-hidden="true" href="#mae----cvpr2022">#</a></h3>
<p><em><strong>Masked Autoencoders Are Scalable Vision Learners</strong></em></p>
<p>非对称掩码自动编码器；Encoder和Decoder架构可以不同</p>
<p>(生成式无监督学习)</p>
<p>CV领域的Bert</p>
<p>⭐NLP与CV的不同：</p>
<ul>
<li>
<p>信息密度不同</p>
<ul>
<li>
<p>NLP：句子信息语义很高，信息密度也高。（人类语言-事先浓缩过的信息）</p>
</li>
<li>
<p>视觉：信息很冗余，也没高级的语义（自然界），像素可以被相邻的重建恢复</p>
</li>
</ul>
</li>
<li>
<p>恢复难度不一致</p>
<ul>
<li>恢复高级语义单词和恢复像素级(低级语义)图片，难度也不一样。</li>
</ul>
</li>
</ul>
<p><em><strong>框架：</strong></em></p>
<p><img alt="image-20240409131436588" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240409131436588.png"></p>
<p>Encoder：位置编码所有Patch都加上。但仅输入未被Mask的Patch。并且Mask比例很高(论文mask75%patch)，迫使模型学到高维的特征，而非捷径。</p>
<p>Decoder：Mask Patch是自学习的向量，并且和Encoded Patch在位置上一致，再次添加位置信息。</p>
<p>简单实现（shuffle，截取前面的作为Encoder输入，后面Patch被Mask；再shuffle逆操作，将Encoded Patch和learnabel patch位置对齐组合好进行Decoder。再重建像素）</p>
<p>通过预测每个屏蔽补丁的像素值来重建输入。解码器的最后一层是线性投影，其输出通道的数量等于补丁中像素值的数量。</p>
<p><strong>损失函数</strong>计算像素空间中重建图像和原始图像之间的<strong>均方误差</strong>。</p>
<p>问题：</p>
<p>​	预训练的输入中具有很大一部分掩码标记，而这在下游任务未损坏的图像中不存在。这种差距可能会降低部署的准确性。</p>
<p><strong>自监督学习</strong>方法通常侧重于预训练的不同借口任务。</p>
<p><strong>对比学习</strong>对两个或多个视图之间的图像相似性和相异性进行建模。</p>
<hr>
<h3 id="wav2vec-20----neurips-2020">wav2vec 2.0  - NeurIPS 2020<a hidden class="anchor" aria-hidden="true" href="#wav2vec-20----neurips-2020">#</a></h3>
<p><em><strong>wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</strong></em></p>
<p><strong>时序信号版本的Bert</strong></p>
<p>[自监督]学习通用特征 =&gt; 再微调任务头</p>
<p>(判别式无监督学习)</p>
<p><em><strong>总体框架：</strong></em></p>
<p><img alt="image-20240411195125065" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240411195125065.png"></p>
<p>1️⃣ 原始信号[X] =CNN=&gt; 浅在特征表示 [Z]</p>
<p>2️⃣ 浅在特征表示 [Z] =Transformer Encoder=&gt; 全局上下文特征表示 [C]</p>
<p>3️⃣ 浅在特征表示 [Z] =量化器=&gt; 对比目标[Q]</p>
<ul>
<li>把原来连续的特征空间假设是d维，拆分成G个子空间（codebook），每个子空间维度是d/G。</li>
<li>然后分别在每个子空间里面聚类（K-mean什么的），一共获得V个中心和其中心特征。</li>
<li>每个类别的特征用其中心特征代替。</li>
</ul>
<p><img alt="image-20240411204733337" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240411204733337.png"></p>
<p><img alt="image-20240411204739181" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240411204739181.png"></p>
<p>量化qt和对应ct</p>
<p>❌ Quantization module 部分不理解</p>
<p><strong>Mask</strong></p>
<p><img alt="image-20240411210825518" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240411210825518.png"></p>
<p>随机起点，遮挡后面t个时间步</p>
<p><img alt="image-20240411210806211" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240411210806211.png"></p>
<p><em><strong>对比损失</strong></em></p>
<p>​		<img alt="image-20240411210912407" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240411210912407.png"></p>
<p>包括 qt 和 K 个干扰项</p>
<p><em><strong>❗❗❗理解不太清晰</strong></em></p>
<hr>
<h3 id="whisper----2022-openai"><strong>Whisper  - 2022 OpenAI</strong><a hidden class="anchor" aria-hidden="true" href="#whisper----2022-openai">#</a></h3>
<p><em><strong>Robust Speech Recognition via Large-Scale Weak Supervision</strong></em></p>
<p>大力出奇迹</p>
<p><em><strong>模型结构</strong></em></p>
<p><img alt="image-20240411211708389" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240411211708389.png"></p>
<p><em><strong>多任务训练</strong></em></p>
<ul>
<li>
<p>[英文口语=&gt; 英文]                            语音识别</p>
</li>
<li>
<p>[多语言口语 =&gt; 英文]                       语音识别 +翻译</p>
</li>
<li>
<p>[多语言口语 =&gt; 对应语言文字]        语音识别</p>
</li>
<li>
<p>识别背景音(无内容声音)</p>
</li>
</ul>
<p>⭐⭐⭐<strong>信号 =&gt; Log-Mel Spectrogram(频谱图)</strong></p>
<p>音素</p>
<p><em><strong>⭐⭐⭐数据集</strong></em></p>
<p>680k小时，超大数据集。 在此基础上预训练，并且0样本迁移(无需特定任务微调)</p>
<hr>
<h3 id="cpc----machine-learning-2018">CPC  - Machine Learning 2018<a hidden class="anchor" aria-hidden="true" href="#cpc----machine-learning-2018">#</a></h3>
<p><em><strong>Representation Learning with Contrastive Predictive Coding</strong></em></p>
<p><em>Contrastive Predictive Coding   - 无监督学习</em></p>
<p><strong>通过预测未来，学习特征表示</strong>  (学习对(高维)信号不同部分之间的底层共享信息进行编码的表示  - 局部平滑度)</p>
<p>不预测原始信号，而是对高维嵌入依赖建模</p>
<p>(判别式无监督学习)</p>
<p><em><strong>CPC框架</strong></em></p>
<p><img alt="image-20240412150754949" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240412150754949.png">
$$
g_{enc}:  local\ feature\ learning\
g_{ar}:  global\ context\ learning
$$</p>
<p><em><strong>对比学习</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 序列： [x1, x2, x3, x4, x5, x6, x7, x8]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># positive sample</span>
</span></span><span class="line"><span class="cl"><span class="c1"># x1, x2, x3, x4 =&gt; c &lt;=&gt; x5, x6, x7, x8</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># negative sample</span>
</span></span><span class="line"><span class="cl"><span class="c1"># x1, x2, x3, x4 =&gt; c &lt;=&gt; xa, xb, xc, xd (同batch中其他的)</span>
</span></span></code></pre></div><p>迫使模型学习序列间的high level feature，学习这种内部的顺序逻辑关系</p>
<p><strong>互信息公式</strong>  - 衡量两个随机变量之间的相互依赖程度
$$
I(x;c)=\sum_{x,c}p(x,c)\log\frac{p(x|c)}{p(x)}. \
I(x;c): x 与 c 的互信息\
p(x,c): x 和 c 同时发生的联合概率分布\
p(x|c): 给定 c 的条件下，x 发生的条件概率分布\
p(x): x 的边缘概率分布
$$
<strong>InfoNCE Loss</strong></p>
<p><strong>最小化CPC定义的损失 =&gt; 实际上最大化 context c_t 和待预测正样本 X_t 之间的互信息</strong></p>
<p>优化目标：最大化似然概率</p>
<p><em>正相关</em></p>
<p><img alt="image-20240415213652095" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240415213652095.png"></p>
<p><em>近似计算互信息</em></p>
<p><img alt="image-20240415213622018" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240415213622018.png"></p>
<p><em>Loss</em></p>
<p><img alt="image-20240415213722239" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240415213722239.png"></p>
<p>X = {x1, x2, &hellip;, xN} N个负样本，从batch中其他数据中采样</p>
<p>E/X  表示似然概率</p>
<p>最大化互信息 =&gt; 最小化Loss  (互信息下界)</p>
<p><img alt="image-20240415213954622" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240415213954622.png"></p>
<p><strong>完整InfoNCE-Loss</strong>
$$
\text{InfoNCELoss} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{\text{exp}(s(x_i, x_i^+))}{\text{exp}(s(x_i, x_i^+)) + \sum_{j=1}^{K} \text{exp}(s(x_i, x_j^-))}
$$</p>
<p><em>不太了解这个最大化互信息的Loss</em></p>
<hr>
<h3 id="dalle-2----20223-openai"><strong>DALL·E 2</strong>  - 2022.3 OpenAI<a hidden class="anchor" aria-hidden="true" href="#dalle-2----20223-openai">#</a></h3>
<p><em>Hierarchical Text-Conditional Image Generation with CLIP Latents</em></p>
<p><em><strong>层级式图生文</strong></em></p>
<p><strong>模型架构</strong></p>
<p><img alt="image-20240416192447568" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240416192447568.png"></p>
<p><em><strong>描述：</strong></em></p>
<ul>
<li>虚线上是CLIP架构(文本和图像的联合表示空间)，学习图文对的关联信息</li>
<li>虚线下是生成框架，prior模型根据Text Embedding生成出CLIP对应的Image Embedding， decoder(diffussion model)根据Image Embedding进行重建</li>
</ul>
<p><em>⭐分步训练</em></p>
<p><em><strong>prior</strong></em></p>
<p>生成CLIP image Embedding (diffusion model)</p>
<p><img alt="image-20240416194435205" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240416194435205.png"></p>
<p><em><strong>decoder</strong></em></p>
<p>是根据image Embedding生成图片 &ndash; 并且这个decoder是多个堆叠，先生成低分辨率，再高清化  -  (diffusion model)</p>
<p><em><strong>简洁表示：</strong></em></p>
<p><img alt="image-20240416193905706" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240416193905706.png"></p>
<p><em>y - 文字</em></p>
<p><em>x - 图片</em></p>
<p><em>zi - 图片嵌入  （显式生成图像表示可以提高图像多样性，同时将照片真实性和标题相似度的损失降至最低）</em></p>
<hr>
<h3 id="diffusion-model----neurips-2020">diffusion model  - NeurIPS 2020<a hidden class="anchor" aria-hidden="true" href="#diffusion-model----neurips-2020">#</a></h3>
<p><em><strong>Denoising Diffusion Probabilistic Models</strong></em></p>
<p><strong>框架：</strong></p>
<p><em><strong>加噪 + 去噪(还原)</strong></em></p>
<p><img alt="image-20240417165835467" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240417165835467.png"></p>
<p><em><strong>正向过程：</strong></em></p>
<p>1️⃣ Xt 是 前一张图片加噪生成的，Z1是服从正太分布的噪声</p>
<p><img alt="image-20240417170233229" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240417170233229.png"></p>
<p><img alt="image-20240417170238304" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240417170238304.png"></p>
<p>βt是超参数=范围为[0.0001,0.02]递增，则αt 是随时间减少， 表示公式一中原图信息越来越少，噪声越来越重</p>
<p>2️⃣ 递推带入一下</p>
<p><img alt="image-20240417170436248" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240417170436248.png"></p>
<p><img alt="image-20240417170509368" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240417170509368.png"></p>
<p>最后可得···</p>
<p><img alt="image-20240417170704121" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240417170704121.png"></p>
<p>Zt_hat 是一个服从正太分布的随机噪声，at_hat = at*at-1*···*a1, <strong>可由X0直接产生任意时间步的加噪图片</strong></p>
<p><strong>反向去噪过程：</strong></p>
<p><em>核心基础</em></p>
<p><img alt="image-20240417171006179" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240417171006179.png"></p>
<p>1️⃣ 用Xt生成Xt-1 ，按贝叶斯公式转换</p>
<p><img alt="image-20240417171023360" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240417171023360.png"></p>
<p>2️⃣ q(Xt|Xt-1) == q(Xt|Xt-1, X0)， 而q(Xt-1) == q(Xt-1|X0) <strong>任意步加噪图可由原图直接产生</strong></p>
<p><img alt="image-20240417171223592" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240417171223592.png"></p>
<p>3️⃣ 反解公式7， X0可由Xt进行估计</p>
<p><img alt="image-20240417171845271" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240417171845271.png"></p>
<p>4️⃣ 带入并整理</p>
<p><img alt="image-20240417172918885" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240417172918885.png"></p>
<p>··· 因此可根据Xt =&gt; Xt-1， 下式为最终的<strong>去噪公式</strong>
$$
x = \frac{1}{\sqrt{\alpha}} \left( x - \frac{1 - \alpha}{\sqrt{1 - \alpha_{\text{hat}}}} \cdot \text{predicted_noise} \right) + \sqrt{\beta} \cdot \text{noise}
$$
β noise 保证多样性</p>
<p><em>Ɛθ表示预测模型</em></p>
<p><img alt="image-20240417175432578" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240417175432578.png"></p>
<p><em><strong>代码逻辑</strong></em></p>
<p>训练：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">images</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pbar</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">t</span> <span class="o">=</span> <span class="n">diffusion</span><span class="o">.</span><span class="n">sample_timesteps</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>	<span class="c1"># 采样几个时间步进行训练</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_t</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">diffusion</span><span class="o">.</span><span class="n">noise_images</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>	<span class="c1"># @ 公式-7</span>
</span></span><span class="line"><span class="cl">    <span class="n">predicted_noise</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>	<span class="c1"># Unet</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sampled_images</span> <span class="o">=</span> <span class="n">diffusion</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># @ 去噪公式</span>
</span></span></code></pre></div><p><em>预测模型</em></p>
<p>Unet 带时间点嵌入(用余弦-位置嵌入实现的)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_dim</span><span class="p">)</span>		<span class="c1"># 嵌入时间步顺序</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>	    <span class="c1"># cnn</span>
</span></span><span class="line"><span class="cl">    <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">down1</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>  <span class="c1"># pooling</span>
</span></span><span class="line"><span class="cl">    <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa1</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>	    <span class="c1"># self attention</span>
</span></span><span class="line"><span class="cl">    <span class="n">x3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">down2</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa2</span><span class="p">(</span><span class="n">x3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">down3</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa3</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bot1</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>	
</span></span><span class="line"><span class="cl">    <span class="n">x4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bot2</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bot3</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up1</span><span class="p">(</span><span class="n">x4</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>	<span class="c1"># 插值上采样，再拼接skip connect</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa6</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></div><p>p(Xt-2|Xt-1, t, y)  t是时间步嵌入，y是条件嵌入</p>
<p>最简单的融合方法就是相加</p>
<p>？？？ 疑惑点 - 反向过程求的t时刻的均值方差用在哪了？</p>
<p>2025/1/14 更新：</p>
<p>// X =&gt; X_noise_t  可以一步生成，可以时间步t可以直接生成对应t时间步的噪声图。</p>
<p>// 使用U-net预测时间步t的噪声 使用MSE-Loss进行训练</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 训练循环</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">x_0</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>  <span class="c1"># x_0 是原始数据</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 1. 随机选择一个时间步 t</span>
</span></span><span class="line"><span class="cl">        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">x_0</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),))</span>  <span class="c1"># 为每个样本随机选择一个时间步</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 2. 前向过程：添加噪声</span>
</span></span><span class="line"><span class="cl">        <span class="n">epsilon</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_0</span><span class="p">)</span>  <span class="c1"># 采样噪声</span>
</span></span><span class="line"><span class="cl">        <span class="n">alpha_bar_t_t</span> <span class="o">=</span> <span class="n">alpha_bar_t</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 选择对应时间步的 alpha_bar_t</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alpha_bar_t_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_0</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_bar_t_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">epsilon</span>  <span class="c1"># 添加噪声</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 3. 反向过程：预测噪声</span>
</span></span><span class="line"><span class="cl">        <span class="n">predicted_epsilon</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>  <span class="c1"># 模型预测噪声</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 4. 计算损失函数</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">predicted_epsilon</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>  <span class="c1"># 均方误差损失</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 5. 反向传播和优化</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span></code></pre></div><p>// 串行，一步一步去噪</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 生成过程</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_samples</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">alpha_bar_t</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">x_T</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>  <span class="c1"># 从标准正态分布中采样初始噪声</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>  <span class="c1"># 从 T-1 到 1</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 预测噪声</span>
</span></span><span class="line"><span class="cl">        <span class="n">epsilon</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_T</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算去噪后的数据   &lt;= 消除噪声</span>
</span></span><span class="line"><span class="cl">        <span class="n">alpha_t</span> <span class="o">=</span> <span class="n">alpha_bar_t</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">/</span> <span class="n">alpha_bar_t</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_T</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_T</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alpha_t</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x_T</span>
</span></span></code></pre></div><hr>
<h3 id="time-frequency-consistency----neurips-2022">Time-Frequency Consistency  - NeurIPS 2022<a hidden class="anchor" aria-hidden="true" href="#time-frequency-consistency----neurips-2022">#</a></h3>
<p><em><strong>Self-Supervised Contrastive Pre-Training for Time Series via Time-Frequency Consistency</strong></em></p>
<p>期望同一示例的基于<strong>时间</strong>和基于<strong>频率</strong>的表示在<strong>时频空间中靠近</strong>在一起</p>
<p>(判别式无监督学习)</p>
<p>pretext task：实例判别 (一对正样本，其余负样本)</p>
<p><em><strong>框架</strong></em></p>
<p><img alt="image-20240423132039482" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240423132039482.png"></p>
<p>时域增强：基于时间特性从 xi 扩充，包括抖动、缩放、时移和邻域分段；</p>
<p>频域增强：频谱特征扰动 xFi 的增强，添加或删除频率分量来扰动频谱 (确保扰动时间序列仍然与原始样本在频域和时域仍相似)</p>
<p>Loss：</p>
<p><img alt="image-20240423133548048" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240423133548048.png"></p>
<p>余弦相似度：衡量两个向量之间相似性，范围[-1, 1]</p>
<p><strong>// 补充 2025/1/14</strong></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20250114163629763.png" alt="image-20250114163629763" style="zoom:40%;" />
<p>样本越相似  =&gt; sim(i, j) 越大 =&gt; exp(sim) 越大 =&gt;  exp(sim)/\sum(exp) 越接近于1 =&gt; log(·)越接近于0</p>
<p>-log(·) 将图像倒置，样本越<strong>不</strong>相似  =&gt; exp(sim)/\sum(exp) 越接近于0 =&gt; loss 越大</p>
<hr>
<h3 id="comet----neurips--2023">COMET  - NeurIPS  2023<a hidden class="anchor" aria-hidden="true" href="#comet----neurips--2023">#</a></h3>
<p><em><strong>Contrast Everything A Hierarchical Contrastive Framework for Medical Time-Series</strong></em></p>
<p>(判别式无监督学习)</p>
<ul>
<li>我们的方法旨在弥合标记数据的有限可用性与医疗时间序列分析中稳健且可概括的模型的需求之间的差距</li>
<li>对比表示学习背后的关键思想是通过将相似的数据靠近在一起并将不相似的数据进一步分开来挖掘数据一致性</li>
</ul>
<p>关键是要利用所有可用的信息；除了样本标签之外，数据集是否还有其他信息？(补充信息)</p>
<p><strong>患者间、患者内进行测试(定义打乱规则)</strong></p>
<p><strong>⭐⭐⭐多级信息</strong></p>
<p><img alt="image-20240424172452759" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240424172452759.png"></p>
<p><strong>多级对比学习框架</strong></p>
<p><img alt="image-20240424172549571" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240424172549571.png"></p>
<p><em><strong>代码分析总结</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">X_train</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="p">[</span><span class="n">Batch</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">segment</span><span class="p">]</span>  <span class="c1"># segment length 330, sampling_rate = 250Hz</span>
</span></span><span class="line"><span class="cl"><span class="n">y_train</span>  <span class="c1"># 心肌梗塞二分类标签， patient_id， segment_id</span>
</span></span></code></pre></div><p><em>细分为样本级别(心跳)，故图示Encoder不同级别对比学习可复用</em></p>
<p><strong>关键，计算不同级别的对比Loss</strong></p>
<p><em><strong>Patient-Level Loss</strong></em></p>
<p><img alt="image-20240424205635784" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240424205635784.png"></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">Batch</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">segment</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">out1</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">out2</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># ? 模拟数据增强后的不同视角或版本，以便在对比学习中生成有效的正例和负例</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 根据y_train 病人id，构建mask矩阵</span>
</span></span><span class="line"><span class="cl"><span class="n">pid1</span><span class="p">,</span> <span class="n">pid2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">str_pid</span><span class="p">,</span> <span class="n">str_pid</span><span class="p">)</span>	
</span></span><span class="line"><span class="cl"><span class="n">pid_matrix</span> <span class="o">=</span> <span class="n">pid1</span> <span class="o">+</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="n">pid2</span>		<span class="c1">#  每项为 id_x - id_y	</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 目标位置 id_x - id_x</span>
</span></span><span class="line"><span class="cl"><span class="n">pids_of_interest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">str_pid</span> <span class="o">+</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="n">str_pid</span><span class="p">)</span>  <span class="c1"># unique combinations of pids of interest i.e. matching </span>
</span></span><span class="line"><span class="cl"><span class="n">bool_matrix_of_interest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">str_pid</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">str_pid</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">pids_of_interest</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">bool_matrix_of_interest</span> <span class="o">+=</span> <span class="n">pid_matrix</span> <span class="o">==</span> <span class="n">pid</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 上三角和下三角目标row，col</span>
</span></span><span class="line"><span class="cl"><span class="n">rows1</span><span class="p">,</span> <span class="n">cols1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">bool_matrix_of_interest</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># upper triangle same patient combs</span>
</span></span><span class="line"><span class="cl"><span class="n">rows2</span><span class="p">,</span> <span class="n">cols2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">bool_matrix_of_interest</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># down triangle same patient combs</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="n">out1</span><span class="p">,</span> <span class="n">out2</span>  <span class="o">=&gt;</span> <span class="n">sim_matrix_exp</span>  <span class="c1"># 余弦相似度矩阵</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># @@ 对比学习：不考虑对角线元素的原因主要是因为对角线元素表示的是特征向量与其自身的相似度</span>
</span></span><span class="line"><span class="cl"><span class="c1"># @@ 我们关注的是如何使具有相同标签的不同特征向量之间更相近，而使不同标签的特征向量之间更疏远</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Loss 分母， 某样本与其他样本相似度之和，  @分上三角和下三角</span>
</span></span><span class="line"><span class="cl"><span class="n">triu_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sim_matrix_exp</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># add column</span>
</span></span><span class="line"><span class="cl"><span class="n">tril_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sim_matrix_exp</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># add row</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">loss_terms</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 取平均 </span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rows1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">triu_elements</span> <span class="o">=</span> <span class="n">sim_matrix_exp</span><span class="p">[</span><span class="n">rows1</span><span class="p">,</span> <span class="n">cols1</span><span class="p">]</span>  <span class="c1"># row and column for upper triangle same patient combinations</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss_triu</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">triu_elements</span> <span class="o">/</span> <span class="n">triu_sum</span><span class="p">[</span><span class="n">rows1</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_triu</span>  <span class="c1"># technically need to add 1 more term for symmetry</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss_terms</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">/</span><span class="n">loss_terms</span>
</span></span></code></pre></div><p><em><strong>Trial-Level Loss</strong></em></p>
<p>同上，只不过patient_id =&gt; segment_id</p>
<p>❌❌❌<em>Sample-Level Loss &amp; Observation-Level Loss</em></p>
<p>看不懂源码~ 😎😭</p>
<hr>
<h3 id="ts2vec----aaai-22">TS2Vec  - AAAI 22<a hidden class="anchor" aria-hidden="true" href="#ts2vec----aaai-22">#</a></h3>
<p><em><strong>TS2Vec: Towards Universal Representation of Time Series</strong></em></p>
<p>(判别式无监督学习)</p>
<p>现存工作局限：</p>
<ul>
<li>它们都没有以不同尺度的时间序列为特征来捕获尺度不变的信息，而这对于时间序列任务的成功至关重要。多尺度特征可以提供不同级别的语义并提高学习表示的泛化能力</li>
<li>粗粒度表示 - 整个时间序列，可能没那么细致</li>
</ul>
<p>之前工作的问题</p>
<p><img alt="image-20240425195237484" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240425195237484.png"></p>
<p>正样本对会误判</p>
<ul>
<li>当存在水平偏移时，子系列一致性很容易受到攻击 (左图)</li>
<li>当出现异常时，时间一致性可能会引入误报对 (右图)</li>
</ul>
<p>​</p>
<p>创新：</p>
<ul>
<li>TS2Vec 中的对比目标基于<strong>增强上下文视图</strong>，即相同子系列在两个增强上下文中的表示应该是一致的 <strong>(上下文语境下)</strong></li>
<li>层级式对比Loss，由细粒度=&gt;粗粒度，局部到全局  # 学习各种语义级别的任意子系列的上下文表示，灵活且通用的表示。 顶级语义级别的对比使模型能够学习实例(样本)级表示</li>
</ul>
<p><em><strong>框架</strong></em></p>
<p><img alt="image-20240425171329665" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240425171329665.png"></p>
<p>流程：</p>
<ul>
<li>实例(一段信号)，分两个重叠的子序列,  [batch, channel, dim_feature]</li>
<li>投影，[batch, channel, dim_feature] =&gt; [batch, channel, dim_hidden]</li>
<li>随机Mask掉部分信号</li>
<li>CNN-Encoder</li>
</ul>
<p>⭐⭐⭐  Hierarchical Contrasting Loss</p>
<ul>
<li><em><strong>instance &amp; temporal contrastive loss</strong></em></li>
</ul>
<p>​		<img alt="image-20240425172445164" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240425172445164.png"></p>
<p>​		<em>用 -F.log_softmax(x, dim=-1) 实现</em></p>
<ul>
<li>
<p>z1 = F.max_pool1d(z1.transpose(1, 2), kernel_size=2).transpose(1, 2)
z2 = F.max_pool1d(z2.transpose(1, 2), kernel_size=2).transpose(1, 2)</p>
<p>编码特征浓缩，<strong>多尺度</strong>的Contrast loss</p>
</li>
</ul>
<p><em><strong>图示Loss过程</strong></em></p>
<p><img alt="image-20240425193417634" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240425193417634.png"></p>
<p>子序列是从原始信号中裁剪下来的，并且<strong>有重叠部分</strong></p>
<p><img alt="image-20240425175053049" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240425175053049.png"></p>
<hr>
<h3 id="timesurl----aaai-2024">TimesURL  - AAAI 2024<a hidden class="anchor" aria-hidden="true" href="#timesurl----aaai-2024">#</a></h3>
<p><em><strong>TimesURL: Self-Supervised Contrastive Learning for Universal Time Series Representation Learning</strong></em></p>
<p>代码在TS2Vec上修改</p>
<p>=&gt; 在这里，我们必须提到，重要的时间变化信息，例如趋势和季节，在多次最大池化操作后会丢失，因此顶层对比实际上无法为下游任务捕获足够的实例级信息</p>
<p>=&gt; 掩码重建进行学习实例级信息</p>
<p>(判别式+生成式 混合 无监督学习)</p>
<p><em>观察</em></p>
<p><img alt="image-20240427213825901" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240427213825901.png"></p>
<p>**简单负样本：**大多数时间序列片段可以被视为容易负样本。 这些片段往往表现出与锚点的语义差异，并且仅贡献较小的梯度，因此无法提供有用的区分信息</p>
<p><strong>硬负样本：</strong> 硬负样本就是离正样本很近，并且模型很难区别的</p>
<p>正样本-<strong>硬负样本</strong>-负样本：，这些样本如果让其远离正样本可以大大提高模型性能。 其有效性被大量的简单负样本所掩盖。（现存框架没有特点显示的指出）</p>
<p>由于时间序列中的局部平滑性和马尔可夫特性，大多数负样本很容易不足以捕获时间信息，因为它们从根本上缺乏驱动对比学习所需的学习信号。 作为图 2 中真实示例，对于每个正锚点（红色方块），相应的负样本（灰色标记）包含许多简单的负样本和很少的困难负样本，即许多负片太远，无法造成对比损失。</p>
<p>对比学习的一个关键组成部分是选择适当的增强，这些增强可以施加一些先验来构建可行的正样本，以便编码器可以被训练来学习鲁棒和有区别的表示</p>
<p><strong>频率混合</strong>用于通过将通过快速傅里叶变换（FFT）运算计算出的一个训练实例 xi 中的一定比例的频率分量替换为同一批次中的另一个随机训练实例 xk 的相同频率分量来生成新的上下文视图 （保持病理相同）</p>
<p><strong>随机裁剪</strong>。 <em><strong>重叠两个子序列</strong></em> - 随机裁剪是<strong>上下文一致性策略</strong>的关键步骤。 它可以保持时间序列的重要时间关系和语义一致性</p>
<p><em><strong>创新点：</strong></em></p>
<ul>
<li>提出双Universum概念，就是利用Mix-up增强F(x)，追加硬负样本。</li>
<li>对比学习+自监督掩码重建，联合优化，来捕获段级和实例级信息， 实现通用表示</li>
</ul>
<p>⭐ 第一类包括预测、异常检测和插补，它们更多地依赖于在分段级别捕获的细粒度信息，因为这些任务需要推断特定的时间戳或子序列。细粒度(局部)</p>
<p>⭐ 第二类包括分类和聚类优先考虑实例级信息（即粗粒度信息），旨在推断整个系列的目标。粗粒度(全局)</p>
<p><em style="color:red; font-weight:bolder">实现 : 分段(对比学习，学习片段)，整句(掩码重建，学习整体)</em></p>
<p><em><strong>框架：</strong></em></p>
<p><img alt="image-20240427213747206" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240427213747206.png"></p>
<p><em><strong>AUG：</strong></em></p>
<p>​	频率增强，<strong>x =&gt; fft() =mask+fusion=&gt; ifft() =&gt; y</strong>，①该篇论文fusion是融合同batch其他信号的某些部分 =&gt; 领域创新 （fusion的信号应该和这个信号相同病理，扩张数据分布）</p>
<p><em><strong>DualConv：</strong></em></p>
<p>​	原始信号的两个增强子视图，z1 = Encoder(x1),  	z1&rsquo; = Encoder(x1&rsquo;),  =&gt; mix-up option =&gt; <strong>z1_mix = α × z1 + (1-α) × z1[torch.randperm(z1.shape[0])]</strong>，z1&rsquo;_mix。</p>
<p>​	[z1, z1&rsquo;, z1_mix, z1&rsquo;mix] @ [z1, z1&rsquo;, z1_mix, z1&rsquo;mix].T   =&gt; sim =&gt; <strong>-F.log_softmax(sim)</strong></p>
<p>​	loss:  俩视图对应段为正样本(分子)</p>
<p><img alt="image-20240427220130014" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240427220130014.png"></p>
<p>负样本在母分，x_mix做负样本增强。</p>
<hr>
<h3 id="mixup----2017-machine-learning">Mixup  - 2017 Machine Learning<a hidden class="anchor" aria-hidden="true" href="#mixup----2017-machine-learning">#</a></h3>
<p><em><strong>mixup: BEYOND EMPIRICAL RISK MINIMIZATION</strong></em></p>
<p>一种简单并且不需要专业领域知识的数据增强</p>
<p>现存问题讨论：</p>
<ul>
<li>过拟合(大模型，直接记忆Train data，走捷径)                                          - overfitting</li>
<li>精心设计样本(对抗性例子，人难以察觉，但模型会给出错误的答案)      - generalize</li>
</ul>
<p>👇</p>
<p><strong>ERM</strong>(经验风险最小化原则)问题</p>
<ul>
<li>一方面，即使存在强正则化，ERM 也允许大型神经网络记忆（而不是归纳）训练数据</li>
<li>另一方面，使用 ERM 训练的神经网络在对训练分布之外的示例进行评估时会极大地改变其预测。</li>
</ul>
<p>CV: 图像的邻近区域定义为其水平反射、轻微旋转和轻微缩放的集合</p>
<p>=&gt;  数据增强始终可以提高泛化能力, 但该过程依赖于数据集，因此需要使用专家知识 （不同领域增强不一定通用）</p>
<p>数据增强假设邻近的示例共享同一类，并且不会对不同类的示例之间的邻近关系进行建模。(聚类)</p>
<p>=&gt; 邻近风险最小化 (<strong>VRM</strong>) 原则</p>
<p>⭐=&gt; 从训练样例的邻近分布中提取额外的虚拟样例，以扩大训练分布的支持度</p>
<p><em><strong>方法：</strong></em></p>
<p><img alt="image-20240428175535852" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240428175535852.png"></p>
<p><strong><em>框架伪代码</em>：</strong></p>
<p><img alt="image-20240428175500700" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240428175500700.png"></p>
<p><em>图例：</em></p>
<p><img alt="image-20240428175951150" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240428175951150.png"></p>
<p>虚拟数据，让数据边界过渡； 当不在Train数据的分布出现时，降低不确定性。 稍微清晰化边界</p>
<hr>
<h3 id="yolo-v1----cvpr-2016">YOLO-v1  - CVPR 2016<a hidden class="anchor" aria-hidden="true" href="#yolo-v1----cvpr-2016">#</a></h3>
<p><em><strong>You Only Look Once: Unified, Real-Time Object Detection</strong></em></p>
<p>⭐将目标检测视作回归问题  =&gt; 预测出来</p>
<p><em><strong>前向推理</strong></em></p>
<p><em>Model:</em></p>
<p><img alt="image-20240429203859825" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240429203859825.png"></p>
<p><em>input: [3, 448, 448]  =&gt; output: [30, 7, 7]</em></p>
<p><img alt="image-20240429204129869" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240429204129869.png"></p>
<p><em>置信度(confidence): 这个值代表了模型认为预测的边界框内存在对象的概率</em></p>
<p><em>框的中心坐标 + 宽高</em></p>
<p><em>框中的物体是什么类</em></p>
<p><img alt="image-20240429210124912" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240429210124912.png"></p>
<p><em><strong>非极大值抑制</strong></em>  （最佳：每个类别独立执行非极大值抑制，从而更精确地处理多类别情况）</p>
<ol>
<li><strong>置信度排序</strong>：首先将所有的预测边界框按照它们的置信度（confidence scores）进行降序排序。</li>
<li><strong>选择最高置信度边界框</strong>：从排序后的列表中选择置信度最高的边界框作为参考框（reference box）。</li>
<li><strong>计算IOU</strong>：计算选中的参考框与列表中其他所有边界框的交并比（IOU）。交并比是两个边界框的交集面积与它们的并集面积的比值。</li>
<li><strong>抑制</strong>：如果参考框与任何其他边界框的IOU超过预先设定的阈值（通常设置为0.5），那么这些边界框会被认为是多余的，并从列表中删除。</li>
<li><strong>重复步骤</strong>：从剩余的边界框列表中再次选择置信度最高的边界框，重复上述过程，直到所有的边界框都被处理完毕。</li>
<li><strong>最终结果</strong>：经过非极大值抑制后，剩余的边界框被认为是对目标位置的最佳预测，它们将被用于最终的目标检测输出。</li>
</ol>
<p><em><strong>训练：</strong></em></p>
<p><img alt="image-20240429210707202" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240429210707202.png"></p>
<p>λcoord = 5,   λnoobj = 0.5,  调整各个部分的重要性
$$
1_{ij}^{obj}: 表示第ij个格子有对象 \
1_{ij}^{noobj}: 表示第ij个格子没有对象\
S^{2}: 图片划分格子 \
B: 每个格子预测多少个框
$$
<em>bounding box loss : 中心点 + 框宽高</em></p>
<p><em>confidence: 格子是否有对象</em></p>
<p><em>classes：格子分类是否正确</em></p>
<hr>
<h3 id="semi-supervised-hybrid-loss-----machine-learning-2023">Semi-Supervised Hybrid Loss  -  Machine Learning 2023<a hidden class="anchor" aria-hidden="true" href="#semi-supervised-hybrid-loss-----machine-learning-2023">#</a></h3>
<p><em><strong>Semi-Supervised End-To-End Contrastive Learning For Time Series Classification</strong></em></p>
<p>1️⃣无标签数据对比学习(增强视图一致性)，2️⃣有标签对比学习(相同种类一致性)，3️⃣有标签分类监督学习</p>
<p>(判别式无监督学习 + 有监督学习)</p>
<p><strong>框架对比</strong></p>
<p><img alt="image-20240504141408686" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240504141408686.png"></p>
<p>⭐ <em><strong>End to End</strong></em></p>
<p><strong>框架</strong></p>
<p><img alt="image-20240504141522823" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240504141522823.png"></p>
<p>Unlabeled Sample : 使用两个增强视图作为positive pair，与其他sample为negative pair (标准的对比学习)</p>
<p>Labeled Sample：1️⃣ 同类型的sample为positive pair，不同类型的sample为negative pair.  2️⃣过分类头，计算分类Loss</p>
<p>❤️ 混合上述三个Loss，联合优化<strong>Encoder</strong></p>
<hr>
<h3 id="simclr----2020">SimCLR  - 2020<a hidden class="anchor" aria-hidden="true" href="#simclr----2020">#</a></h3>
<p><em><strong>A Simple Framework for Contrastive Learning of Visual Representations</strong></em></p>
<p><em>贡献：</em></p>
<ul>
<li>
<p><strong>数据增强</strong>对于对比学习至关重要 (裁剪缩放，翻转，颜色紊乱，旋转， 掩盖， 高斯噪声， 高斯模糊，Sobel 滤波)  - 裁剪缩放+颜色紊乱 比较好</p>
</li>
<li>
<p>在经过Resnet编码器后，追加MLP能增强模型性能</p>
</li>
<li>
<p>样本x，增强视图xi和xj(正样本)，batch size =N，一共2N的增强视图，对于某个样本x，xi和xj为正样本，和batch中剩余的样本的增强为负样本</p>
<p>(大batchsize, 性能更好， 全局BN)</p>
</li>
</ul>
<p><em><strong>样本自成一类</strong></em>，来尽可能地让编码器找到图像中最重要的特征</p>
<p><em>框架</em></p>
<p><img alt="image-20240504192858744" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240504192858744.png"></p>
<p>共享参数 shared weight</p>
<p><em>Loss</em></p>
<p><img alt="image-20240504192936596" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240504192936596.png"></p>
<p>上面是正样本对，下面是负样本对
-log_softmax() =&gt; 挑选出需要的值</p>
<p><em>算法</em></p>
<p><img alt="image-20240504193222346" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240504193222346.png"></p>
<hr>
<h3 id="vilt----2021">ViLT  - 2021<a hidden class="anchor" aria-hidden="true" href="#vilt----2021">#</a></h3>
<p><em><strong>ViLT Vision-and-Language Transformer Without Convolution or Region Supervision</strong></em></p>
<p>极简结构的图片文多模态融合</p>
<p>速度限制-问题分析</p>
<p><img alt="image-20240506202622982" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240506202622982.png"></p>
<p>归纳总结：</p>
<p><img alt="image-20240506201201840" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240506201201840.png"></p>
<p>(a) vision embedding 参数量 &gt; Text Embedding &gt; Modality Interaction  # 缺点，视觉嵌入太重(比重太大)，并且融合非常简单即点乘算相似度</p>
<p>(b) vision embedding和Text embedding 占比差不多  &gt; Modality Interaction   # 模态融合之前，工作太繁杂，而且前抽取特征不好，限制后面融合，并且不重视后面的模态融合操作。</p>
<p>(c) 重视visual  Embed和后期的modality interaction，# text 和 vision不均等，重要性不平衡</p>
<p>=&gt; 简单框架，Text词嵌入(bert中的BertEmbeddings加载训练后的权重)，vision用patch projection，都很快</p>
<p>⭐ 1. 图像和文本前期嵌入应该有相似均匀的表达能力 2. 这两种模态是否在深层网络中相互作用。</p>
<p>模型框架：</p>
<p><img alt="image-20240506202930648" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240506202930648.png"></p>
<p><em>初始化参数-ViT，而不是bert</em></p>
<p>优化目标(主要)：</p>
<ul>
<li>Image Text Matching：0.5概率将图片替换为与文本不匹配的图片，预测一致性(二分类问题)</li>
<li>Masked Language Modeling：预测被遮掩的词</li>
</ul>
<p>text cls: 预测图文是否一致，二分类</p>
<p>text token set： 全局上下文=&gt;预被掩词</p>
<p>text token set 和 visual token set：进行对齐Loss</p>
<hr>
<h3 id="beit-v3-2022">BEIT-v3 2022<a hidden class="anchor" aria-hidden="true" href="#beit-v3-2022">#</a></h3>
<p><em><strong>Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks</strong></em></p>
<p>统一Vision 和 NLP</p>
<p>⭐ 核心思想是图像可以被建模为一门外语，这样我们就可以对图像、文本和图文对进行统一的掩码“语言”建模。</p>
<p><strong>结果非常好</strong></p>
<p><img alt="image-20240506221847766" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240506221847766.png"></p>
<p><em>基础块</em></p>
<p><img alt="image-20240506221730469" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240506221730469.png"></p>
<p>共享注意力矩阵(都是一个物体的不同视角)，但是最后的FFN各个模态专享</p>
<p><em>拓展到不同的模态</em>：</p>
<p><img alt="image-20240506221937618" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240506221937618.png"></p>
<p><em>任务</em>：</p>
<p><strong>图像字幕任务</strong>：采用了特殊的自注意力掩模。 <em><strong>图像标记（即图像块）只能在图像序列内双向相互关注</strong></em>。 <em><strong>标题的标记可以关注图像标记、它们的左侧标题标记以及它们本身</strong></em>。 在微调过程中，我们随机屏蔽一定比例的标题标记。 该模型经过训练，可以根据图像的线索及其左侧标题上下文来恢复这些标记。 我们还屏蔽了特殊的边界标记 [SEP]，以帮助模型学习终止生成</p>
<p><strong>视觉问答：</strong> 将任务表述为分类问题。 该模型经过训练，可以从训练集中 3129 个最常见的候选答案中预测答案。我们将给定问题和图像的嵌入连接起来，然后将输入嵌入输入多路转换器以联合编码图像-问题对。 最终的池化输出被输入到分类器层来预测答案。</p>
<p>**图像文本检索任务：**是测量图像和文本之间的相似度。 根据检索目标的模态，有两个方向：图像到文本检索和文本到图像检索。 <em>双编码器模型</em>分别对图像和文本进行编码以获得它们的表示。 然后我们计算这些表示的余弦相似度分数。</p>
<p><strong>图像分类：</strong> 将该任务制定为图像到文本检索任务。 我们使用类别名称作为文本来构建图像-文本对。 BEIT-3 被训练为双编码器，以找到图像最相关的标签。 在推理过程中，我们首先计算可能的类名的特征嵌入和图像的特征嵌入。 然后计算它们的余弦相似度分数以预测每个图像最可能的标签。</p>
<hr>
<h3 id="albef----2021">ALBEF  - 2021<a hidden class="anchor" aria-hidden="true" href="#albef----2021">#</a></h3>
<p><em><strong>Align before Fuse</strong>: Vision and Language Representation Learning with Momentum Distillation</em></p>
<p>现存问题：大多数现有方法采用基于变压器的多模态编码器来联合建模-视觉Token（基于区域的图像特征）和文本Token。 <strong>由于视觉标记和单词标记未对齐</strong>，因此多模态编码器学习图像文本交互具有挑战性。</p>
<p>（1）图像特征和文本符号映射仍然停留在他们自己的空间，使得多模态编码器很难学习建模他们之间的交互；</p>
<p>（2）物体检测器 &mdash; 标注费钱，使用费算力 &mdash; 在预训练阶段需要标注矩形框，在推理阶段高分辨率图像，如 600*1000，速度较慢；</p>
<p>（3）广泛使用的 image-text 数据集均是从网上搜集的带有严重噪声的数据，现有的预训练目标，如 MLM 可能过拟合到文本数据，降低了模型的泛化性能。</p>
<p><em>框架：</em></p>
<p><img alt="image-20240507195518743" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240507195518743.png"></p>
<p>image encoder: ViT(ImageNet预训练参数)  - CLS token</p>
<p>text encoder: Bert(预训练参数)  - CLS token</p>
<p>multimodal encoder: Bert(预训练参数) + cross-attention</p>
<p>⭐ **在传入multi-modal encoder前，使用ITC迫使模型进行对齐 ** <em>align before fuse的align</em></p>
<ul>
<li>对齐图像特征和文本特征，使多模态融合编码器更容易执行跨模态学习</li>
<li>改进了单模态编码器，以更好地理解图像和文本的语义</li>
<li>它学习一个共同的低维空间来嵌入图像和文本，这使得图像文本匹配目标能够通过我们的对比硬负挖掘找到更多信息样本。</li>
</ul>
<p><strong>Image-Text Contrastive Loss：</strong></p>
<p>正样本对：配对的Image-Text</p>
<p>负样本对：Queue存储着的样本表示</p>
<p>Image =&gt; 匹配Text-Queue(Momentum)</p>
<p>Text =&gt; 匹配Image-Queue(Momentum)</p>
<p>(代码中利用了Momentum Distillation &hellip;)</p>
<p><strong>Image-Text Matching:</strong></p>
<p>有了multi-modal encoder输出的 embed token (正确样本的表征) =&gt; 即喂入Multi-modal encoder的 text embed = (positive), Image embed = (positive), 拿<strong>融合的CLS</strong>作为最终表征，过MLP =&gt; 二分类预测是否匹配；这部分Targets=(1, 1, &hellip;, 1)</p>
<p>从同batch中，按相似性大小随机挑选一个(hard)负样本，</p>
<p>然后，text embed = (positive, &hellip;, negetive, &hellip;) , Image embed = (negetive, &hellip;, positive)</p>
<p>multi-modal encoder =&gt; CLS =&gt; MLP =&gt; two probability</p>
<p>Targets = (0, 0, &hellip;, 0)</p>
<p>// 重新梳理如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 图像i-文本j =&gt; 多模态编码器 =&gt; 是否匹配；</span>
</span></span><span class="line"><span class="cl"><span class="n">图像</span> <span class="mi">1</span><span class="o">-</span><span class="n">文本</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">匹配对</span>
</span></span><span class="line"><span class="cl"><span class="n">图像</span> <span class="mi">2</span><span class="o">-</span><span class="n">文本</span> <span class="mi">2</span> <span class="p">:</span> <span class="n">匹配对</span>
</span></span><span class="line"><span class="cl"><span class="n">图像</span> <span class="mi">1</span><span class="o">-</span><span class="n">文本</span> <span class="mi">2</span> <span class="p">:</span> <span class="n">不匹配</span>  <span class="o">//</span> <span class="n">这里使用余弦相似度选取最困难的样本</span>
</span></span><span class="line"><span class="cl"><span class="n">图像</span> <span class="mi">2</span><span class="o">-</span><span class="n">文本</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">不匹配</span>  <span class="o">//</span> <span class="n">这里使用余弦相似度选取最困难的样本</span>
</span></span></code></pre></div><p><strong>Masked Language Modeling</strong>:</p>
<p>屏蔽掉一些词，通过从图片模态信息中预测掉被屏蔽的词(多分类Loss)</p>
<p>这里也借助了图像的信息去更好的恢复被mask掉的单词</p>
<p>【这里只对匹配的对计算 掩码Loss】</p>
<p><img alt="image-20240507203423762" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240507203423762.png"></p>
<p>目的：缓解noisy web data的不足，真正的label不一定有momentum的好</p>
<p><img alt="image-20250115224719284" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20250115224719284.png"></p>
<p>真实label不一定比momentum model给出的predict label好，=&gt; 使用KL散度进行约束 一致性</p>
<p><strong>最大化互信息视角解释：</strong></p>
<p>在自监督学习中，a 和 b 是同一图像的两个增强。 在视觉语言表示学习中，我们将 a 和 b 视为捕获其语义的图像文本对的不同变体。 我们的目标是学习对观点变化不变的表征。</p>
<p>最小化Loss =&gt; 最大化互信息的下限(最大化了图像-文本对的**不同“视图”**之间的互信息（MI）的下限)</p>
<p>InfoNCE Loss</p>
<p><img alt="image-20240507201135974" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240507201135974.png"></p>
<p><em><strong>Image-Text Contrastive Loss</strong></em></p>
<p><img alt="image-20240507201202181" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240507201202181.png"></p>
<p>最大化Text和Image中的互信息， ITC 将两个单独的模态（即 I 和 T）视为图像-文本对的两个视图</p>
<p><em><strong>MLM:</strong></em></p>
<p><img alt="image-20240507201419353" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240507201419353.png"></p>
<p>MLM 将图像-文本对的两个视图视为：(1) 随机选择的单词标记，以及 (2) 图像 + 带有该单词屏蔽的上下文文本。</p>
<hr>
<h3 id="instance-discrimination----2018">Instance discrimination  - 2018<a hidden class="anchor" aria-hidden="true" href="#instance-discrimination----2018">#</a></h3>
<p><em><strong>Unsupervised Feature Learning via Non-Parametric Instance Discrimination</strong></em></p>
<p>首次提出个体判别任务！</p>
<p>观察：</p>
<p><img alt="image-20240508125442186" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240508125442186.png"></p>
<p>在监督学习中。在预测&rsquo;花豹&rsquo;时，预测概率除了&rsquo;花豹&rsquo;，<strong>剩余</strong>预测得分比较高的是&rsquo;美洲虎&rsquo;、&lsquo;猎豹&rsquo;； <strong>最不相似</strong>的是&rsquo;救生艇&rsquo;、&lsquo;购物车&rsquo;、&lsquo;书柜&rsquo;;</p>
<p><em><strong>最高响应的类都是视觉相关的</strong></em></p>
<p>⭐ 并不是语义标签，<strong>而是数据本身的明显相似性使某些类比其他类更接近</strong>；</p>
<p>❗❗❗ <strong>个体判别：将类监督发挥到了极致，并学习了区分各个实例的特征表示。</strong></p>
<p>这些观察结果表明，典型的判别学习方法可以自动<strong>发现语义类别之间的明显相似性，而无需明确指导这样做</strong>。</p>
<p>我们能否通过纯粹的判别学习来学习反映实例之间明显相似性的有意义的度量？ <strong>图像本身就是独特的，并且每个图像都可能与同一语义类别中的其他图像显着不同</strong>。如果我们学会<strong>在没有任何语义类别概念的情况下区分各个实例，我们最终可能会得到一个捕获实例之间明显相似性的表示，就像类明智的监督学习如何仍然保留类之间的明显相似性一样</strong>。</p>
<p><strong>目标</strong>:</p>
<p>​	在没有监督的情况下学习嵌入函数 v = fθ(x)。 fθ 是一个具有参数 θ 的深度神经网络，将图像 x 映射到特征 v。这种嵌入将在图像空间上产生一个度量，对于实例 x 和 y, dθ(x, y) = |fθ(x) − fθ(y)|。 良好的嵌入应该将视觉上相似的图像映射得彼此更接近。 我们新颖的无监督特征学习方法是实例级区分。 我们将每个图像实例视为其自己的不同类，并训练分类器来区分各个实例类。</p>
<p><strong>方法：</strong></p>
<p><img alt="image-20240508130437824" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240508130437824.png"></p>
<p>用一个memory bank存储4096个样本embed feature(128-dimention) 随着网络更新, 目的是让特征在嵌入空间中远离(每一个样本都是一个类)，学习那种有监督时类和类之间相似聚集的现象。</p>
<hr>
<h3 id="byol----20206">BYOL  - 2020/6<a hidden class="anchor" aria-hidden="true" href="#byol----20206">#</a></h3>
<p><em><strong>Bootstrap Your Own Latent A New Approach to Self-Supervised Learning</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">无监督学习</span><span class="err">：</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">判别式</span><span class="err">：</span><span class="n">从增强视图的表示中</span><span class="err">，</span><span class="n">他们学会区分同一图像的另一个增强视图的表示和不同图像的增强视图的表示</span> <span class="o">=&gt;</span> <span class="n">这种判别方法通常需要将增强视图的每个表示与许多反例进行比较</span><span class="err">。</span>
</span></span><span class="line"><span class="cl">    <span class="n">生成式</span><span class="err">：</span><span class="n">通过预测同一图像的不同视图</span><span class="err">（</span><span class="n">例如</span><span class="err">，</span><span class="n">不同的随机裁剪</span><span class="err">）</span><span class="n">来学习表示</span> <span class="o">=&gt;</span> <span class="n">图像的增强视图的表示应该能够预测同一图像的另一个增强视图的表示</span><span class="err">。</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><em><strong>方法：</strong></em></p>
<p><img alt="image-20240508172636506" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240508172636506.png"></p>
<p>在线网络θ + 目标网络γ(提供回归目标，γ = α×γ + (1-α)×θ ，指数移动平均 )</p>
<p><strong>yθ</strong>是目标编码器，其余的训练好后丢掉</p>
<p>⭐一张图片的两个增强表示的相同的语义 =&gt; 在高维的嵌入表示中，应该可以预测对方(相近)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">yθ</span><span class="err">：</span><span class="n">Encoder</span>
</span></span><span class="line"><span class="cl"><span class="n">zθ</span><span class="err">：</span><span class="n">Projection</span> <span class="n">head</span>
</span></span><span class="line"><span class="cl"><span class="n">qθ</span><span class="err">：</span><span class="n">Prediction</span> <span class="n">head</span>
</span></span></code></pre></div><p><em><strong>Loss:</strong></em>
$$
注意图像增强t(x)和t^{<code>}x会对等的传给online\ net 和 target\ net\\ Loss: 1/2 × ( || q_{θ}(z_{θ}) - z^{</code>}<em>{ξ}||^{2}  + || q</em>{θ}(z_{θ}) - z^{`}_{ξ}||^{2}  )
$$
<em><strong>Train:</strong></em>
$$
θ：online\ net\ parameters\
ξ：target\ net\ parameters\
θ &lt;- optimizer(θ),\ \ \ ξ &lt;- αξ + (1-α)θ
$$</p>
<hr>
<h3 id="dino---2021">DINO - 2021<a hidden class="anchor" aria-hidden="true" href="#dino---2021">#</a></h3>
<p><em><strong>Emerging Properties in Self-Supervised Vision Transformers</strong></em></p>
<p><img alt="image-20240508213644391" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240508213644391.png"></p>
<p>ViT最后的CLS注意力图示⭐⭐⭐</p>
<p>探讨：<em>质疑自监督学习是否为 Vision Transformer (ViT) 提供了比卷积网络 (convnets) 更突出的新属性？</em></p>
<p>框架：(借鉴BYOL)</p>
<p><img alt="image-20240508213533118" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240508213533118.png"></p>
<p>教师是在训练过程中动态构建的。知识蒸馏就不再被用作自监督预训练的后处理步骤，而是直接作为自监督目标。</p>
<p>其中学生和教师具有相同的架构并在训练期间使用蒸馏。</p>
<p>教师在我们工作中用学生的动量平均值进行更新。</p>
<p>增强策略：</p>
<pre><code>1. 多裁剪策略构建图像的不同扭曲视图或裁剪。 更准确地说，根据给定的图像，我们生成一组 V 的不同视图。 该集合包含两个全局视图 xg 1 和 xg 2 以及几个分辨率较小的局部视图。
1.  所有的裁剪都通过学生传递，而只有全局观点通过老师传递，因此鼓励“局部到全局”的对应。
</code></pre>
<p>伪代码：</p>
<p><img alt="image-20240508213511774" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240508213511774.png"></p>
<p>防止模型坍塌：</p>
<pre><code>1. *对动量教师输出进行居中和锐化，以避免模型崩溃。*
2. **居中（Centering）**：对动量教师的输出进行居中操作是为了减少批次之间的偏差，增加输出的稳定性。具体做法是从每个输出中减去其均值，确保输出围绕零分布，这有助于避免网络输出在特征空间内偏向某一方向，从而降低了模型坍塌的风险。
3. **锐化（Sharpening）**：锐化是通过增加输出分布的峰值来实现的，目的是使模型的输出更加区分明显，即使不同类别之间的区别更加清晰。这通常通过提高输出概率分布的熵来实现，比如可以采用温度调整（temperature scaling）等方法来调整概率分布，使得主要的概率值更加突出，而其他的概率值则相对降低。
</code></pre>
<p>图示：</p>
<p><img alt="image-20240508215946714" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240508215946714.png"></p>
<p><em>不同颜色是不同的注意力头</em></p>
<p><img alt="image-20240508220018510" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240508220018510.png"></p>
<p>无监督注意力更能学到本质！</p>
<hr>
<h3 id="simsiam---cvpr-2021">SimSiam - CVPR 2021<a hidden class="anchor" aria-hidden="true" href="#simsiam---cvpr-2021">#</a></h3>
<p><em><strong>Exploring Simple Siamese Representation Learning</strong></em></p>
<p>简单设计的Siamese(孪生)网络。 我们的极简主义方法的竞争力表明</p>
<p><img alt="image-20240509130843870" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240509130843870.png"></p>
<p>1️⃣ “没有动量编码器的 BYOL”</p>
<p>2️⃣ “没有负样本的 SimCLR“ + stop-grad(⭐这个非常关键， 这个对防止模型坍塌很关键)</p>
<p><em>方法：</em></p>
<p><img alt="image-20240509131933070" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240509131933070.png"></p>
<p>一幅图像的两个增强视图由同一编码器网络 f（主干网络加投影 MLP）处理。 然后在一侧应用预测 MLP-h，在另一侧应用停止梯度操作。 该模型最大化了双方之间的相似性。 它既不使用负对也不使用动量编码器。</p>
<p><em>伪代码：</em></p>
<p><img alt="image-20240509132032286" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240509132032286.png"></p>
<p><em style="font-weight:bold; font-size: 24px">消融</em></p>
<p><em><strong>Loss:</strong></em></p>
<p><em>负的余弦相似度 和 交叉熵</em></p>
<p>相似度：<img alt="image-20240509132907059" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240509132907059.png"></p>
<p>交叉熵：<img alt="image-20240509132841007" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240509132841007.png"></p>
<p><img alt="image-20240509132922553" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240509132922553.png"></p>
<p><img alt="image-20240509132830913" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240509132830913.png"></p>
<p><em><strong>BatchNorm的影响：</strong></em></p>
<p><img alt="image-20240509132146667" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240509132146667.png"></p>
<p><em><strong>BatchSize的影响：</strong></em></p>
<p><img alt="image-20240509132226252" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240509132226252.png"></p>
<p><em><strong>预测头的影响：</strong></em></p>
<p><img alt="image-20240509132304829" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240509132304829.png"></p>
<p><em><strong>Loss的对称性：</strong></em></p>
<p><img alt="image-20240509132550886" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240509132550886.png"></p>
<p>sym对称；asym非对称；asym. 2×(每个图像采样两对来粗略地补偿对称性)</p>
<hr>
<h3 id="hilo-attention----neurips-2022">HiLo Attention  - NeurIPS 2022<a hidden class="anchor" aria-hidden="true" href="#hilo-attention----neurips-2022">#</a></h3>
<p><em><strong>Fast Vision Transformers with HiLo Attention</strong></em></p>
<p>图像中的高频捕获局部精细细节，低频关注全局结构，而多头自注意力层忽略了不同频率的特征。 因此，我们建议通过将头部分为两组来解开注意力层中的高频/低频模式，其中一组通过每个<strong>局部窗口内的自注意力对高频进行编码</strong>，另一组通过在每个局部窗口内执行<strong>全局注意力来对低频进行编码</strong>，输入特征图中每个窗口和每个查询位置的<strong>平均池化低频键和值</strong>。</p>
<p>创新点：</p>
<ul>
<li>将自注意力分为高频和低频
<ul>
<li>高频捕捉局部精细细节（轮廓、边缘）</li>
<li>低频捕获整体结构or趋势</li>
</ul>
</li>
<li>high部分是window级别的attn</li>
<li>low部分是space reduce(经过pool)的粗糙级别的attn</li>
</ul>
<p>图示：</p>
<p><img alt="image-20240519135320979" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240519135320979.png"></p>
<p>伪代码：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">HiLoAttention</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">high</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># window-partition</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_token</span><span class="p">,</span> <span class="n">dim_token</span><span class="p">]</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_window</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">dim_token</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># multi-head</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_window</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">dim_token</span><span class="p">]</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_window</span><span class="p">,</span> <span class="n">num_head</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># do attention</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># reshape to restore</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">low</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>	<span class="c1"># [batch, channel, length]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sr_cnn</span><span class="p">(</span><span class="n">x_</span><span class="p">)</span>        <span class="c1"># [batch, channel, _length] reduce length || avgpool or dwconv</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">()</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_head</span><span class="p">,</span> <span class="n">num_token</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">()[</span><span class="o">...</span><span class="p">]</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_head</span><span class="p">,</span> <span class="o">*</span><span class="n">num_token</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># do attention</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># reshape to restore</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">x1</span> <span class="o">=</span> <span class="n">high</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x2</span> <span class="o">=</span> <span class="n">low</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<h3 id="cmt----cvpr-2022">CMT  - CVPR 2022<a hidden class="anchor" aria-hidden="true" href="#cmt----cvpr-2022">#</a></h3>
<p><em><strong>CMT: Convolutional Neural Networks Meet Vision Transformers</strong></em></p>
<p>注重点：与之前基于 CNN 和基于 Transformer 的模型相比，在准确性和效率方面获得了更好的权衡。</p>
<p>⭐ 由于 patch 大小固定，Transformer 很难显式地提取低分辨率和多尺度特征  =&gt; 图像是二维的（即具有宽度和高度），并且在图像中的每个像素位置都与其周围的像素有关。这种空间局部信息非常重要，例如，边缘检测、纹理分析等都依赖于这种局部关系。=&gt; Patchfiy 后削弱了pixel之间的关系，只补充了Patch间的位置信息。</p>
<p><em>CNN、Transformer 、CNN&amp;Transformer</em></p>
<p><img alt="image-20240519144138963" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240519144138963.png"></p>
<p>在每个阶段，产生层次表示  &ndash; 金字塔结构</p>
<ul>
<li>定制的Stem</li>
<li>Block内部混合CNN和MHSA ❤️ [DWConv(Skip-con)  + SR-MHSA(Skip-con) + IRFFN(Skip-Conv)]</li>
</ul>
<hr>
<h3 id="conformer---2020">Conformer - 2020<a hidden class="anchor" aria-hidden="true" href="#conformer---2020">#</a></h3>
<p><em><strong>Conformer: Convolution-augmented Transformer for Speech Recognition</strong></em></p>
<p>集成了 CNN 和 Transformer 组件以进行端到端识别的架构</p>
<p>分析：</p>
<ul>
<li>虽然 Transformer 擅长对远程全局上下文进行建模，但它们提取细粒度局部特征模式的能力较差；</li>
<li>卷积神经网络（CNN）利用局部信息，在本地窗口上学习共享的基于位置的内核，能够捕获边缘和形状等特征。使用本地连接的限制之一是需要更多的层或参数来捕获全局信息。</li>
</ul>
<p>⭐ 将卷积与自注意力<strong>有机结合</strong>起来。 我们假设全局和局部交互对于参数效率都很重要。 为了实现这一目标，我们提出了一种自注意力和卷积的新颖组合，将实现两全其美</p>
<p><em><strong>architecture</strong></em></p>
<p><img alt="image-20240519150910428" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240519150910428.png"></p>
<p><em><strong>Convolution Module</strong></em></p>
<p><img alt="image-20240519151021832" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240519151021832.png"></p>
<p><em><strong>Feed Forward Module</strong></em></p>
<p><img alt="image-20240519151158840" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240519151158840.png"></p>
<hr>
<h3 id="pathformer----iclr-2024">PathFormer  - ICLR 2024<a hidden class="anchor" aria-hidden="true" href="#pathformer----iclr-2024">#</a></h3>
<p><em>PATHFORMER: MULTI-SCALE TRANSFORMERS WITH ADAPTIVE PATHWAYS FOR TIME SERIES FORECASTING</em></p>
<p><strong>architecture</strong></p>
<p><img alt="image-20240519170648016" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240519170648016.png"></p>
<p><em><strong>Dual-Attention</strong></em></p>
<p><img alt="image-20240519170423224" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240519170423224.png"></p>
<hr>
<h3 id="mobile-former-----cvpr-2022">Mobile-Former  -  CVPR 2022<a hidden class="anchor" aria-hidden="true" href="#mobile-former-----cvpr-2022">#</a></h3>
<p><em><strong>Mobile-Former: Bridging MobileNet and Transformer</strong></em></p>
<p>动机：</p>
<ul>
<li>
<p>如何设计高效的网络来有效地编码本地处理和全局交互？</p>
</li>
<li>
<p>最近工作：串联组合卷积和视觉变换器的好处，无论是在开始时使用卷积还是将卷积交织到每个变换器块中</p>
</li>
<li>
<p>视觉变换器（ViT）[10,34]展示了全局处理的优势，并实现了比 CNN 显着的性能提升，如何在计算资源或者参数量有限的情况下充分挖掘结合两者的优势，=&gt; parameters efficient</p>
</li>
</ul>
<p>贡献：</p>
<ul>
<li>并行设计 + 双路桥接； 利用了 MobileNet 在本地处理和 Transformer 在全局交互方面的优势；实现局部和全局特征的双向融合
<ul>
<li>全局Token只有初始化为0的很少的Token，利用Cross Attention 进行交互</li>
<li>⭐大概就是在MobileNet为主干的基础上添加ViT全局Token的信息注入</li>
</ul>
</li>
</ul>
<p><em><strong>architecture</strong></em></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240520212934048.png" alt="image-20240520212934048" style="zoom:50%;" />
<p><em><strong>Interaction</strong></em></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240520213014827.png" alt="image-20240520213014827" style="zoom:50%;" />
<p><em><strong>pseudo code</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">q</span> <span class="o">=</span> <span class="n">FC</span><span class="p">(</span><span class="n">token</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(),</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">()</span>	<span class="c1"># shape =&gt; batch, num_token, dim_token;</span>
</span></span><span class="line"><span class="cl"><span class="n">do</span> <span class="n">Local2Global</span><span class="o">-</span><span class="n">CrossAttn</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">token</span> <span class="o">=&gt;</span> <span class="n">MHSA</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">MobileNetBlock</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">q</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(),</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">FC</span><span class="p">(</span><span class="n">token</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">do</span> <span class="n">Global2Local</span><span class="o">-</span><span class="n">CrossAttn</span><span class="p">()</span>
</span></span></code></pre></div><hr>
<h3 id="vit-adapter----iclr-2023">ViT Adapter  - ICLR 2023<a hidden class="anchor" aria-hidden="true" href="#vit-adapter----iclr-2023">#</a></h3>
<p><em><strong>VISION TRANSFORMER ADAPTER FOR DENSE PREDICTIONS</strong></em></p>
<p>动机</p>
<ul>
<li>在不改变原有ViT的基础上(利用大规模预训练参数)，添加辅助器帮助Transformer学习弱项；【使用现成的预训练 ViT 适应密集的预测任务】</li>
<li>ViT 单尺度和低分辨率表示的弱点 =&gt; 注入一些多尺度特征(CNN)给单尺度的ViT</li>
</ul>
<p>​		&hellip;表明卷积可以帮助 Transformer 更好地捕获局部空间信息，对与补丁嵌入层并行的图像的局部空间上下文进行建模，以免改变 ViT 的原始架构。</p>
<p>贡献</p>
<ul>
<li>ViT Adapter: [Spatial Prior Module,  Spatial Feature Injector,  Multi-Scale Feature Extractor]</li>
</ul>
<p><em><strong>paradigm compare</strong></em></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240520214548775.png" alt="image-20240520214548775" style="zoom: 50%;" />
<p><em><strong>architecture</strong></em></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240520214838670.png" alt="image-20240520214838670" style="zoom: 50%;" />
<ul>
<li>（c）用于根据输入图像对局部空间上下文进行建模的空间先验模块， Adapter: Spatial feature token set;  ViT: origin feature map;</li>
<li>（d）用于将空间先验引入ViT的空间特征注入器</li>
<li>（e）用于从单个图像<strong>重新组织</strong>多尺度特征的多尺度特征提取器 -ViT 的尺度特征</li>
</ul>
<p>采用稀疏注意力来降低计算成本</p>
<p><em><strong>pseudo code</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Spatial prior module</span>
</span></span><span class="line"><span class="cl"><span class="n">X_vit</span> <span class="o">=</span> <span class="n">ResnetBlock</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x1</span> <span class="o">=</span> <span class="n">PointConv</span><span class="p">(</span><span class="n">Conv</span><span class="p">(</span><span class="n">X_vit</span><span class="p">))</span>	    <span class="c1"># down-sample:  HW/8^2 and project channel  to D dimension</span>
</span></span><span class="line"><span class="cl"><span class="n">x2</span> <span class="o">=</span> <span class="n">PointConv</span><span class="p">(</span><span class="n">Conv</span><span class="p">(</span><span class="n">x2</span><span class="p">))</span>	    <span class="c1"># down-sample:  HW/16^2						to D dimension</span>
</span></span><span class="line"><span class="cl"><span class="n">x3</span> <span class="o">=</span> <span class="n">PointConv</span><span class="p">(</span><span class="n">Conv</span><span class="p">(</span><span class="n">x3</span><span class="p">))</span>	    <span class="c1"># down-sample:  HW/32^2						to D dimension</span>
</span></span><span class="line"><span class="cl"><span class="n">X_vit</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">x3</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="n">num_token</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># injector // spatial feature to ViT</span>
</span></span><span class="line"><span class="cl"><span class="n">q</span> <span class="o">=</span> <span class="n">FC</span><span class="p">(</span><span class="n">X_vit</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">FC</span><span class="p">(</span><span class="n">X_spm</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>		<span class="c1"># spm: spatial prior module</span>
</span></span><span class="line"><span class="cl"><span class="n">do</span> <span class="n">Cross</span><span class="o">-</span><span class="n">Attn</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Multi-Scale Feature Extractor</span>
</span></span><span class="line"><span class="cl"><span class="n">q</span> <span class="o">=</span> <span class="n">FC</span><span class="p">(</span><span class="n">X_spm</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">FC</span><span class="p">(</span><span class="n">X_vit</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>		<span class="c1"># </span>
</span></span></code></pre></div><p><em><strong>analyze</strong></em>：</p>
<ul>
<li>⭐ 研究表明，ViT 呈现出学习低频全局信号的特征(整体、模糊和粗糙)，而 CNN 则倾向于提取高频信息（例如局部边缘和纹理）</li>
</ul>
<p><em><strong>visualize</strong></em></p>
<p><img alt="image-20240520220239886" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240520220239886.png"></p>
<p>傅里叶变换特征图的傅里叶频谱和相对对数幅度（超过 100 张图像的平均值） =&gt; 表明 ViT-Adapter 比 ViT 捕获更多的高频信号</p>
<p>我们还<em><strong>可视化</strong></em>了图5（b）（c）中的stride-8<em><strong>特征图</strong></em>，这表明ViT的特征是模糊和粗糙的</p>
<hr>
<h3 id="inception-transformer----neurips-2022">Inception Transformer  - NeurIPS 2022<a hidden class="anchor" aria-hidden="true" href="#inception-transformer----neurips-2022">#</a></h3>
<p><em><strong>Inception Transformer</strong></em></p>
<p>动机：</p>
<ul>
<li>
<p>Transformer 具有很强的建立远程依赖关系的能力，但无法捕获主要传达局部信息的高频；</p>
</li>
<li>
<p>ViT 及其变体非常有能力捕获视觉数据中的低频，主要包括场景或对象的全局形状和结构，但对于学习高频（主要包括局部边缘和纹理）不是很强大(CNNs很擅长，它们通过感受野内的局部卷积覆盖更多的局部信息，从而有效地提取高频表示)；</p>
</li>
<li>
<p>最近的研究[21-25]考虑到CNN和ViT的互补优势，将它们集成起来。 一些方法[21,22,24,25]以<strong>串行</strong>方式堆叠卷积层和注意力层，以将局部信息注入全局上下文中。 不幸的是，这种串行方式仅在一层中对一种类型的依赖关系（全局或局部）进行建模，并且在局部性建模期间丢弃全局信息，反之亦然。❤️ 每个模块都不够全面=&gt;模型要么只有局部感知能力，要么只有全局建模能力  =&gt; 在ECG中，有些疾病不仅仅是局部或全局的病理特征，而且是节律异常伴随着波形形态异常；从这一角度出发，我们希望能够充分的利用Transformer的全局依赖感知能力和CNN的强大的局部感知能力，交互融合有力结合两者优势； 【就像在人类视觉系统中一样，高频分量的细节有助于较低层捕获视觉基本特征，并逐渐收集局部信息以对输入有全局的理解】</p>
</li>
<li>
<p>层级式网络，多尺度分辨率特征图，每部分均能全局+局部感知；并且设计频率斜坡结构  =&gt; 底层更注重高频信息(细节信息，局部模式、纹理边缘)；高层更注重低频信息(整体轮廓，全局)</p>
</li>
</ul>
<p>创新点：</p>
<ul>
<li>Transformer中的Multi-Head Self-Attention =&gt; Inception Mixer ;
<ul>
<li>按channel分两组：1. 低频组；2. 高频组；
<ul>
<li>低频组 池化稀疏注意力，但仅最低两块用；</li>
<li>高频组 [MaxPool, DWConv]；</li>
</ul>
</li>
</ul>
</li>
<li>频率斜坡结构: 高频组&gt;低频组  =&gt;  高频组&lt;低频组
<ul>
<li>底层在捕获高频细节方面发挥更多作用，而顶层在建模低频全局信息方面发挥更多作用</li>
</ul>
</li>
</ul>
<p><em><strong>architecture</strong></em></p>
<p><img alt="image-20240520204419726" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240520204419726.png"></p>
<p><em><strong>Inception Mixer</strong></em></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240520204536925.png" alt="image-20240520204536925" style="zoom: 50%;" />
<p><em><strong>pseudo code</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># x : [batch, channel, width, hight]</span>
</span></span><span class="line"><span class="cl"><span class="n">x_h</span><span class="p">,</span> <span class="n">x_l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x_h1</span><span class="p">,</span> <span class="n">x_h2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">x_h</span><span class="p">,</span> <span class="n">chunks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">y_h1</span> <span class="o">=</span> <span class="n">FC</span><span class="p">(</span><span class="n">MaxPool</span><span class="p">(</span><span class="n">x_h1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">y_h2</span> <span class="o">=</span> <span class="n">DWConv</span><span class="p">(</span><span class="n">FC</span><span class="p">(</span><span class="n">x_h2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">y_l</span> <span class="o">=</span> <span class="n">MSA</span><span class="p">(</span><span class="n">AvePooling</span> <span class="p">(</span><span class="n">X_l</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">ITM</span><span class="p">(</span><span class="n">LN</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>  <span class="c1"># ITM : Inception Mixer</span>
</span></span><span class="line"><span class="cl"><span class="n">H</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">+</span> <span class="n">FFN</span><span class="p">(</span><span class="n">LN</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
</span></span></code></pre></div><p><img alt="image-20240520212116579" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240520212116579.png"></p>
<p>局部|高频  &amp; 全局|低频  - <em>傅里叶谱</em></p>
<hr>
<h3 id="transnext----cvpr-2024">TransNeXt  - CVPR 2024<a hidden class="anchor" aria-hidden="true" href="#transnext----cvpr-2024">#</a></h3>
<p><em><strong>TransNeXt: Robust Foveal Visual Perception for Vision Transformers</strong></em></p>
<p><em><strong>analysis</strong></em></p>
<ul>
<li>目前的稀疏Attention：
<ul>
<li>Local Attention[限制计算量，n×固定窗口计算量]: window-level attention, =&gt; cross-window attn information exchange 需要堆叠很深才能实现全局感知</li>
<li>patially downsamples【降低计算的Token数量】: pool or dwconv =&gt; 信息丢失问题； 【细粒度(丢失)=&gt; 粗粒度】</li>
</ul>
</li>
</ul>
<p><em><strong>motivation</strong></em></p>
<ul>
<li>⭐ 观察：生物视觉对视觉焦点周围的特征具有较高的敏锐度，而对远处的特征具有较低的敏锐度。</li>
<li>结合window-level attn &amp; spatial downsample attn，临近的token执行pixel-level attn，稍远的区域执行pool-level attn, 实现视觉仿生聚焦attn</li>
</ul>
<p><em><strong>contribution</strong></em></p>
<ul>
<li>
<p>focus attn 【局部细粒度，全局粗粒度】</p>
</li>
<li>
<p>focus attn 升级 aggregated attention 【QKV 注意力、LKV 注意力和 QLV 注意力统一】</p>
<ul>
<li>QLV 与传统的 QKV 注意力机制不同，它破坏了键和值之间的一对一对应关系，从而为当前查询学习更多隐含的相对位置信息。</li>
<li>LKV：增强表达能力，通过引入可学习的键和值，模型可以学习到更多有用的特征，增强了对复杂关系的建模能力</li>
</ul>
</li>
<li>
<p>length-scaled cosine attention  【双路注意力concat经过同一个softmax】</p>
</li>
<li>
<p>convolutional GLU替换MLP</p>
</li>
</ul>
<p><em><strong>architecture</strong></em></p>
<p><img alt="image-20240522150100876" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240522150100876.png"></p>
<p><em>left figure: focus attn; right figure: aggregated attn(add QKV-attn、LKV-attn、QLV-attn)</em></p>
<p>共享同一个Softmax（作用可能是这里进行多注意力的制约交互）</p>
<p><img alt="image-20240522150532379" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240522150532379.png"></p>
<p>ConvGLU: 卷积 GLU (ConvGLU) 中的每个标记都拥有一个独特的门控信号，基于其最接近的细粒度特征。 这解决了SE机制中全局平均池化过于粗粒度的缺点。 它还满足了一些没有位置编码设计、需要深度卷积提供位置信息的ViT模型的需求。</p>
<hr>
<h3 id="efficientvit----cvpr-2023">EfficientViT  - CVPR 2023<a hidden class="anchor" aria-hidden="true" href="#efficientvit----cvpr-2023">#</a></h3>
<p><em><strong>EfficientViT Memory Efficient Vision Transformer with Cascaded Group Attention</strong></em></p>
<p><em>权衡性能和代价</em></p>
<p><strong>motivation</strong></p>
<ul>
<li>发现现有 Transformer 模型的速度通常受到内存低效操作的限制，尤其是 MHSA 中的张量整形和逐元素函数;</li>
</ul>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240522153808010.png" alt="image-20240522153808010" style="zoom:33%;" />
<ul>
<li>
<p>虽然Transformer性能很好，但是代价很高，不适合实时应用;  =&gt;优化;</p>
</li>
<li>
<p>发现注意力图在头部之间具有高度相似性，导致计算冗余。// 显式分解每个头的计算可以缓解这个问题，同时提高计算效率</p>
</li>
</ul>
<p><em><strong>contribution</strong></em></p>
<ul>
<li>设计了一种具有三明治布局的新构建块，即在高效 FFN 层之间使用单个内存绑定的 MHSA，从而提高内存效率，同时增强通道通信;</li>
<li>提出了一个级联的组注意力模块，为注意力头提供完整特征的不同分割，这不仅节省了计算成本，而且提高了注意力多样性。</li>
</ul>
<p><em><strong>architecture</strong></em></p>
<p><img alt="image-20240522153524595" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240522153524595.png"></p>
<p>Token Interaction：DWConv，增强局部交互能力，引入局部结构信息的归纳偏差来增强模型能力</p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240522154554278.png" alt="image-20240522154554278" style="zoom: 50%;" />
<p>三明治结构中，局部建模和全局Attn, 即[Token Interaction, FFN][Cascaded Group Attention][Token Interaction, FFN]堆叠不是1:1:1, 而是N:1:N, Why? because Attention 计算量太大了，能少用就少用。</p>
<p><em>Cascaded Group Attention pseudo code</em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">feature_group</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qkv_group</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># split head</span>
</span></span><span class="line"><span class="cl"><span class="n">feature</span> <span class="o">=</span> <span class="n">feature_group</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>	<span class="c1"># first head</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">feature_out</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">qkv</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">qkv_group</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">feature</span> <span class="o">=</span> <span class="n">feature</span> <span class="o">+</span> <span class="n">feature_group</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>    <span class="c1"># Cascaded Group </span>
</span></span><span class="line"><span class="cl">    <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">()</span><span class="o">.</span><span class="n">permute</span><span class="p">()</span> 	<span class="c1"># shape: B, H, N, C/H</span>
</span></span><span class="line"><span class="cl">    <span class="n">Q</span> <span class="o">=</span> <span class="n">DWConv</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">Q</span><span class="p">)</span>  <span class="c1"># enhance Query Token Set</span>
</span></span><span class="line"><span class="cl">	<span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span><span class="nd">@K.transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="err">×</span> <span class="n">scale</span><span class="p">)</span><span class="nd">@V</span>
</span></span><span class="line"><span class="cl">    <span class="n">feature_out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">feature_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">FC</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span></code></pre></div><p>不同于传统Attn，这里先分头再线性映射，头的信息会越来越丰富。 并且实现中(Cascaded Group Attention in Window-level Attention )</p>
<hr>
<h3 id="emo----iccv-2023">EMO  - ICCV 2023<a hidden class="anchor" aria-hidden="true" href="#emo----iccv-2023">#</a></h3>
<p><em>Rethinking Mobile Block for Efficient Attention-based Models</em></p>
<p>目标：轻量级 CNN 设计高效的混合模型，并在权衡精度、参数和 FLOP 的情况下获得比基于 CNN 的模型更好的性能</p>
<p>出发点：我们能否为仅使用基本算子的基于注意力的模型构建一个轻量级的类似 IRB 的基础设施？</p>
<p><em><strong>基本算子结构对比</strong></em></p>
<p><img alt="image-20240528221243289" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240528221243289.png"></p>
<ul>
<li>
<p>Multi-head self attention: 线性映射qkv，MHSA, 投影回来</p>
</li>
<li>
<p>Feed Forward Network: Linear-Linear</p>
</li>
<li>
<p>Inverted Residual Block: Conv1x1-DWConv-Conv1x1</p>
</li>
</ul>
<p>=&gt; 综合考量 提出基本算子Meta Mobile Block</p>
<p><em><strong>Meta Former Block vs Inverted Residual Block</strong></em></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240528221642558.png" alt="image-20240528221642558" style="zoom:80%;" />
<p><em>更加细致的抽象</em></p>
<p>iRMB（Inverted Residual Mobile Block）</p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240528222426166.png" alt="image-20240528222426166" style="zoom:80%;" />
<ul>
<li>
<p>！！！ 由于 MHSA 更适合对更深层的语义特征进行建模，因此我们仅在之前的工作之后的 stage-3/4 中打开它 。</p>
</li>
<li>
<p>Conv:</p>
<ul>
<li>BN+SiLU与DWConv结合；</li>
</ul>
</li>
<li>
<p>W-MHSA(window-level attention 更加高效):</p>
<ul>
<li>LN+GeLU与EW-MHSA结合。</li>
<li>解释EW-MHSA
<ul>
<li>因为iRMB，会先升维，导致MHSA计算量变高， Q,K维度不变，而V的维度变长了，拿attn-score加权求和时应用的是扩展V</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>​</p>
<p>深度设计，灵活的设计</p>
<p><img alt="image-20240528223053135" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240528223053135.png"></p>
<hr>
<h3 id="focal-attention----neurips-2021">Focal Attention  - NeurIPS 2021<a hidden class="anchor" aria-hidden="true" href="#focal-attention----neurips-2021">#</a></h3>
<p><em><strong>Focal Attention for Long-Range Interactions in Vision Transformers</strong></em></p>
<p><em>观察：</em></p>
<p><img alt="image-20240528223844857" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240528223844857.png"></p>
<p>图 1：左：DeiT-Tiny 模型 [55] 第一层中给定查询块（蓝色）处三个头的注意力图的可视化。 右图：焦点注意力机制的说明性描述。 使用三个粒度级别来组成蓝色查询的注意区域。</p>
<p>创新点：</p>
<ul>
<li>Close =&gt; Far</li>
<li>Fine   =&gt;  Coarse</li>
</ul>
<p>图示：</p>
<p><img alt="image-20240528224313848" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240528224313848.png"></p>
<p>伪代码：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="mf">1.</span> <span class="n">使用torch</span><span class="o">.</span><span class="n">roll</span> <span class="n">再按窗口划分</span> <span class="o">=&gt;</span> <span class="n">收集细粒度周边Token</span><span class="p">,</span> <span class="n">再使用mask掩码掉多余不需要的Token</span>
</span></span><span class="line"><span class="cl"><span class="mf">2.</span> <span class="n">先分窗口</span><span class="err">，</span><span class="n">执行窗口内Pool</span><span class="err">，</span><span class="n">生成超粗粒度Token</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Q</span><span class="p">:</span> <span class="n">窗口内Token</span>
</span></span><span class="line"><span class="cl"><span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">:</span> <span class="n">窗口内Token</span> <span class="o">+</span> <span class="n">周边细粒度Token</span> <span class="o">+</span> <span class="n">Pool</span><span class="o">-</span><span class="n">Token</span>
</span></span></code></pre></div><hr>
<h3 id="cloformer----cvpr-2023">CloFormer  - CVPR 2023<a hidden class="anchor" aria-hidden="true" href="#cloformer----cvpr-2023">#</a></h3>
<p><em>Rethinking Local Perception in Lightweight Vision Transformer</em></p>
<p><em><strong>architecture</strong></em></p>
<p><img alt="image-20240605130022083" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240605130022083.png"></p>
<p><em><strong>局部+全局 感知并行</strong></em></p>
<p><em>局部感知，类似于CNN中的卷积注意力用在自注意力分支中</em></p>
<p><em><strong>FFN 内追加局部感知增强模块</strong></em></p>
<hr>
<h3 id="metaformer----2023">MetaFormer  - 2023<a hidden class="anchor" aria-hidden="true" href="#metaformer----2023">#</a></h3>
<p>我们并不试图引入新颖的令牌混合器，而只是将令牌混合器指定为最基本或常用的运算符来探测 MetaFormer 的能力</p>
<p><img alt="image-20240605130403587" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240605130403587.png"></p>
<p><img alt="image-20240605130304047" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240605130304047.png"></p>
<p>⭐<em>探索Meta Block的潜力！</em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">TokenMixer</span><span class="p">(</span><span class="n">Norm</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">ChannelMixer</span><span class="p">(</span><span class="n">Norm</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</span></span></code></pre></div><hr>
<h3 id="maxvit----eccv-2022">MaxViT  - ECCV 2022<a hidden class="anchor" aria-hidden="true" href="#maxvit----eccv-2022">#</a></h3>
<p><em>MaxViT: Multi-Axis Vision Transformer</em></p>
<p>动机：解决Self-Attention平方复杂度问题</p>
<p><em>框架：</em></p>
<p><img alt="image-20240615215037327" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240615215037327.png"></p>
<p>1️⃣ MobileNetV2中的倒残差块 =&gt; 提供增强局部感知 &amp; 隐式编码位置信息</p>
<p>2️⃣ Block-Attention =&gt; window-level attention ❌ 限制感受野 ⭐ 降低计算量</p>
<p>3️⃣ Grid-Attention =&gt; 感受野遍及全局的扩张卷积做法 1. 分窗口 2. 收集每个窗口相同次序的Token成组. 3. 组内计算注意力</p>
<p><em>Attention illustration</em></p>
<p><img alt="image-20240615215710510" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240615215710510.png"></p>
<hr>
<h3 id="segment-anything">Segment Anything<a hidden class="anchor" aria-hidden="true" href="#segment-anything">#</a></h3>
<p><em>即时分割</em></p>
<p><em>模型组件</em></p>
<p><img alt="image-20240615220259186" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240615220259186.png"></p>
<p>模型框架：</p>
<p><img alt="image-20240615220352268" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240615220352268.png"></p>
<p><em>prompt encoder：</em></p>
<ul>
<li>​	Sparse prompts:
<ul>
<li>point: point =&gt; 256 dimensional vectorial embedding. 这个使用index去索引位置嵌入像Swin-T， foreground or backgroud embedding（自学习）. to add together.</li>
<li>box: 左上角位置编码 + 左上角的学习嵌入；左上角位置编码+“右下角”的学习嵌入</li>
<li>text: clip的text encoder.</li>
</ul>
</li>
<li>dense prompts:
<ul>
<li>mask: CNN =&gt; 256 特征向量。有则加mask，没有就加可学习的表示无mask的学习嵌入</li>
</ul>
</li>
</ul>
<p><em>mask decoder：</em></p>
<p><img alt="image-20240615220440257" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240615220440257.png"></p>
<hr>
<h3 id="cit----iccv-2021">CiT -  ICCV 2021<a hidden class="anchor" aria-hidden="true" href="#cit----iccv-2021">#</a></h3>
<p>Incorporating Convolution Designs into Visual Transformers</p>
<p><em>局部增强</em></p>
<p><img alt="image-20240615220916238" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240615220916238.png"></p>
<p>就是Inverted Resiual Block [Conv1×1 =&gt; Depth-wise Conv3×3 =&gt; Conv1×1] 对Token进行局部信息增强</p>
<p>框架：</p>
<p><img alt="image-20240615220738212" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240615220738212.png"></p>
<p>⭐利用了每个Stage中的Class Token，这样可以有层级式信息，而且梯度会通过这个CLS Token直接传递给前面部分</p>
<hr>
<h3 id="bi-interaction-light-vit">Bi-Interaction Light-ViT<a hidden class="anchor" aria-hidden="true" href="#bi-interaction-light-vit">#</a></h3>
<p><em>Lightweight Vision Transformer with Bidirectional Interaction</em></p>
<p><em>图示：</em></p>
<p><img alt="image-20240615222535371" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240615222535371.png"></p>
<p><em>框架：</em></p>
<p><img alt="image-20240615222639804" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240615222639804.png"></p>
<p>⭐想法很超前⭐</p>
<p>局部与全局的相互调制</p>
<hr>
<h3 id="unireplknet">UniRepLKNet<a hidden class="anchor" aria-hidden="true" href="#unireplknet">#</a></h3>
<p><em>UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition</em></p>
<p>⭐超大卷积核 Conv winwinwin</p>
<p>当我们向小内核 ConvNet 添加 3×3 卷积时，我们期望它同时产生三种效果</p>
<ol>
<li>使感受野更大，</li>
<li>增加空间模式的抽象层次（例如，从角度和纹理到物体的形状）</li>
<li>通过使其更深，引入更多可学习的参数和非线性来提高模型的一般表示能力</li>
</ol>
<p>相比之下，我们认为大内核架构中的这三种影响应该是解耦的，因为模型应该利用大内核的强大优势——能够看到广泛而不深入的能力</p>
<p>由于在扩大 ERF(感受野) 方面，增加内核大小比堆叠更多层更有效，因此可以使用少量大内核层构建足够的 ERF，从而可以节省计算预算以用于其他高效结构 在增加空间模式的抽象层次或总体上增加深度方面更有效。</p>
<p>框架：</p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240615223821379.png" alt="image-20240615223821379" style="zoom:50%;" />
<p>重参数化</p>
<p><img alt="image-20240615223858785" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240615223858785.png"></p>
<p>块设计</p>
<p><img alt="image-20240615223925194" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240615223925194.png"></p>
<hr>
<h3 id="edgevits----eccv-2022">EdgeViTs  - ECCV 2022<a hidden class="anchor" aria-hidden="true" href="#edgevits----eccv-2022">#</a></h3>
<p><em>EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers</em></p>
<p><em>架构：</em></p>
<p><img alt="image-20240629144615355" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240629144615355.png"></p>
<p><strong>#</strong> <em>类似MobileViT</em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">LocalAgg</span><span class="p">(</span><span class="n">Norm</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">+</span> <span class="n">X</span>  <span class="c1"># </span>
</span></span><span class="line"><span class="cl"><span class="n">Y</span> <span class="o">=</span> <span class="n">FFN</span><span class="p">(</span><span class="n">Norm</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">+</span> <span class="n">X</span>
</span></span><span class="line"><span class="cl"><span class="n">Z</span> <span class="o">=</span> <span class="n">LocalProp</span><span class="p">(</span><span class="n">GlobalSparseAttn</span><span class="p">(</span><span class="n">Norm</span><span class="p">(</span><span class="n">Y</span><span class="p">)))</span> <span class="o">+</span> <span class="n">Y</span>
</span></span><span class="line"><span class="cl"><span class="n">Xout</span> <span class="o">=</span> <span class="n">FFN</span><span class="p">(</span><span class="n">Norm</span><span class="p">(</span><span class="n">Z</span><span class="p">))</span> <span class="o">+</span> <span class="n">Z</span>
</span></span></code></pre></div><p><em>图示：</em> （选举 =&gt; 精英 =&gt; 分发）</p>
<p><img alt="image-20240629144951121" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240629144951121.png"></p>
<hr>
<h3 id="knnk-nearest-neighbors">KNN(K-Nearest Neighbors)<a hidden class="anchor" aria-hidden="true" href="#knnk-nearest-neighbors">#</a></h3>
<p><em>K-近邻算法</em></p>
<p>核心思想：<strong>相似的样本具有相似的输出</strong>。</p>
<p>=&gt; KNN通过计算输入样本与训练数据集中所有样本的距离，找到距离最近的K个样本，然后根据这些样本的类别来决定输入样本的类别</p>
<p>主要步骤:</p>
<ul>
<li>
<p><strong>选择K值</strong>：选择一个正整数K，代表你要比较的邻居数量。</p>
</li>
<li>
<p><strong>计算距离</strong>：对每个待分类样本，计算它与训练数据集中所有样本的距离。常用的距离度量有欧氏距离、曼哈顿距离和余弦相似度等。</p>
<ul>
<li>$$
\textbf{欧氏距离}: d(x, x_i) = \sqrt{\sum_{j=1}^{m}(x_j-x_{ij})^2}
$$</li>
</ul>
</li>
<li>
<p><strong>选择最近的K个邻居</strong>：根据计算得到的距离，从训练数据集中选择距离待分类样本最近的K个样本</p>
</li>
<li>
<p><strong>投票或加权</strong>：在分类任务中，K个邻居中最多的类别即为待分类样本的预测类别。在回归任务中，可以对K个邻居的数值进行平均或者加权平均。</p>
</li>
<li>
<p><strong>输出结果</strong>：输出投票或加权后的结果作为待分类样本的预测结果。</p>
</li>
</ul>
<hr>
<h3 id="shufflenet----cvpr-2018">ShuffleNet  - CVPR 2018<a hidden class="anchor" aria-hidden="true" href="#shufflenet----cvpr-2018">#</a></h3>
<p>出发点：构建高效模型</p>
<p><em><strong>⭐V1⭐</strong></em></p>
<p>缺点发现：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">=&gt;</span> <span class="n">Conv1x1</span>    <span class="n">_</span> <span class="n">Norm</span><span class="o">+</span><span class="n">ReLU</span>  <span class="c1"># Point-wise</span>
</span></span><span class="line"><span class="cl"><span class="o">=&gt;</span> <span class="n">DWConv3x3</span>  <span class="n">_</span> <span class="n">Norm</span>       <span class="c1"># Depth-wise</span>
</span></span><span class="line"><span class="cl"><span class="o">=&gt;</span> <span class="n">Conv1x1</span>    <span class="n">_</span> <span class="n">Norm</span><span class="o">+</span><span class="n">ReLU</span>  <span class="c1"># Point-wise</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 为了高效 =&gt; 只是将Conv3x3 =&gt; Conv1x1</span>
</span></span><span class="line"><span class="cl"><span class="c1"># =&gt; 但是Conv1x1占据了93.4%的乘法-加法</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># =&gt; 目标，优化PWConv</span>
</span></span></code></pre></div><p><em>操作图示：</em></p>
<p><img alt="image-20240724224557015" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240724224557015.png"></p>
<p><em>卷积分组减少计算量 （⭐优化组间通信⭐）</em></p>
<p><em>ShuffleNet Basic Block</em></p>
<p><img alt="image-20240724225221695" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240724225221695.png"></p>
<ul>
<li>(a) ResNet bottleneck unit  &lt;= DWConv</li>
<li>(b) 优化PWConv（Group Conv），并且Channel Shuffle，执行Group communication  # 无下采样，输入输出shape一致</li>
<li>(c)  恒等映射占据一半的Channel，另一半精修过的Feature Map</li>
</ul>
<p><em><strong>⭐V2⭐</strong></em></p>
<p><img alt="image-20240724225722883" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240724225722883.png"></p>
<p>=&gt; 分析各个类型操作占据的计算成本</p>
<p>除了FLOPS指标，吞吐量和速度更为直接直观，符合真实贴切</p>
<p>(使用FLOP作为计算复杂性的唯一度量是不够的，并且可能导致次优设计)</p>
<p>(具有相同FLOP的操作可能具有不同的运行时间)</p>
<ol>
<li>访存时间 &lt;=</li>
<li>并行度    &lt;=</li>
</ol>
<p><img alt="image-20240724225839060" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240724225839060.png"></p>
<ul>
<li>(a) basic shuffle net block                                            -v1</li>
<li>(b) basic shuffle net block with downsample           -v1</li>
<li>(c) v2</li>
<li>(d) v2 with downsample</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># shape equivalence</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">channel</span><span class="o">-</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># =&gt; (B, 32, H, W), (B, 32, H, W)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># x1 作为恒等映射，残差连接？ 特征复用？</span>
</span></span><span class="line"><span class="cl"><span class="n">out</span> <span class="o">=</span> <span class="n">concat</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">block</span><span class="p">(</span><span class="n">x2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">out</span> <span class="o">=</span> <span class="n">channel</span><span class="o">-</span><span class="n">shuffle</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># with downsample</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span> <span class="c1"># =&gt; (B, 64, H, W), (B, 64, H, W)</span>
</span></span><span class="line"><span class="cl"><span class="n">out</span> <span class="o">=</span> <span class="n">concat</span><span class="p">(</span><span class="n">branch1</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="n">branch2</span><span class="p">(</span><span class="n">x2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">out</span> <span class="o">=</span> <span class="n">channel</span><span class="o">-</span><span class="n">shuffle</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span></code></pre></div><p>特征复用示意图：</p>
<p><img alt="image-20240724230720785" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240724230720785.png">
$$
l1-norm = \sum_{i=1}^n{|v_i|}
$$
<em>相邻层更高效</em></p>
<p><img alt="image-20240724232423791" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240724232423791.png"></p>
<p>准确率参数贡献✔️</p>
<hr>
<h3 id="repvgg---reparams---cvpr-2021">RepVGG - ReParams - CVPR 2021<a hidden class="anchor" aria-hidden="true" href="#repvgg---reparams---cvpr-2021">#</a></h3>
<p><em>结构分析</em></p>
<p><img alt="image-20240731130434746" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240731130434746.png"></p>
<p><em>内存分析</em>：</p>
<p><img alt="image-20240731130540271" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240731130540271.png"></p>
<p>=&gt; 权衡：性能和计算内存成本</p>
<p>Train：多分支结构，性能好</p>
<p>Test： 单分支结构，速度快，内存少</p>
<p>⭐⭐⭐重参化：</p>
<p><img alt="image-20240731130753331" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240731130753331.png"></p>
<p>细节：</p>
<p><img alt="image-20240731130825585" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240731130825585.png"></p>
<p><em>举例第一个卷积后的元素</em></p>
<hr>
<h3 id="agent-attention---eccv-2024">Agent Attention - ECCV 2024<a hidden class="anchor" aria-hidden="true" href="#agent-attention---eccv-2024">#</a></h3>
<p>Attn图示：</p>
<p><img alt="image-20240731131233068" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240731131233068.png"></p>
<p>做法：</p>
<p><img alt="image-20240731131548727" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240731131548727.png"></p>
<p>code：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">qkv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">qkv</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">agent_token</span> <span class="o">=</span> <span class="n">pool</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">agent_attn</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">agent_token</span> <span class="o">*</span> <span class="n">scale</span> <span class="o">@</span> <span class="n">k</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">position_bias</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">agent_v</span> <span class="o">=</span> <span class="n">agent_attn</span> <span class="o">@</span> <span class="n">v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">q_attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">((</span><span class="n">q</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span> <span class="o">@</span> <span class="n">agent_tokens</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">agent_bias</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">q_attn</span> <span class="o">@</span> <span class="n">agent_v</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dwc</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 复杂度</span>
</span></span><span class="line"><span class="cl"><span class="n">o</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">K</span><span class="p">)</span> <span class="o">+</span> <span class="n">o</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="n">K</span><span class="p">)</span>
</span></span></code></pre></div><p>享受 =&gt; 高表达性和低计算复杂度的优势</p>
<hr>
<h3 id="patchtst---iclr-2023">PatchTST - ICLR 2023<a hidden class="anchor" aria-hidden="true" href="#patchtst---iclr-2023">#</a></h3>
<ol>
<li>
<p>Patchify .</p>
<ol>
<li>有监督 =&gt; 可以重叠</li>
<li>自监督 =&gt; 不可以重叠，避免网络可以从重叠区域走捷径学习</li>
</ol>
</li>
<li>
<p>多变量独立：</p>
<ol>
<li>
<p>每个时间序列将有自己的潜在表示，通过共享权重机制交叉学习 ？？？</p>
<p>共享Encoder权重，不同通道使用相同的模型参数。这种方法允许模型在不同的任务之间共享知识</p>
</li>
</ol>
</li>
</ol>
<p><strong>Overview</strong></p>
<p><img alt="image-20240801210057622" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240801210057622.png"></p>
<p><img alt="image-20240801205504650" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240801205504650.png"></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">length</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">num_token</span><span class="p">,</span> <span class="n">len_token</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="o">*</span><span class="n">channel</span><span class="p">,</span> <span class="n">num_sample</span><span class="p">,</span> <span class="n">dim_hidden</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">=&gt;</span> <span class="n">Transformer</span> <span class="n">Encoder</span> <span class="n">but</span> <span class="n">residual</span> <span class="n">attn</span> <span class="c1"># </span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">=&gt;</span> <span class="n">Linear</span> <span class="n">head</span> <span class="o">=&gt;</span>
</span></span></code></pre></div><hr>
<h3 id="crossformer---iclr-2023">CrossFormer - ICLR 2023<a hidden class="anchor" aria-hidden="true" href="#crossformer---iclr-2023">#</a></h3>
<p>TRANSFORMER UTILIZING CROSSDIMENSION DEPENDENCY FOR MULTIVARIATE TIME SERIES FORECASTING</p>
<p>创新点：</p>
<ul>
<li>显示建模时间依赖关系 + 通道依赖关系</li>
<li>两阶段注意力 （时间：MHSA，通道：Router MHSA）</li>
</ul>
<p>嵌入方式 and 依据：</p>
<p><img alt="image-20240802201801644" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240802201801644.png"></p>
<p>自注意力呈现小局部一致性，一坨而不是一个。</p>
<ul>
<li>保持通道独立 ✔️</li>
<li>注意力优化     ✔️</li>
</ul>
<p><em><strong>TwoStageAttention</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Step 1. Time Dependency</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">length</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">num_patch</span><span class="p">,</span> <span class="n">dim_patch</span><span class="p">]</span> <span class="c1"># &lt;= DSW (Dimension-Segment-Wise)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="o">*</span><span class="n">channel</span><span class="p">,</span> <span class="n">num_patch</span><span class="p">,</span> <span class="n">dim_patch</span><span class="p">]</span>  
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">TransformerEncoer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># &lt;= capture time dependency</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Step 2. Channel Dependency</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">num_patch</span><span class="p">,</span> <span class="n">dim_patch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="o">*</span><span class="n">num_patch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">dim_patch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">router</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_router</span><span class="p">,</span> <span class="n">dim_patch</span><span class="p">]</span> <span class="c1"># &lt;= router</span>
</span></span><span class="line"><span class="cl"><span class="n">router</span> <span class="o">=&gt;</span> <span class="n">repeat</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="o">*</span><span class="p">,</span> <span class="n">num_router</span><span class="p">,</span> <span class="n">dim_patch</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">router</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># &lt;= capture channel dependency   </span>
</span></span><span class="line"><span class="cl"><span class="n">z</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">router</span><span class="p">,</span> <span class="n">router</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># save per stage output</span>
</span></span><span class="line"><span class="cl"><span class="o">=&gt;</span> <span class="n">Unet</span> <span class="n">Decoder</span> <span class="o">=&gt;</span> <span class="n">to</span> <span class="n">predict</span>
</span></span></code></pre></div><p>ECG与多变量的异同：</p>
<ul>
<li>相似点
<ul>
<li>不同通道贡献不同 =&gt; DSW-patch, 能够更加细粒度编码局部波形  == 多变量(通道)</li>
<li>patch化的成功！！！</li>
</ul>
</li>
<li>不同点
<ul>
<li>由于是对心脏电活动的同一时间不同角度的观察 =&gt; 病理位置相同 =&gt;是否能够通过<strong>共享</strong>策略 降低计算成本🤔❓</li>
</ul>
</li>
</ul>
<hr>
<h3 id="informer---aaai-2021-best">Informer - AAAI 2021 Best<a hidden class="anchor" aria-hidden="true" href="#informer---aaai-2021-best">#</a></h3>
<p>&ndash; TopK-Q</p>
<p>观察：(Q&amp;K 是等价的)</p>
<p><img alt="image-20240803141041758" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240803141041758.png"></p>
<p><img alt="image-20240803141402493" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240803141402493.png"></p>
<p>注意力呈现长尾分布：</p>
<p>Query分为活跃于惰性Token</p>
<p>衡量指标：</p>
<p><img alt="image-20240803141306326" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240803141306326.png"></p>
<p>注意力优化：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span>
</span></span><span class="line"><span class="cl"><span class="c1"># probe</span>
</span></span><span class="line"><span class="cl"><span class="n">K</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_head</span><span class="p">,</span> <span class="n">len_token</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">K_sample</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_head</span><span class="p">,</span> <span class="n">random_len</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">Q_K_sample</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_head</span><span class="p">,</span> <span class="n">len_token</span><span class="p">,</span> <span class="n">random_len</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">M</span> <span class="o">=</span> <span class="n">Q_K_sample</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">Q_K_sample</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">L_K</span><span class="p">)</span>  <span class="c1"># 衡量指标</span>
</span></span><span class="line"><span class="cl"><span class="n">M_top</span> <span class="o">=</span> <span class="n">M</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">n_top</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Q_reduce</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">M_top</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl"><span class="n">attn_active</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q_reduce</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">scale</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">contex</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">()</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_head</span><span class="p">,</span> <span class="n">len_token</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">]</span>  <span class="c1"># 均匀分布的就直接取V的均值</span>
</span></span><span class="line"><span class="cl"><span class="n">contex</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">M_top</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">attn_active</span><span class="nd">@V</span>
</span></span></code></pre></div><p>结构优化：</p>
<p><img alt="image-20240803142458572" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240803142458572.png"></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Encoder:</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">num_layer</span> <span class="o">...</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># &lt;- maxpool(act(norm(conv())))</span>
</span></span><span class="line"><span class="cl"><span class="k">return</span> <span class="n">enc_out</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Decoder:</span>
</span></span><span class="line"><span class="cl"><span class="n">cross</span> <span class="o">=</span> <span class="n">enc_out</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="c1"># 预测引导</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">num_layer</span> <span class="o">...</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cross</span><span class="p">,</span> <span class="n">x_mask</span><span class="o">=</span><span class="n">x_mask</span><span class="p">,</span> <span class="n">cross_mask</span><span class="o">=</span><span class="n">cross_mask</span><span class="p">)</span>  <span class="c1"># Note：Masked MHSA-ProbeAttn</span>
</span></span></code></pre></div><p>框架逻辑</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">Class</span> <span class="n">Exp_Basic</span><span class="p">(</span><span class="n">Object</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_acquire_device</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_get_data</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">vali</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="n">Class</span> <span class="n">Exp_Model</span><span class="p">(</span><span class="n">Exp_Basic</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_select_optimizer</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_select_criterion</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">_process_on_batch</span><span class="p">():</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># data_loader.py</span>
</span></span><span class="line"><span class="cl"><span class="n">Class</span> <span class="n">Dataset_XXX</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">__read_data__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># main.py</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;[Model] Task&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>
</span></span><span class="line"><span class="cl"><span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">setting</span> <span class="o">=</span> <span class="o">...</span><span class="n">args</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">exp</span> <span class="o">=</span> <span class="n">Exp_Model</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">exp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">setting</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">exp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">setting</span><span class="p">)</span>
</span></span></code></pre></div><img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20240817204741361.png" alt="image-20240817204741361" style="zoom: 50%;" />
<hr>
<h3 id="adaptive-token-dictionary----cvpr2024">Adaptive Token Dictionary  - CVPR2024<a hidden class="anchor" aria-hidden="true" href="#adaptive-token-dictionary----cvpr2024">#</a></h3>
<p>Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary</p>
<p>扩展局部窗口的限制</p>
<p><img alt="image-20240919153745713" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240919153745713.png"></p>
<ol>
<li>window-based self-attention</li>
<li>token dictionary cross-attention =&gt; Attention(Q(XW),K(TW), V(TW))</li>
<li>基于2的Attn，将token map排序分group(类)，进行group内部的Attention</li>
</ol>
<p><img alt="image-20240919154126696" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240919154126696.png"></p>
<p><strong>Architecture</strong></p>
<p><img alt="image-20240919154258809" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240919154258809.png"></p>
<hr>
<h3 id="poly-kernel-inception----cvpr-2024">Poly Kernel Inception  - CVPR 2024<a hidden class="anchor" aria-hidden="true" href="#poly-kernel-inception----cvpr-2024">#</a></h3>
<p>Poly Kernel Inception Network for Remote Sensing Detection</p>
<p>遥感图像中的目标检测面临着多种挑战，包括<strong>目标尺度变化大</strong>、测距环境多样等。现有的方法试图通过大核卷积或扩张卷积来扩展脊柱的空间感受野来解决这些挑战。然而，前者通常会<strong>引入相当大的背景噪声</strong>，而后者则有生成<strong>过度稀疏的特征表示</strong>的风险。本文提出了一种多核初始化网络（PKINet）来解决上述问题。PKINet采用无膨胀的多尺度卷积核来提取<strong>不同尺度</strong>的对象特征并捕获<strong>局部上下文</strong>。此外，一个上下文锚注意（CAA）模块并行引入捕获远程上下文信息。</p>
<ol>
<li><strong>不同尺度-局部上下文</strong></li>
<li><strong>并行引入捕获远程上下文信息</strong></li>
</ol>
<p><img alt="image-20240919154538153" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240919154538153.png"></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 十字架型汇聚 =&gt; 近似标准的DWConvKxK =&gt; 降低参数量</span>
</span></span><span class="line"><span class="cl"><span class="n">agg</span> <span class="o">=</span> <span class="n">Conv1x1</span><span class="p">(</span><span class="n">AvgPool</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">agg</span> <span class="o">=</span> <span class="n">Conv1x1</span><span class="p">(</span><span class="n">DWConvKx1</span><span class="p">(</span><span class="n">DWConv1xK</span><span class="p">(</span><span class="n">agg</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl"><span class="n">attn</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">(</span><span class="n">agg</span><span class="p">)</span> 
</span></span></code></pre></div><hr>
<h3 id="danet----cvpr-2019">DANet  - CVPR 2019<a hidden class="anchor" aria-hidden="true" href="#danet----cvpr-2019">#</a></h3>
<p>Dual Attention Network for Scene Segmentation</p>
<p><img alt="image-20240925140803581" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240925140803581.png"></p>
<p>At the end of the model, we use the dual attention mechanism to explicitly capture position and channel dependencies.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PAM_Module</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Position attention module&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#Ref from SAGAN</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">PAM_Module</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">chanel_in</span> <span class="o">=</span> <span class="n">in_dim</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">query_conv</span> <span class="o">=</span> <span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_dim</span><span class="o">//</span><span class="mi">8</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">key_conv</span> <span class="o">=</span> <span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_dim</span><span class="o">//</span><span class="mi">8</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">value_conv</span> <span class="o">=</span> <span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">            inputs :
</span></span></span><span class="line"><span class="cl"><span class="s2">                x : input feature maps( B X C X H X W)
</span></span></span><span class="line"><span class="cl"><span class="s2">            returns :
</span></span></span><span class="line"><span class="cl"><span class="s2">                out : attention value + input feature
</span></span></span><span class="line"><span class="cl"><span class="s2">                attention: B X (HxW) X (HxW)
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">proj_query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span><span class="o">*</span><span class="n">height</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">proj_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span><span class="o">*</span><span class="n">height</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">energy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">proj_query</span><span class="p">,</span> <span class="n">proj_key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">energy</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">proj_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span><span class="o">*</span><span class="n">height</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">proj_value</span><span class="p">,</span> <span class="n">attention</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">out</span> <span class="o">+</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">out</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">CAM_Module</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34; Channel attention module&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">CAM_Module</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">chanel_in</span> <span class="o">=</span> <span class="n">in_dim</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span>  <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">            inputs :
</span></span></span><span class="line"><span class="cl"><span class="s2">                x : input feature maps( B X C X H X W)
</span></span></span><span class="line"><span class="cl"><span class="s2">            returns :
</span></span></span><span class="line"><span class="cl"><span class="s2">                out : attention value + input feature
</span></span></span><span class="line"><span class="cl"><span class="s2">                attention: B X C X C
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">proj_query</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">proj_key</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">energy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">proj_query</span><span class="p">,</span> <span class="n">proj_key</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">energy_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">energy</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">energy</span><span class="p">)</span><span class="o">-</span><span class="n">energy</span>
</span></span><span class="line"><span class="cl">        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">energy_new</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">proj_value</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">proj_value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">out</span> <span class="o">+</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">out</span>
</span></span></code></pre></div><hr>
<h3 id="mixnet">MixNet<a hidden class="anchor" aria-hidden="true" href="#mixnet">#</a></h3>
<p><em>MixConv: Mixed Depthwise Convolutional Kernels</em></p>
<p><img alt="image-20240925145109345" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240925145109345.png"></p>
<p>convulution =&gt; capture local pattern</p>
<ul>
<li>early stages: edges</li>
<li>later stages: objects⭐</li>
</ul>
<p>这项研究表明了单个内核大小的局限性：我们既需要大内核来捕获高分辨率模式，也需要小内核来捕获低分辨率模式，以获得更好的模型精度和效率</p>
<p>在单一机制下实现多种效果，进行增强</p>
<hr>
<h3 id="multimodal-learning">Multimodal Learning<a hidden class="anchor" aria-hidden="true" href="#multimodal-learning">#</a></h3>
<p>Multimodal Learning With Transformers: A Survey</p>
<p><strong>融合策略</strong></p>
<p><img alt="image-20240925151957579" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20240925151957579.png"></p>
<hr>
<h3 id="cf-vit">CF-ViT<a hidden class="anchor" aria-hidden="true" href="#cf-vit">#</a></h3>
<p>CF-ViT: A General Coarse-to-Fine Method for Vision Transformer</p>
<p>对于分类任务 - 不需要那么精细的patch</p>
<p><img alt="image-20241004170321655" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20241004170321655.png"></p>
<p>两步策略：</p>
<ol>
<li>粗粒度patch=&gt;ViT =&gt; 预测得分  =若得分小于设定的置信度&gt;  将重要区域细分 =ViT&gt;  最终预测</li>
</ol>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20241004170409259.png" alt="image-20241004170409259" style="zoom:80%;" />
<p>特征复用</p>
<p><img alt="image-20241004170546367" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20241004170546367.png"></p>
<p><img alt="image-20241004170631330" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20241004170631330.png"></p>
<ul>
<li>不重要区域 =&gt; 大尺度粗略的Patch (可能有不相关的背景干扰)</li>
<li>重要区域 =&gt; 小尺度精细的patch(更多边缘细节) &lt;= 第一阶段粗略的Patch充当区域嵌入</li>
</ul>
<p>利用ViT中[CLS] Token与其他Token的Attn累计区域的重要性</p>
<p>Global-Attn = αAttn_l + (1-α)Attn_l+1</p>
<p>Training</p>
<p>loss = CE(pf, y) + KL(pc,pf)</p>
<p>训练时每次均进行Patch精细推理。使用精细模型指导粗略Patch推理</p>
<hr>
<h3 id="dual-aggregation-transformer">Dual Aggregation Transformer<a hidden class="anchor" aria-hidden="true" href="#dual-aggregation-transformer">#</a></h3>
<p>Dual Aggregation Transformer for Image Super-Resolution</p>
<p>Motivation: 现有方法利用自我注意沿着不同的维度，空间或通道，并取得了令人印象深刻的性能。这启发我们将Transformer中的两个维度结合起来，以获得更强大的表示能力。</p>
<p><img alt="image-20241004172905131" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20241004172905131.png"></p>
<p><img alt="image-20241004172825801" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20241004172825801.png"></p>
<hr>
<h3 id="dual-vision-transformer">Dual Vision Transformer<a hidden class="anchor" aria-hidden="true" href="#dual-vision-transformer">#</a></h3>
<p>研究全局语义和更精细的像素级特征之间的依赖关系 =&gt; pixel-level token  &amp;  semantic token</p>
<p>分解和集成的全局语义和本地功能</p>
<p><img alt="image-20250114144508261" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20250114144508261.png"></p>
<p><img alt="image-20241030220700886" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20241030220700886.png"></p>
<hr>
<h3 id="conformer-resnet--vit">Conformer: ResNet + ViT<a hidden class="anchor" aria-hidden="true" href="#conformer-resnet--vit">#</a></h3>
<p>并行结构 =&gt; 同时保留局部和全局特征 (保持CNN和ViT架构的优势)</p>
<p><img alt="image-20241030221038768" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20241030221038768.png"></p>
<hr>
<h3 id="twins-local-global">Twins： [Local-Global]<a hidden class="anchor" aria-hidden="true" href="#twins-local-global">#</a></h3>
<p><img alt="image-20241030221335460" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20241030221335460.png"></p>
<p><img alt="image-20241030221352090" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20241030221352090.png"></p>
<p><strong>[LSA-FFN] =&gt; [GSA-FFN]</strong></p>
<p><em>window-self-attention =&gt; global-self-attention</em></p>
<hr>
<h3 id="msg-transformer">MSG-Transformer<a hidden class="anchor" aria-hidden="true" href="#msg-transformer">#</a></h3>
<p><em>架构</em></p>
<p><img alt="image-20241031135433480" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20241031135433480.png"></p>
<p>有趣点 - Shuffle-Net ?</p>
<p><img alt="image-20241031135458522" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20241031135458522.png"></p>
<p><em>局部信使  - 传递信息</em></p>
<p><img alt="image-20241031135530570" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20241031135530570.png"></p>
<hr>
<h3 id="dilateformer">DilateFormer<a hidden class="anchor" aria-hidden="true" href="#dilateformer">#</a></h3>
<p>IEEE TRANSACTIONS ON MULTIMEDIA  &ndash;  sci-1</p>
<p><strong>overview</strong></p>
<p><img alt="image-20250114142903267" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20250114142903267.png"></p>
<p><strong>novel</strong></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20250114142955809.png" alt="image-20250114142955809" style="zoom:50%;" />
<p>不同注意力头部，进行细微的调整</p>
<hr>
<h3 id="scopevit">ScopeViT<a hidden class="anchor" aria-hidden="true" href="#scopevit">#</a></h3>
<p>Pattern Recognition</p>
<p><strong>Architecture</strong></p>
<p><img alt="image-20250114143121321" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20250114143121321.png"></p>
<p><strong>novel</strong></p>
<p>串行交叉：[多尺度, 多份KV]+[dilated Attention]
$$
𝐐 = 𝑋𝐖_𝑄,𝐊_𝑖 = 𝑃_𝑖𝐖^𝐾_𝑖, 𝐕_𝑖 = 𝑃_𝑖𝐖^V_𝑖
$$
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20250114143427612.png" alt="image-20250114143427612" style="zoom:50%;" /></p>
<p>1 Query不变， KV通过多个不同内核大小的DWConv生成多尺度 KV  (粗粒度)</p>
<p>2 Stride Attention (细粒度)</p>
<hr>
<h3 id="fastvit---iccv">FastViT - ICCV<a hidden class="anchor" aria-hidden="true" href="#fastvit---iccv">#</a></h3>
<p><strong>overview</strong></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20250114144650219.png" alt="image-20250114144650219" style="zoom:75%;" />
<p>Stem:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">reparams</span><span class="o">-</span><span class="n">conv</span>
</span></span></code></pre></div><p>前三阶段：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">BN</span><span class="p">(</span><span class="n">DWConv</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>  <span class="c1"># re-params</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="p">(</span><span class="n">DWConv</span><span class="o">-&gt;</span><span class="n">BN</span><span class="o">-&gt;</span><span class="n">Conv1x1</span><span class="o">-&gt;</span><span class="n">GELU</span><span class="o">-&gt;</span><span class="n">Conv1x1</span><span class="p">)</span>  <span class="c1"># ConvFFN</span>
</span></span></code></pre></div><p>最后阶段：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">DWConv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># CPE  convolution position embedding</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">BN</span><span class="p">(</span><span class="n">Attention</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>  <span class="c1"># Attention</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="p">(</span><span class="n">DWConv</span><span class="o">-&gt;</span><span class="n">BN</span><span class="o">-&gt;</span><span class="n">Conv1x1</span><span class="o">-&gt;</span><span class="n">GELU</span><span class="o">-&gt;</span><span class="n">Conv1x1</span><span class="p">)</span>  <span class="c1"># ConvFFN</span>
</span></span></code></pre></div><hr>
<h3 id="integration-of-cnn--attention">Integration of CNN + Attention<a hidden class="anchor" aria-hidden="true" href="#integration-of-cnn--attention">#</a></h3>
<p>Revisiting the Integration of Convolution and Attention for Vision Backbone</p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20250114145624850.png" alt="image-20250114145624850" style="zoom:50%;" />
<p><strong>novel</strong></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20250114145649376.png" alt="image-20250114145649376" style="zoom:67%;" />
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">Conv</span><span class="o">-</span><span class="n">part</span><span class="p">:</span> <span class="p">[</span><span class="n">Conv1x1</span><span class="o">-&gt;</span><span class="n">DWConv5x5</span><span class="o">-&gt;</span><span class="n">Conv1x1</span><span class="p">]</span> <span class="o">=&gt;</span> <span class="n">X_conv</span>  <span class="c1"># ConvFFN ?</span>
</span></span><span class="line"><span class="cl"><span class="n">Attn</span><span class="o">-</span><span class="n">part</span><span class="p">:</span> 
</span></span><span class="line"><span class="cl"><span class="mf">1.</span> <span class="p">[</span><span class="n">聚簇</span><span class="p">]</span> <span class="n">Clustering</span><span class="err">：</span><span class="n">X</span> <span class="o">-&gt;</span> <span class="n">pooling</span> <span class="o">-&gt;</span> <span class="n">cluster</span>
</span></span><span class="line"><span class="cl"><span class="mf">2.</span> <span class="p">[</span><span class="n">提炼</span><span class="p">]</span> <span class="n">cluster</span><span class="nd">@X.T</span> <span class="o">-&gt;</span> <span class="n">score</span><span class="nd">@X</span> <span class="o">-&gt;</span> <span class="n">cluster</span><span class="err">$</span>  <span class="p">{</span><span class="n">这里用点积相似度举例</span><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="mf">3.</span> <span class="p">[</span><span class="n">全局</span><span class="p">]</span> <span class="n">cluster</span><span class="err">$</span> <span class="o">-&gt;</span> <span class="n">MHSA</span> <span class="o">-&gt;</span> <span class="n">cluster</span><span class="err">$</span>
</span></span><span class="line"><span class="cl"><span class="mf">4.</span> <span class="p">[</span><span class="n">分发</span><span class="p">]</span> <span class="n">cluster</span><span class="err">$</span> <span class="o">-&gt;</span> <span class="n">cluster</span><span class="nd">@score.T</span> <span class="o">=&gt;</span> <span class="n">X_attn</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">Y</span> <span class="o">=</span> <span class="n">X_conv</span> <span class="o">+</span> <span class="n">X_attn</span>
</span></span></code></pre></div><hr>
<h3 id="repnext">RepNeXt<a hidden class="anchor" aria-hidden="true" href="#repnext">#</a></h3>
<p><strong>overview</strong></p>
<p><img alt="image-20250114151317648" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20250114151317648.png"></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">Token</span><span class="o">-</span><span class="n">Mixer</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="mf">1.</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="mf">2.</span> <span class="n">DWConv3x3</span> <span class="o">+</span> <span class="p">(</span><span class="n">DWConv1x3</span> <span class="o">+</span> <span class="n">DWConv3x1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="mf">3.</span> <span class="n">DWConv7x7</span> <span class="o">+</span> <span class="n">DWConv3x5</span> <span class="o">+</span> <span class="n">DWConv5x3</span> <span class="o">+</span> <span class="p">(</span><span class="n">DWConv1x5</span> <span class="o">-&gt;</span> <span class="n">DWConv5x1</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">DWConv1x7</span> <span class="o">-&gt;</span> <span class="n">DWConv7x1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="mf">4.</span> <span class="p">(</span><span class="n">DWConv1x11</span> <span class="o">-&gt;</span> <span class="n">DWConv11x1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="c1"># Fusion  =&gt; 重参数化融合</span>
</span></span></code></pre></div><p><strong>InceptionNeXt</strong></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20250114151447542.png" alt="image-20250114151447542" style="zoom:50%;" />
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">Block</span><span class="p">:</span> <span class="p">[</span><span class="n">Token</span><span class="o">-</span><span class="n">Mixer</span> <span class="o">-&gt;</span> <span class="n">Norm</span> <span class="o">-&gt;</span> <span class="n">FFN</span><span class="p">]</span>
</span></span></code></pre></div><p>2025/1/14 <strong>Tidying up</strong></p>
<hr>
<hr>
<h3 id="fish-speech-tech-report">Fish-Speech Tech-report<a hidden class="anchor" aria-hidden="true" href="#fish-speech-tech-report">#</a></h3>
<p>Text-to-Speech End2End Model</p>
<p><strong>两阶段训练策略：</strong></p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20250215131815505.png" alt="image-20250215131815505" style="zoom: 50%;" />
<p><em>Stage 1:</em></p>
<p>​	Audio:Mel Spectrogram =&gt; 【Encoder】 =&gt; Embedding =&gt; Quantize Tokens =&gt; 【⭐Decoder⭐】=&gt; Audio</p>
<p>​	<strong>⭐重构目标⭐</strong></p>
<p><em>Stage 2:</em></p>
<p>​	Text:Quantize Tokens =&gt; 【✨AR Model✨】=&gt; Quantize Tokens</p>
<p>​    ⭐<strong>Text:Audio一致性 + 自回归预测Next</strong>⭐</p>
<p><em>Inference:</em></p>
<p>​	Text:Prompt-Tokens =&gt; 【✨AR Model✨】=&gt; Quantize Tokens  =&gt; 【⭐Decoder⭐】=&gt; Audio</p>
<p><em>Vector Quantize Tech:</em></p>
<p>Example：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">有一组连续的温度数据</span><span class="err">（</span><span class="n">如</span> <span class="mf">20.3</span><span class="err">°</span><span class="n">C</span><span class="p">,</span> <span class="mf">21.7</span><span class="err">°</span><span class="n">C</span><span class="p">,</span> <span class="mf">22.5</span><span class="err">°</span><span class="n">C</span><span class="p">,</span> <span class="mf">19.8</span><span class="err">°</span><span class="n">C</span><span class="err">），</span><span class="n">你想将其离散化为几个类别</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="mf">1.</span><span class="n">低温</span><span class="p">:</span> <span class="mi">15</span><span class="err">°</span><span class="n">C</span> <span class="o">-</span> <span class="mi">20</span><span class="err">°</span><span class="n">C</span>
</span></span><span class="line"><span class="cl"><span class="mf">2.</span><span class="n">中温</span><span class="p">:</span> <span class="mi">20</span><span class="err">°</span><span class="n">C</span> <span class="o">-</span> <span class="mi">25</span><span class="err">°</span><span class="n">C</span>
</span></span><span class="line"><span class="cl"><span class="mf">3.</span><span class="n">高温</span><span class="p">:</span> <span class="mi">25</span><span class="err">°</span><span class="n">C</span> <span class="o">-</span> <span class="mi">30</span><span class="err">°</span><span class="n">C</span>
</span></span><span class="line"><span class="cl"><span class="o">=&gt;</span>
</span></span><span class="line"><span class="cl"><span class="mf">20.3</span><span class="err">°</span><span class="n">C</span> <span class="err">→</span> <span class="n">中温</span>
</span></span><span class="line"><span class="cl"><span class="mf">21.7</span><span class="err">°</span><span class="n">C</span> <span class="err">→</span> <span class="n">中温</span>
</span></span><span class="line"><span class="cl"><span class="mf">22.5</span><span class="err">°</span><span class="n">C</span> <span class="err">→</span> <span class="n">中温</span>
</span></span><span class="line"><span class="cl"><span class="mf">19.8</span><span class="err">°</span><span class="n">C</span> <span class="err">→</span> <span class="n">低温</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">假设编码本有</span> <span class="mi">512</span> <span class="n">个向量</span> <span class="n">Shape</span><span class="p">:[</span><span class="mi">512</span><span class="p">,</span> <span class="n">dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">Encoder得到的Embedding</span> <span class="n">Shape</span><span class="p">:[</span><span class="n">T</span><span class="p">,</span> <span class="n">dim</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">对于</span> <span class="n">Encoder</span> <span class="n">输出的每个时间步的特征向量</span><span class="err">，</span><span class="n">VQ</span> <span class="n">会找到编码本中与之最接近的向量</span><span class="err">，</span><span class="n">并用其索引表示</span><span class="err">。</span>
</span></span><span class="line"><span class="cl"><span class="n">最终输出是一个离散的索引序列</span><span class="err">，</span><span class="n">例如</span> <span class="p">[</span><span class="mi">42</span><span class="p">,</span> <span class="mi">123</span><span class="p">,</span> <span class="mi">87</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span><span class="err">，</span><span class="n">每个索引对应编码本中的一个向量</span><span class="err">。</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">编码本随机初始化</span><span class="err">，</span><span class="n">在训练过程中</span><span class="err">，</span><span class="n">编码本会通过梯度下降和优化算法</span><span class="err">（</span><span class="n">如</span> <span class="n">Adam</span><span class="err">）</span><span class="n">不断更新</span><span class="err">：</span>
</span></span><span class="line"><span class="cl"><span class="n">最近邻搜索</span> <span class="o">=&gt;</span> <span class="n">量化误差计算</span> <span class="o">=&gt;</span> <span class="n">梯度更新</span><span class="p">,</span><span class="n">BP</span>
</span></span><span class="line"><span class="cl"><span class="n">编码本的作用</span> <span class="o">=&gt;</span> <span class="n">降维与压缩</span> <span class="o">+</span> <span class="n">离散化表示</span> <span class="o">+</span> <span class="n">提升生成质量</span><span class="p">(</span><span class="n">通过离散化减少生成过程中的模糊性</span><span class="err">，</span><span class="n">提升生成语音的自然度s</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<h3 id="blip">Blip<a hidden class="anchor" aria-hidden="true" href="#blip">#</a></h3>
<ul>
<li>Image-2-Text 任务之一</li>
</ul>
<p>Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation Architecture</p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20250215133226745.png" alt="image-20250215133226745" style="zoom:50%;" />
<p>相同颜色共享参数</p>
<p>Stage 1：</p>
<p>Image =&gt; 【Image Encoder:ViT】 =&gt; image Embedding</p>
<p>Text =&gt; 【Text Encoder:Bert】 =&gt; text Embedding</p>
<p>目标：图文一致性, 训练Encoder</p>
<p>Stage 2：</p>
<p>Image =&gt; 【Image Encoder:ViT:Freeze🥶】 =&gt; image Embedding</p>
<p>Text =&gt; 【Text Encoder:Bert:Freeze🥶 + Cross-Attention:image Embedding🥵】 =&gt; Linear:2class</p>
<p>目标：图文是否匹配-2分类, 训练Cross-Attention部分</p>
<p>Stage 3:</p>
<p>Image =&gt; 【Image Encoder:ViT:Freeze🥶】 =&gt; image Embedding</p>
<p>Text =&gt; 【Text Decoder:GPT🥵 + Cross-Attention:image Embedding:Freeze🥶】 =&gt; Linear:multi-class</p>
<p>目标：Image Embedding + text 自回归预测Next</p>
<p>Inference</p>
<p>Image =&gt; 【⭐Image Encoder⭐】 =&gt; image Embedding</p>
<p>Prompt-Text =&gt; 【⭐Text Decoder:GPT⭐ + Cross-Attention:image Embedding:⭐】 =&gt; Linear:multi-class  =&gt; Next-Token</p>
<p>实现Image =&gt; Text</p>
<hr>
<h3 id="vit-with-deformable-attn">ViT with Deformable Attn<a hidden class="anchor" aria-hidden="true" href="#vit-with-deformable-attn">#</a></h3>
<p>Vision Transformer with Deformable Attention</p>
<p><img alt="image-20250310222653412" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20250310222653412.png"></p>
<p>全部采样点如下：<img alt="image-20250310222724290" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20250310222724290.png"></p>
<p>高得分key采样点如下<img alt="image-20250310222715496" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20250310222715496.png"></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-py" data-lang="py"><span class="line"><span class="cl"><span class="c1"># 1. 生成 query</span>
</span></span><span class="line"><span class="cl"><span class="n">q</span> <span class="o">=</span> <span class="n">Conv1x1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">offset</span> <span class="o">=</span> <span class="n">ConvKxK</span><span class="p">(</span><span class="n">q</span><span class="p">)</span><span class="o">.</span><span class="n">conv1x1</span><span class="p">()</span><span class="o">=&gt;</span> <span class="o">//</span> <span class="n">将通道映射为2</span><span class="err">，</span><span class="n">并且为了提高采样点的多样性</span><span class="err">，</span><span class="n">将通道分组</span><span class="err">，</span><span class="n">每组获取不一样的信息</span><span class="err">。</span>
</span></span><span class="line"><span class="cl"><span class="n">reference</span> <span class="o">=</span> <span class="n">规整的网格</span>
</span></span><span class="line"><span class="cl"><span class="n">pos</span> <span class="o">=</span> <span class="n">reference</span> <span class="o">+</span> <span class="n">offset</span> <span class="o">//</span> <span class="n">固定点</span> <span class="o">+</span> <span class="n">偏移</span>
</span></span><span class="line"><span class="cl"><span class="n">x_sampled</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">grid_sample</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pos</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">k</span> <span class="o">=</span> <span class="n">Conv1x1</span><span class="p">(</span><span class="n">x_sampled</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">v</span> <span class="o">=</span> <span class="n">Conv1x1</span><span class="p">(</span><span class="n">x_sampled</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">MHSA</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
</span></span></code></pre></div><p>⭐⭐⭐</p>
<p>PVT下采样技术导致严重的信息丢失❗，而Swin-T的shiftwindow注意力导致感受野的增长要慢得多❗，这限制了对大型物体建模的潜力。Deformable DETR已经通过在每个尺度上设置Nk = 4的较低数量的键来减少这种开销，并且作为检测头工作良好，但是由于不可接受的信息丢失，在骨干网络中关注如此少的键是不好❗</p>
<p>这不就是步幅Attention，添加了可变嘛</p>
<hr>
<h3 id="bevformer">BEVFormer<a hidden class="anchor" aria-hidden="true" href="#bevformer">#</a></h3>
<p>BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</p>
<p>网络架构信息流思路：</p>
<p><img alt="image-20250310223908226" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20250310223908226.png"></p>
<p>具体：</p>
<p><img alt="image-20250310223507001" loading="lazy" src="c:\\Users\\韦龙\\AppData\\Roaming\\Typora\\typora-user-images\\image-20250310223507001.png"></p>
<ul>
<li>一组可学习的BEV Queries，二维网格，模拟鸟瞰图；</li>
<li>Spatial Cross Attention，每个视图经过backone提取，拿其中多个层级的输出，拼接为多尺度特征(多个层的特征图，校准通道)。然后每个位置的q，只查询对应几个视图的周边几个k；</li>
<li>Temporal Attention，t时刻的BEV中的q，查询t-1时刻，相应位置周边的几个k。 这里的t-1时刻的BEV特征，需要通过一个角度还是啥校准空间对齐；</li>
</ul>
<hr>
<h3 id="deformable-detr">Deformable DETR<a hidden class="anchor" aria-hidden="true" href="#deformable-detr">#</a></h3>
<p>architecture figure：</p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20250310230354425.png" alt="image-20250310230354425" style="zoom:50%;" />
<p>Attn figure：</p>
<img src="C:\Users\韦龙\AppData\Roaming\Typora\typora-user-images\image-20250310230305133.png" alt="image-20250310230305133" style="zoom: 50%;" />
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 伪代码 - 单尺度的</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DeformableAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_points</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_points</span> <span class="o">=</span> <span class="n">num_points</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 用于预测采样偏移的线性层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">offset_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">num_points</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 用于计算注意力权重的线性层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">attn_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">num_points</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 输出投影层</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	
</span></span><span class="line"><span class="cl">    <span class="c1"># reference_points, 每个采样点初始，共用同一个reference_point，靠offset进行局部位置偏移</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">reference_points</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">        <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 预测采样偏移</span>
</span></span><span class="line"><span class="cl">        <span class="n">offsets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">offset_proj</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 生成采样点</span>
</span></span><span class="line"><span class="cl">        <span class="n">sampling_points</span> <span class="o">=</span> <span class="n">reference_points</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">offsets</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 双线性插值采样特征值</span>
</span></span><span class="line"><span class="cl">        <span class="n">sampled_value</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">grid_sample</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">value</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">sampling_points</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">            <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">            <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 计算注意力权重</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_proj</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_points</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 加权求和</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">sampled_value</span> <span class="o">*</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># 输出投影</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/dl/">DL</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/learning/java_learning/">
    <span class="title">« Prev</span>
    <br>
    <span>Java学习笔记</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">LongCoding&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
