<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>深度学习论文汇总2 | LongCoding&#39;s Blog</title>
<meta name="keywords" content="DL">
<meta name="description" content="CAM  - CVPR 2015
Learning Deep Features for Discriminative Localization
弱监督对象定位  - 仅提供Image level label
期望：每个单元被其感受野内的某种视觉模式激活。因此 fk （表示空间位置 (x, y) 处最后一个卷积层中单元 k 的激活//输出特征图的一个像素）是该视觉模式存在的地图。类激活图只是这些视觉模式在不同空间位置的存在的加权线性和
计算卷积特征图对于特定输出单元的重要性来实现的
⭐⭐⭐网络可以保留其卓越的定位能力，直到最后一层   =&gt; 深层特征的定位能力
❗❗❗尽管接受了图像级标签的训练，CNN 仍具有出色的对象定位能力
缺陷：卷积特征图→全局平均池化→softmax层  // 特定网络结构

做法图示

数学公式
在卷积特征图上执行全局平均池化，并将它们用作全连接层的特征，产生所需的输出分类;
❗❗❗将输出层的权重投影回卷积特征图来识别图像区域的重要性

Grad-CAM  - ICCV 2017
适用CNN模型
但论文提到在CNN&#43;LSTM的也能定位有区别的图像区域


α 捕获特征图 k 对于目标类 c 的重要性  // 与CAM的分类线性层权重作用一致

ReLU的作用，只对对感兴趣的类别有积极影响的特征感兴趣。负像素可能属于图像中的其他类别
上述操作 =&gt; 具有类别区分性并且可以很好地定位相关图像区域   - 最后特征图比较小!
				  但缺乏显示细粒度重要性的能力 （能区分猫狗，但对为什么识别为猫，不够精确）

通过点乘法融合 引导反向传播 和 Grad-CAM =&gt; 可视化">
<meta name="author" content="LongWei">
<link rel="canonical" href="http://121.40.252.207/posts/learning/paper_reading2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="http://121.40.252.207/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://121.40.252.207/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://121.40.252.207/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://121.40.252.207/apple-touch-icon.png">
<link rel="mask-icon" href="http://121.40.252.207/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://121.40.252.207/posts/learning/paper_reading2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="http://121.40.252.207/posts/learning/paper_reading2/">
  <meta property="og:site_name" content="LongCoding&#39;s Blog">
  <meta property="og:title" content="深度学习论文汇总2">
  <meta property="og:description" content="CAM - CVPR 2015 Learning Deep Features for Discriminative Localization
弱监督对象定位 - 仅提供Image level label
期望：每个单元被其感受野内的某种视觉模式激活。因此 fk （表示空间位置 (x, y) 处最后一个卷积层中单元 k 的激活//输出特征图的一个像素）是该视觉模式存在的地图。类激活图只是这些视觉模式在不同空间位置的存在的加权线性和
计算卷积特征图对于特定输出单元的重要性来实现的
⭐⭐⭐网络可以保留其卓越的定位能力，直到最后一层 =&gt; 深层特征的定位能力
❗❗❗尽管接受了图像级标签的训练，CNN 仍具有出色的对象定位能力
缺陷：卷积特征图→全局平均池化→softmax层 // 特定网络结构
做法图示数学公式在卷积特征图上执行全局平均池化，并将它们用作全连接层的特征，产生所需的输出分类;
❗❗❗将输出层的权重投影回卷积特征图来识别图像区域的重要性
Grad-CAM - ICCV 2017 适用CNN模型
但论文提到在CNN&#43;LSTM的也能定位有区别的图像区域
α 捕获特征图 k 对于目标类 c 的重要性 // 与CAM的分类线性层权重作用一致
ReLU的作用，只对对感兴趣的类别有积极影响的特征感兴趣。负像素可能属于图像中的其他类别
上述操作 =&gt; 具有类别区分性并且可以很好地定位相关图像区域 - 最后特征图比较小!
但缺乏显示细粒度重要性的能力 （能区分猫狗，但对为什么识别为猫，不够精确）通过点乘法融合 引导反向传播 和 Grad-CAM =&gt; 可视化">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-19T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-04-19T00:00:00+00:00">
    <meta property="article:tag" content="DL">
      <meta property="og:image" content="http://121.40.252.207/papermod-cover.png">
      <meta property="og:see_also" content="http://121.40.252.207/posts/learning/minibaseline_learning/">
      <meta property="og:see_also" content="http://121.40.252.207/posts/learning/paper_reading1/">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://121.40.252.207/papermod-cover.png">
<meta name="twitter:title" content="深度学习论文汇总2">
<meta name="twitter:description" content="CAM  - CVPR 2015
Learning Deep Features for Discriminative Localization
弱监督对象定位  - 仅提供Image level label
期望：每个单元被其感受野内的某种视觉模式激活。因此 fk （表示空间位置 (x, y) 处最后一个卷积层中单元 k 的激活//输出特征图的一个像素）是该视觉模式存在的地图。类激活图只是这些视觉模式在不同空间位置的存在的加权线性和
计算卷积特征图对于特定输出单元的重要性来实现的
⭐⭐⭐网络可以保留其卓越的定位能力，直到最后一层   =&gt; 深层特征的定位能力
❗❗❗尽管接受了图像级标签的训练，CNN 仍具有出色的对象定位能力
缺陷：卷积特征图→全局平均池化→softmax层  // 特定网络结构

做法图示

数学公式
在卷积特征图上执行全局平均池化，并将它们用作全连接层的特征，产生所需的输出分类;
❗❗❗将输出层的权重投影回卷积特征图来识别图像区域的重要性

Grad-CAM  - ICCV 2017
适用CNN模型
但论文提到在CNN&#43;LSTM的也能定位有区别的图像区域


α 捕获特征图 k 对于目标类 c 的重要性  // 与CAM的分类线性层权重作用一致

ReLU的作用，只对对感兴趣的类别有积极影响的特征感兴趣。负像素可能属于图像中的其他类别
上述操作 =&gt; 具有类别区分性并且可以很好地定位相关图像区域   - 最后特征图比较小!
				  但缺乏显示细粒度重要性的能力 （能区分猫狗，但对为什么识别为猫，不够精确）

通过点乘法融合 引导反向传播 和 Grad-CAM =&gt; 可视化">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://121.40.252.207/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "深度学习论文汇总2",
      "item": "http://121.40.252.207/posts/learning/paper_reading2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "深度学习论文汇总2",
  "name": "深度学习论文汇总2",
  "description": "CAM - CVPR 2015 Learning Deep Features for Discriminative Localization\n弱监督对象定位 - 仅提供Image level label\n期望：每个单元被其感受野内的某种视觉模式激活。因此 fk （表示空间位置 (x, y) 处最后一个卷积层中单元 k 的激活//输出特征图的一个像素）是该视觉模式存在的地图。类激活图只是这些视觉模式在不同空间位置的存在的加权线性和\n计算卷积特征图对于特定输出单元的重要性来实现的\n⭐⭐⭐网络可以保留其卓越的定位能力，直到最后一层 =\u0026gt; 深层特征的定位能力\n❗❗❗尽管接受了图像级标签的训练，CNN 仍具有出色的对象定位能力\n缺陷：卷积特征图→全局平均池化→softmax层 // 特定网络结构\n做法图示\r数学公式\r在卷积特征图上执行全局平均池化，并将它们用作全连接层的特征，产生所需的输出分类;\n❗❗❗将输出层的权重投影回卷积特征图来识别图像区域的重要性\nGrad-CAM - ICCV 2017 适用CNN模型\n但论文提到在CNN+LSTM的也能定位有区别的图像区域\nα 捕获特征图 k 对于目标类 c 的重要性 // 与CAM的分类线性层权重作用一致\nReLU的作用，只对对感兴趣的类别有积极影响的特征感兴趣。负像素可能属于图像中的其他类别\n上述操作 =\u0026gt; 具有类别区分性并且可以很好地定位相关图像区域 - 最后特征图比较小!\n但缺乏显示细粒度重要性的能力 （能区分猫狗，但对为什么识别为猫，不够精确）\r通过点乘法融合 引导反向传播 和 Grad-CAM =\u0026gt; 可视化\n",
  "keywords": [
    "DL"
  ],
  "articleBody": "CAM - CVPR 2015 Learning Deep Features for Discriminative Localization\n弱监督对象定位 - 仅提供Image level label\n期望：每个单元被其感受野内的某种视觉模式激活。因此 fk （表示空间位置 (x, y) 处最后一个卷积层中单元 k 的激活//输出特征图的一个像素）是该视觉模式存在的地图。类激活图只是这些视觉模式在不同空间位置的存在的加权线性和\n计算卷积特征图对于特定输出单元的重要性来实现的\n⭐⭐⭐网络可以保留其卓越的定位能力，直到最后一层 =\u003e 深层特征的定位能力\n❗❗❗尽管接受了图像级标签的训练，CNN 仍具有出色的对象定位能力\n缺陷：卷积特征图→全局平均池化→softmax层 // 特定网络结构\n做法图示\r数学公式\r在卷积特征图上执行全局平均池化，并将它们用作全连接层的特征，产生所需的输出分类;\n❗❗❗将输出层的权重投影回卷积特征图来识别图像区域的重要性\nGrad-CAM - ICCV 2017 适用CNN模型\n但论文提到在CNN+LSTM的也能定位有区别的图像区域\nα 捕获特征图 k 对于目标类 c 的重要性 // 与CAM的分类线性层权重作用一致\nReLU的作用，只对对感兴趣的类别有积极影响的特征感兴趣。负像素可能属于图像中的其他类别\n上述操作 =\u003e 具有类别区分性并且可以很好地定位相关图像区域 - 最后特征图比较小!\n但缺乏显示细粒度重要性的能力 （能区分猫狗，但对为什么识别为猫，不够精确）\r通过点乘法融合 引导反向传播 和 Grad-CAM =\u003e 可视化\nGrad-CAM : 类别区分性\nGuided Backprop： 细节纹理重要程序。 做法：将梯度值小于等于零的部分置为零，保留梯度值大于零的部分 =\u003e 以突出输入图像中对预测结果有积极影响的区域，来实现对神经网络中每个像素对最终预测结果的影响进行可视化和解释\n浅层卷积关注纹理特征，深层网络关注本质的那种特征？\nDETR - ECCV 2020 ⭐End-to-End⭐ Object Detection with Transformers\n传统：设置锚框 + 非极大值抑制(去除多余的框)\n创新：集合预测(预测分类 + 锚框)\n前向流程\r模型框架\rCNN backbone - local information fusion\nTransformer encoder - global information fusion\nobject queries - learnable information vector 作用： =\u003e anchor\nFFN: 1. classification =\u003e output class vector 2. box =\u003e output 4 number [center_x, center_y, width, hight]\nobject queries: 作用就是锚框，并且一次性生成100个 » 图片检测的物体数\n如何将object queries 与 groundtrue一一对应？\n匈牙利算法 寻找最佳匹配\n匹配loss = 分类loss(分类正确率) + 框loss(框的重叠度)\n=\u003e 匈牙利算法 =\u003e 哪些框与GT最佳匹配(预测框与GT一一对应)\n最终回传梯度优化参数LOSS = 分类loss(分类正确率) + [框loss + 与框大小无关的iou loss]\n因为框也是生成的，且Transformer容易出大框(全局建模)\nBert - 2018 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding - Computation and Language\nBidirectional Transformers 意思是\nTransformer Encoder中的自注意力计算是全局的，每个Token能观测到其余的Token序列； 而\nTransformer Decoder中由于进行的是Masked Multi Head Self Attention，所以序列只能观测到自己与之前的语境；\n这对于文本上下文语境建模是有弊端的！\n(生成式无监督学习)\n总体结构\n[CLS] 分类Token，凝聚全局语义信息\n[SEP] 分割符，划分句子范围\n用的某个语料库进行词嵌入\n⭐训练方法\n完型填空：使某个词随机被 [Mask] 字符遮挡；为防止模型对[Mask]字符敏感，遮挡时使用概率遮挡 =\u003e 1. 仍替换为[Mask]字符 2.随机替换为其他字符 3. 保持不变 =\u003e 迫使模型学习上下文语境 [句子内信息建模]\n预测下一句： [句子间信息建模]\n丰富的上下文信息：通过考虑单词的左右上下文，BERT 能够更好地理解词义和句法结构，这对于理解语言的复杂性至关重要。\nGPT generative pretrain transformer\n初略版\n模型图：(Transformer Decoder -仅Masked Attention版)\n⭐ input: word =\u003e index(查找词汇表) =\u003e embedding + position embedding\n[batch, sequence_length, embedding_dim]\t# sequence_length 可由多个句子组成，可以使用标识句子结束，用来填充，使得sequence_length在batch内长度一致\n⭐ train：\nx1 =\u003e x2\nx1, x2 =\u003e x3\nx1, x2, x3 =\u003e x4\n因为每个词都要预测下一个词，故使用masked attention （mask-softmax），防止答案泄漏\nCLIP - 2021 Learning Transferable Visual Models From Natural Language Supervision\n实现zero-shot，上游大数据集预训练好，下游任务迁移学习无需样本微调\n多模态模型的总体目标就是：训练一个模型，一方面能统一特征表达，另一方面又能让不同模态特征间学到相关性\n(判别式无监督学习)\n工作目的 痛点：\n在特点数据集上进行标签训练 =\u003e 输入没见过的类别，那么模型就不能输出正确的结果\n数据出现分布偏移，动物图片与卡通动物图片 =\u003e 识别不出来\n图片 - 文字描述 ， 模型学习配对关系\n一个对象的不同视角表示：图片和文本描述\n对比学习方法 （Train） Text Encoder (resnet/vit) 学习文本描述的 深度特征 - 单模态内特征 // T_i == 一个文本特征 Image Encoder(transformer) 学习图片的 深度特征 - 单模态内特征 // I_i == 一个图像特征 将多模态特征投影到跨模态空间 // 矩阵映射，(特征向量)到同一纬度 计算余弦相似度，很明显，正确配对的位置为对角线 计算Loss： （相似度logit作为预测分数） 按行计算Loss，在每一行范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一张图片，我们都希望找到和它最相似的文字 按列计算Loss，在每一列的范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一段文字，我们都希望找到和它最相似的图片 最后将这两个Loss相加取平均，代表我们在模型优化过程中考虑了“图片-\u003e文字”和“文字-\u003e图片”的双向关系 encoder均从头开始训练\n预测 创建一个标签全集， [f’A photo of a {object}’ for object in dataset_labels] Text Encoder 学习上述 模板文字描述 的 深度特征 Image Encoder 学习 待预测图片 的 深度特征 计算余弦相似度 =\u003e 取最高分作为预测目标 缺点 每次预测需要构建标签全集描述 对抽象任务，性能较差 Two-Stream - 2014 Two-Stream Convolutional Networks for Action Recognition in Videos\n⭐先前工作中通过使用堆叠视频帧作为网络的输入来解决此任务，但结果明显比最好的手工制作的浅层表示差。 =\u003e 这可能表明学习的时空特征不能很好地捕捉运动（简单堆叠 =\u003e 让模型从庞大的数据中学习❌很难） =\u003e 模型很难识别该类特征 =\u003e 预先处理成模型擅长的数据形式\n❗❗❗虽然多帧信息很重要，但以适当的方式将其呈现给 ConvNet 也很重要（饱和）\n信息显式建模 =\u003e 可以简化学习过程\n创新点\n空间流从静止视频帧执行动作识别 时间流以识别密集光流形式的运动动作 =\u003e 贴合人类视觉：识别物体 + 识别运动 =\u003e ⭐[结果角度]表明两个识别流是互补的\n光流：帧帧之间像素变化(描述运动变化的数据形式) | 水平方向和垂直方向 \u003c= 有用的线索 (与魔改模型不一样的思路)\n⭐⭐⭐此类输入明确描述了视频帧之间的运动 =\u003e 这使得识别更容易 =\u003e 因为网络不需要隐式估计运动 （显示建模）\n可以从光流数据 =\u003e 时空局部特征 || 运动学特征 || 运动是使用光流位移场明确表示的\n模型框架（双模态）\n提高模型迁移能力 类似于CLIP的目的\n考虑将两个数据集合并为一个，由于类集之间的交叉，这并不简单\n=\u003e 多任务学习 =\u003e 最后生成多个分类头，对应两个数据集分类。混合多个数据集进行实验\nMOCO - CVPR 2020 Momentum Contrast for Unsupervised Visual Representation Learning\n(判别式无监督学习)\n目标：\n1️⃣ 构建一个足够大的动态词典，包含足够多的负样本，使模型真能够学到判别式的特征; 在海量数据中学到真正的样本分布\r2️⃣ 因为词典是动态变化的，为了使词典中的负样本特征尽可能的保持一致性(模型参数不同，时间维度上，得到的特征向量存在不一致性)，提出动量更新 =\u003e 动量模型的缓慢更新确保了字典中的特征相对稳定，从而提供更一致的负样本，提升对比学习的效果。\r$$ θ_{k} ←mθ_{k} + (1 −m)θ_{q} $$ m ∈ [0, 1) 是动量系数。论文中Query Encoder 和Key Encoder是一样配置架构的编码器。 目的，缓慢的更新Key Encoder，构建一个又大又一致的动态词典。\n框架伪代码\npretext task ： 实例判别任务。目标拉近正样本对在特征空间的距离，并使负样本对尽可能的远离。\n1# f_q, f_k: encoder networks for query and key 2# queue: dictionary as a queue of K keys (CxK) 3# m: momentum 4# t: temperature 5f_k.params = f_q.params # initialize 6for x in loader: # load a minibatch x with N samples 7 x_q = aug(x) # a randomly augmented version 8 x_k = aug(x) # another randomly augmented version 9 10 q = f_q.forward(x_q) # queries: NxC 11 k = f_k.forward(x_k) # keys: NxC 12 k = k.detach() # no gradient to keys 13 14 # positive logits: Nx1 15 l_pos = bmm(q.view(N,1,C), k.view(N,C,1)) 16 17 # negative logits: NxK 18 l_neg = mm(q.view(N,C), queue.view(C,K)) 19 20 # logits: Nx(1+K) 21 logits = cat([l_pos, l_neg], dim=1) 22 23 # contrastive loss, Eqn.(1) 24 labels = zeros(N) # positives are the 0-th 25 loss = CrossEntropyLoss(logits/t, labels) 26 27 # SGD update: query network 28 loss.backward() 29 update(f_q.params) 30 31 # momentum update: key network 32 f_k.params = m*f_k.params+(1-m)*f_q.params 33 34 # update dictionary 35 enqueue(queue, k) # enqueue the current minibatch 36 dequeue(queue) # dequeue the earliest minibatch 两个不同视角的同一张图片为正样本对，不同图片为负样本对。\n抽特征：q, k； 最大化q k相似且与负样本远离\n更新Q_Encoder, 使用Q_Encoder参数更新K_Encoder，动量缓慢更新\n无监督预训练后好，取Q_Encoder作为抽取特征骨干网络，冻结其参数，微调分类头，在进行泛化测试\nMAE - CVPR2022 Masked Autoencoders Are Scalable Vision Learners\n非对称掩码自动编码器；Encoder和Decoder架构可以不同\n(生成式无监督学习)\nCV领域的Bert\n⭐NLP与CV的不同：\n信息密度不同\nNLP：句子信息语义很高，信息密度也高。（人类语言-事先浓缩过的信息）\n视觉：信息很冗余，也没高级的语义（自然界），像素可以被相邻的重建恢复\n恢复难度不一致\n恢复高级语义单词和恢复像素级(低级语义)图片，难度也不一样。 框架：\nEncoder：位置编码所有Patch都加上。但仅输入未被Mask的Patch。并且Mask比例很高(论文mask75%patch)，迫使模型学到高维的特征，而非捷径。\nDecoder：Mask Patch是自学习的向量，并且和Encoded Patch在位置上一致，再次添加位置信息。\n简单实现（shuffle，截取前面的作为Encoder输入，后面Patch被Mask；再shuffle逆操作，将Encoded Patch和learnabel patch位置对齐组合好进行Decoder。再重建像素）\n通过预测每个屏蔽补丁的像素值来重建输入。解码器的最后一层是线性投影，其输出通道的数量等于补丁中像素值的数量。\n损失函数计算像素空间中重建图像和原始图像之间的均方误差。\n问题：\n预训练的输入中具有很大一部分掩码标记，而这在下游任务未损坏的图像中不存在。这种差距可能会降低部署的准确性。\r自监督学习方法通常侧重于预训练的不同借口任务。\n对比学习对两个或多个视图之间的图像相似性和相异性进行建模。\nwav2vec 2.0 - NeurIPS 2020 wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\n时序信号版本的Bert\n[自监督]学习通用特征 =\u003e 再微调任务头\n(判别式无监督学习)\n总体框架：\n1️⃣ 原始信号[X] =CNN=\u003e 浅在特征表示 [Z]\n2️⃣ 浅在特征表示 [Z] =Transformer Encoder=\u003e 全局上下文特征表示 [C]\n3️⃣ 浅在特征表示 [Z] =量化器=\u003e 对比目标[Q]\n把原来连续的特征空间假设是d维，拆分成G个子空间（codebook），每个子空间维度是d/G。 然后分别在每个子空间里面聚类（K-mean什么的），一共获得V个中心和其中心特征。 每个类别的特征用其中心特征代替。 量化qt和对应ct\n❌ Quantization module 部分不理解\nMask\n随机起点，遮挡后面t个时间步\n对比损失\n![image-20240411210912407](http://sthda9dn6.hd-bkt.clouddn.com/FqYwQ0MDlAXtdrK4q6VHMuWRn3km)\r包括 qt 和 K 个干扰项\n❗❗❗理解不太清晰\nWhisper - 2022 OpenAI Robust Speech Recognition via Large-Scale Weak Supervision\n大力出奇迹\n模型结构\n多任务训练\n[英文口语=\u003e 英文] 语音识别\n[多语言口语 =\u003e 英文] 语音识别 +翻译\n[多语言口语 =\u003e 对应语言文字] 语音识别\n识别背景音(无内容声音)\n⭐⭐⭐信号 =\u003e Log-Mel Spectrogram(频谱图)\n音素\n⭐⭐⭐数据集\n680k小时，超大数据集。 在此基础上预训练，并且0样本迁移(无需特定任务微调)\nCPC - Machine Learning 2018 Representation Learning with Contrastive Predictive Coding\nContrastive Predictive Coding - 无监督学习\n通过预测未来，学习特征表示 (学习对(高维)信号不同部分之间的底层共享信息进行编码的表示 - 局部平滑度)\n不预测原始信号，而是对高维嵌入依赖建模\n(判别式无监督学习)\nCPC框架\n$$ g_{enc}: local\\ feature\\ learning\\ g_{ar}: global\\ context\\ learning $$\n对比学习\n1# 序列： [x1, x2, x3, x4, x5, x6, x7, x8] 2 3# positive sample 4# x1, x2, x3, x4 =\u003e c \u003c=\u003e x5, x6, x7, x8 5 6# negative sample 7# x1, x2, x3, x4 =\u003e c \u003c=\u003e xa, xb, xc, xd (同batch中其他的) 迫使模型学习序列间的high level feature，学习这种内部的顺序逻辑关系\n互信息公式 - 衡量两个随机变量之间的相互依赖程度 $$ I(x;c)=\\sum_{x,c}p(x,c)\\log\\frac{p(x|c)}{p(x)}. \\ I(x;c): x 与 c 的互信息\\ p(x,c): x 和 c 同时发生的联合概率分布\\ p(x|c): 给定 c 的条件下，x 发生的条件概率分布\\ p(x): x 的边缘概率分布 $$ InfoNCE Loss\n最小化CPC定义的损失 =\u003e 实际上最大化 context c_t 和待预测正样本 X_t 之间的互信息\n优化目标：最大化似然概率\n正相关\n近似计算互信息\nLoss\nX = {x1, x2, …, xN} N个负样本，从batch中其他数据中采样\nE/X 表示似然概率\n最大化互信息 =\u003e 最小化Loss (互信息下界)\n完整InfoNCE-Loss $$ \\text{InfoNCELoss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\text{exp}(s(x_i, x_i^+))}{\\text{exp}(s(x_i, x_i^+)) + \\sum_{j=1}^{K} \\text{exp}(s(x_i, x_j^-))} $$\n不太了解这个最大化互信息的Loss\nDALL·E 2 - 2022.3 OpenAI Hierarchical Text-Conditional Image Generation with CLIP Latents\n层级式图生文\n模型架构\n描述：\n虚线上是CLIP架构(文本和图像的联合表示空间)，学习图文对的关联信息 虚线下是生成框架，prior模型根据Text Embedding生成出CLIP对应的Image Embedding， decoder(diffussion model)根据Image Embedding进行重建 ⭐分步训练\nprior\n生成CLIP image Embedding (diffusion model)\ndecoder\n是根据image Embedding生成图片 – 并且这个decoder是多个堆叠，先生成低分辨率，再高清化 - (diffusion model)\n简洁表示：\ny - 文字\nx - 图片\nzi - 图片嵌入 （显式生成图像表示可以提高图像多样性，同时将照片真实性和标题相似度的损失降至最低）\ndiffusion model - NeurIPS 2020 Denoising Diffusion Probabilistic Models\n框架：\n加噪 + 去噪(还原)\n正向过程：\n1️⃣ Xt 是 前一张图片加噪生成的，Z1是服从正太分布的噪声\nβt是超参数=范围为[0.0001,0.02]递增，则αt 是随时间减少， 表示公式一中原图信息越来越少，噪声越来越重\n2️⃣ 递推带入一下\n最后可得···\nZt_hat 是一个服从正太分布的随机噪声，at_hat = at*at-1*···*a1, 可由X0直接产生任意时间步的加噪图片\n反向去噪过程：\n核心基础\n1️⃣ 用Xt生成Xt-1 ，按贝叶斯公式转换\n2️⃣ q(Xt|Xt-1) == q(Xt|Xt-1, X0)， 而q(Xt-1) == q(Xt-1|X0) 任意步加噪图可由原图直接产生\n3️⃣ 反解公式7， X0可由Xt进行估计\n4️⃣ 带入并整理\n··· 因此可根据Xt =\u003e Xt-1， 下式为最终的去噪公式 $$ x = \\frac{1}{\\sqrt{\\alpha}} \\left( x - \\frac{1 - \\alpha}{\\sqrt{1 - \\alpha_{\\text{hat}}}} \\cdot \\text{predicted_noise} \\right) + \\sqrt{\\beta} \\cdot \\text{noise} $$ β noise 保证多样性\nƐθ表示预测模型\n代码逻辑\n训练：\n1for i, images in enumerate(pbar): 2 images = images.to(device) 3 t = diffusion.sample_timesteps(images.shape[0]).to(device)\t# 采样几个时间步进行训练 4 x_t, noise = diffusion.noise_images(images, t)\t# @ 公式-7 5 predicted_noise = model(x_t, t)\t# Unet 6 loss = mse(noise, predicted_noise) 7 8 optimizer.zero_grad() 9 loss.backward() 10 optimizer.step() 11 12sampled_images = diffusion.sample(model, n=images.shape[0]) # @ 去噪公式 预测模型\nUnet 带时间点嵌入(用余弦-位置嵌入实现的)\n1def forward(self, x, t): 2 t = t.unsqueeze(-1).type(torch.float) 3 t = self.pos_encoding(t, self.time_dim)\t# 嵌入时间步顺序 4 5 x1 = self.inc(x)\t# cnn 6 x2 = self.down1(x1, t) # pooling 7 x2 = self.sa1(x2)\t# self attention 8 x3 = self.down2(x2, t) 9 x3 = self.sa2(x3) 10 x4 = self.down3(x3, t) 11 x4 = self.sa3(x4) 12 13 x4 = self.bot1(x4)\t14 x4 = self.bot2(x4) 15 x4 = self.bot3(x4) 16 17 x = self.up1(x4, x3, t)\t# 插值上采样，再拼接skip connect 18 x = self.sa4(x) 19 x = self.up2(x, x2, t) 20 x = self.sa5(x) 21 x = self.up3(x, x1, t) 22 x = self.sa6(x) 23 output = self.outc(x) 24 return output p(Xt-2|Xt-1, t, y) t是时间步嵌入，y是条件嵌入\n最简单的融合方法就是相加\n？？？ 疑惑点 - 反向过程求的t时刻的均值方差用在哪了？\n2025/1/14 更新：\n// X =\u003e X_noise_t 可以一步生成，可以时间步t可以直接生成对应t时间步的噪声图。\n// 使用U-net预测时间步t的噪声 使用MSE-Loss进行训练\n1# 训练循环 2for epoch in range(num_epochs): 3 for x_0 in dataloader: # x_0 是原始数据 4 # 1. 随机选择一个时间步 t 5 t = torch.randint(0, T, (x_0.size(0),)) # 为每个样本随机选择一个时间步 6 7 # 2. 前向过程：添加噪声 8 epsilon = torch.randn_like(x_0) # 采样噪声 9 alpha_bar_t_t = alpha_bar_t[t].view(-1, 1, 1, 1) # 选择对应时间步的 alpha_bar_t 10 x_t = torch.sqrt(alpha_bar_t_t) * x_0 + torch.sqrt(1 - alpha_bar_t_t) * epsilon # 添加噪声 11 12 # 3. 反向过程：预测噪声 13 predicted_epsilon = model(x_t, t) # 模型预测噪声 14 15 # 4. 计算损失函数 16 loss = F.mse_loss(predicted_epsilon, epsilon) # 均方误差损失 17 18 # 5. 反向传播和优化 19 optimizer.zero_grad() 20 loss.backward() 21 optimizer.step() // 串行，一步一步去噪\n1# 生成过程 2def generate_samples(model, num_samples, T, alpha_bar_t): 3 x_T = torch.randn(num_samples, ...) # 从标准正态分布中采样初始噪声 4 for t in range(T-1, 0, -1): # 从 T-1 到 1 5 # 预测噪声 6 epsilon = model(x_T, t) 7 8 # 计算去噪后的数据 \u003c= 消除噪声 9 alpha_t = alpha_bar_t[t] / alpha_bar_t[t-1] 10 x_T = (x_T - torch.sqrt(1 - alpha_t) * epsilon) / torch.sqrt(alpha_t) 11 12 return x_T Time-Frequency Consistency - NeurIPS 2022 Self-Supervised Contrastive Pre-Training for Time Series via Time-Frequency Consistency\n期望同一示例的基于时间和基于频率的表示在时频空间中靠近在一起\n(判别式无监督学习)\npretext task：实例判别 (一对正样本，其余负样本)\n框架\n时域增强：基于时间特性从 xi 扩充，包括抖动、缩放、时移和邻域分段；\n频域增强：频谱特征扰动 xFi 的增强，添加或删除频率分量来扰动频谱 (确保扰动时间序列仍然与原始样本在频域和时域仍相似)\nLoss：\n余弦相似度：衡量两个向量之间相似性，范围[-1, 1]\n// 补充 2025/1/14\n样本越相似 =\u003e sim(i, j) 越大 =\u003e exp(sim) 越大 =\u003e exp(sim)/\\sum(exp) 越接近于1 =\u003e log(·)越接近于0\n-log(·) 将图像倒置，样本越不相似 =\u003e exp(sim)/\\sum(exp) 越接近于0 =\u003e loss 越大\nCOMET - NeurIPS 2023 Contrast Everything A Hierarchical Contrastive Framework for Medical Time-Series\n(判别式无监督学习)\n我们的方法旨在弥合标记数据的有限可用性与医疗时间序列分析中稳健且可概括的模型的需求之间的差距 对比表示学习背后的关键思想是通过将相似的数据靠近在一起并将不相似的数据进一步分开来挖掘数据一致性 关键是要利用所有可用的信息；除了样本标签之外，数据集是否还有其他信息？(补充信息)\n患者间、患者内进行测试(定义打乱规则)\n⭐⭐⭐多级信息\n多级对比学习框架\n代码分析总结\n1X_train.size == [Batch, channels, segment] # segment length 330, sampling_rate = 250Hz 2y_train # 心肌梗塞二分类标签， patient_id， segment_id 细分为样本级别(心跳)，故图示Encoder不同级别对比学习可复用\n关键，计算不同级别的对比Loss\nPatient-Level Loss\n1x = [Batch, channels, segment] 2 3out1 = net(x) 4out2 = net(x) # ? 模拟数据增强后的不同视角或版本，以便在对比学习中生成有效的正例和负例 5 6# 根据y_train 病人id，构建mask矩阵 7pid1, pid2 = np.meshgrid(str_pid, str_pid)\t8pid_matrix = pid1 + '-' + pid2\t# 每项为 id_x - id_y\t9# 目标位置 id_x - id_x 10pids_of_interest = np.unique(str_pid + '-' + str_pid) # unique combinations of pids of interest i.e. matching 11bool_matrix_of_interest = np.zeros((len(str_pid), len(str_pid))) 12for pid in pids_of_interest: 13 bool_matrix_of_interest += pid_matrix == pid 14 15# 上三角和下三角目标row，col 16rows1, cols1 = np.where(np.triu(bool_matrix_of_interest, 1)) # upper triangle same patient combs 17rows2, cols2 = np.where(np.tril(bool_matrix_of_interest, -1)) # down triangle same patient combs 18 19out1, out2 =\u003e sim_matrix_exp # 余弦相似度矩阵 20 21# @@ 对比学习：不考虑对角线元素的原因主要是因为对角线元素表示的是特征向量与其自身的相似度 22# @@ 我们关注的是如何使具有相同标签的不同特征向量之间更相近，而使不同标签的特征向量之间更疏远 23 24# Loss 分母， 某样本与其他样本相似度之和， @分上三角和下三角 25triu_sum = torch.sum(sim_matrix_exp, 1) # add column 26tril_sum = torch.sum(sim_matrix_exp, 0) # add row 27 28loss_terms = 0 # 取平均 29if len(rows1) \u003e 0: 30 triu_elements = sim_matrix_exp[rows1, cols1] # row and column for upper triangle same patient combinations 31 loss_triu = -torch.mean(torch.log(triu_elements / triu_sum[rows1])) 32 loss += loss_triu # technically need to add 1 more term for symmetry 33 loss_terms += 1 34... 35 36loss = loss/loss_terms Trial-Level Loss\n同上，只不过patient_id =\u003e segment_id\n❌❌❌Sample-Level Loss \u0026 Observation-Level Loss\n看不懂源码~ 😎😭\nTS2Vec - AAAI 22 TS2Vec: Towards Universal Representation of Time Series\n(判别式无监督学习)\n现存工作局限：\n它们都没有以不同尺度的时间序列为特征来捕获尺度不变的信息，而这对于时间序列任务的成功至关重要。多尺度特征可以提供不同级别的语义并提高学习表示的泛化能力 粗粒度表示 - 整个时间序列，可能没那么细致 之前工作的问题\n正样本对会误判\n当存在水平偏移时，子系列一致性很容易受到攻击 (左图) 当出现异常时，时间一致性可能会引入误报对 (右图) 创新：\nTS2Vec 中的对比目标基于增强上下文视图，即相同子系列在两个增强上下文中的表示应该是一致的 (上下文语境下) 层级式对比Loss，由细粒度=\u003e粗粒度，局部到全局 # 学习各种语义级别的任意子系列的上下文表示，灵活且通用的表示。 顶级语义级别的对比使模型能够学习实例(样本)级表示 框架\n流程：\n实例(一段信号)，分两个重叠的子序列, [batch, channel, dim_feature] 投影，[batch, channel, dim_feature] =\u003e [batch, channel, dim_hidden] 随机Mask掉部分信号 CNN-Encoder ⭐⭐⭐ Hierarchical Contrasting Loss\ninstance \u0026 temporal contrastive loss\n![image-20240425172445164](http://sthda9dn6.hd-bkt.clouddn.com/FjeDH748BsrYQtTOEOmSjYW_SEMr)\r*用 -F.log_softmax(x, dim=-1) 实现*\rz1 = F.max_pool1d(z1.transpose(1, 2), kernel_size=2).transpose(1, 2) z2 = F.max_pool1d(z2.transpose(1, 2), kernel_size=2).transpose(1, 2)\n编码特征浓缩，多尺度的Contrast loss\n图示Loss过程\n子序列是从原始信号中裁剪下来的，并且有重叠部分\nTimesURL - AAAI 2024 TimesURL: Self-Supervised Contrastive Learning for Universal Time Series Representation Learning\n代码在TS2Vec上修改\n=\u003e 在这里，我们必须提到，重要的时间变化信息，例如趋势和季节，在多次最大池化操作后会丢失，因此顶层对比实际上无法为下游任务捕获足够的实例级信息\n=\u003e 掩码重建进行学习实例级信息\n(判别式+生成式 混合 无监督学习)\n观察\n**简单负样本：**大多数时间序列片段可以被视为容易负样本。 这些片段往往表现出与锚点的语义差异，并且仅贡献较小的梯度，因此无法提供有用的区分信息\n硬负样本： 硬负样本就是离正样本很近，并且模型很难区别的\n正样本-硬负样本-负样本：，这些样本如果让其远离正样本可以大大提高模型性能。 其有效性被大量的简单负样本所掩盖。（现存框架没有特点显示的指出）\n由于时间序列中的局部平滑性和马尔可夫特性，大多数负样本很容易不足以捕获时间信息，因为它们从根本上缺乏驱动对比学习所需的学习信号。 作为图 2 中真实示例，对于每个正锚点（红色方块），相应的负样本（灰色标记）包含许多简单的负样本和很少的困难负样本，即许多负片太远，无法造成对比损失。\n对比学习的一个关键组成部分是选择适当的增强，这些增强可以施加一些先验来构建可行的正样本，以便编码器可以被训练来学习鲁棒和有区别的表示\n频率混合用于通过将通过快速傅里叶变换（FFT）运算计算出的一个训练实例 xi 中的一定比例的频率分量替换为同一批次中的另一个随机训练实例 xk 的相同频率分量来生成新的上下文视图 （保持病理相同）\n随机裁剪。 重叠两个子序列 - 随机裁剪是上下文一致性策略的关键步骤。 它可以保持时间序列的重要时间关系和语义一致性\n创新点：\n提出双Universum概念，就是利用Mix-up增强F(x)，追加硬负样本。 对比学习+自监督掩码重建，联合优化，来捕获段级和实例级信息， 实现通用表示 ⭐ 第一类包括预测、异常检测和插补，它们更多地依赖于在分段级别捕获的细粒度信息，因为这些任务需要推断特定的时间戳或子序列。细粒度(局部)\n⭐ 第二类包括分类和聚类优先考虑实例级信息（即粗粒度信息），旨在推断整个系列的目标。粗粒度(全局)\n实现 : 分段(对比学习，学习片段)，整句(掩码重建，学习整体)\n框架：\nAUG：\n频率增强，**x =\u003e fft() =mask+fusion=\u003e ifft() =\u003e y**，①该篇论文fusion是融合同batch其他信号的某些部分 =\u003e 领域创新 （fusion的信号应该和这个信号相同病理，扩张数据分布）\rDualConv：\n原始信号的两个增强子视图，z1 = Encoder(x1), z1' = Encoder(x1'), =\u003e mix-up option =\u003e **z1_mix = α × z1 + (1-α) × z1[torch.randperm(z1.shape[0])]**，z1'_mix。 [z1, z1', z1_mix, z1'mix] @ [z1, z1', z1_mix, z1'mix].T =\u003e sim =\u003e **-F.log_softmax(sim)**\rloss: 俩视图对应段为正样本(分子)\r负样本在母分，x_mix做负样本增强。\nMixup - 2017 Machine Learning mixup: BEYOND EMPIRICAL RISK MINIMIZATION\n一种简单并且不需要专业领域知识的数据增强\n现存问题讨论：\n过拟合(大模型，直接记忆Train data，走捷径) - overfitting 精心设计样本(对抗性例子，人难以察觉，但模型会给出错误的答案) - generalize 👇\nERM(经验风险最小化原则)问题\n一方面，即使存在强正则化，ERM 也允许大型神经网络记忆（而不是归纳）训练数据 另一方面，使用 ERM 训练的神经网络在对训练分布之外的示例进行评估时会极大地改变其预测。 CV: 图像的邻近区域定义为其水平反射、轻微旋转和轻微缩放的集合\n=\u003e 数据增强始终可以提高泛化能力, 但该过程依赖于数据集，因此需要使用专家知识 （不同领域增强不一定通用）\n数据增强假设邻近的示例共享同一类，并且不会对不同类的示例之间的邻近关系进行建模。(聚类)\n=\u003e 邻近风险最小化 (VRM) 原则\n⭐=\u003e 从训练样例的邻近分布中提取额外的虚拟样例，以扩大训练分布的支持度\n方法：\n框架伪代码：\n图例：\n虚拟数据，让数据边界过渡； 当不在Train数据的分布出现时，降低不确定性。 稍微清晰化边界\nYOLO-v1 - CVPR 2016 You Only Look Once: Unified, Real-Time Object Detection\n⭐将目标检测视作回归问题 =\u003e 预测出来\n前向推理\nModel:\ninput: [3, 448, 448] =\u003e output: [30, 7, 7]\n置信度(confidence): 这个值代表了模型认为预测的边界框内存在对象的概率\n框的中心坐标 + 宽高\n框中的物体是什么类\n非极大值抑制 （最佳：每个类别独立执行非极大值抑制，从而更精确地处理多类别情况）\n置信度排序：首先将所有的预测边界框按照它们的置信度（confidence scores）进行降序排序。 选择最高置信度边界框：从排序后的列表中选择置信度最高的边界框作为参考框（reference box）。 计算IOU：计算选中的参考框与列表中其他所有边界框的交并比（IOU）。交并比是两个边界框的交集面积与它们的并集面积的比值。 抑制：如果参考框与任何其他边界框的IOU超过预先设定的阈值（通常设置为0.5），那么这些边界框会被认为是多余的，并从列表中删除。 重复步骤：从剩余的边界框列表中再次选择置信度最高的边界框，重复上述过程，直到所有的边界框都被处理完毕。 最终结果：经过非极大值抑制后，剩余的边界框被认为是对目标位置的最佳预测，它们将被用于最终的目标检测输出。 训练：\nλcoord = 5, λnoobj = 0.5, 调整各个部分的重要性 $$ 1_{ij}^{obj}: 表示第ij个格子有对象 \\ 1_{ij}^{noobj}: 表示第ij个格子没有对象\\ S^{2}: 图片划分格子 \\ B: 每个格子预测多少个框 $$ bounding box loss : 中心点 + 框宽高\nconfidence: 格子是否有对象\nclasses：格子分类是否正确\nSemi-Supervised Hybrid Loss - Machine Learning 2023 Semi-Supervised End-To-End Contrastive Learning For Time Series Classification\n1️⃣无标签数据对比学习(增强视图一致性)，2️⃣有标签对比学习(相同种类一致性)，3️⃣有标签分类监督学习\n(判别式无监督学习 + 有监督学习)\n框架对比\n⭐ End to End\n框架\nUnlabeled Sample : 使用两个增强视图作为positive pair，与其他sample为negative pair (标准的对比学习)\nLabeled Sample：1️⃣ 同类型的sample为positive pair，不同类型的sample为negative pair. 2️⃣过分类头，计算分类Loss\n❤️ 混合上述三个Loss，联合优化Encoder\nSimCLR - 2020 A Simple Framework for Contrastive Learning of Visual Representations\n贡献：\n数据增强对于对比学习至关重要 (裁剪缩放，翻转，颜色紊乱，旋转， 掩盖， 高斯噪声， 高斯模糊，Sobel 滤波) - 裁剪缩放+颜色紊乱 比较好\n在经过Resnet编码器后，追加MLP能增强模型性能\n样本x，增强视图xi和xj(正样本)，batch size =N，一共2N的增强视图，对于某个样本x，xi和xj为正样本，和batch中剩余的样本的增强为负样本\n(大batchsize, 性能更好， 全局BN)\n样本自成一类，来尽可能地让编码器找到图像中最重要的特征\n框架\n共享参数 shared weight\nLoss\n上面是正样本对，下面是负样本对 -log_softmax() =\u003e 挑选出需要的值\n算法\nViLT - 2021 ViLT Vision-and-Language Transformer Without Convolution or Region Supervision\n极简结构的图片文多模态融合\n速度限制-问题分析\n归纳总结：\n(a) vision embedding 参数量 \u003e Text Embedding \u003e Modality Interaction # 缺点，视觉嵌入太重(比重太大)，并且融合非常简单即点乘算相似度\n(b) vision embedding和Text embedding 占比差不多 \u003e Modality Interaction # 模态融合之前，工作太繁杂，而且前抽取特征不好，限制后面融合，并且不重视后面的模态融合操作。\n(c) 重视visual Embed和后期的modality interaction，# text 和 vision不均等，重要性不平衡\n=\u003e 简单框架，Text词嵌入(bert中的BertEmbeddings加载训练后的权重)，vision用patch projection，都很快\n⭐ 1. 图像和文本前期嵌入应该有相似均匀的表达能力 2. 这两种模态是否在深层网络中相互作用。\n模型框架：\n初始化参数-ViT，而不是bert\n优化目标(主要)：\nImage Text Matching：0.5概率将图片替换为与文本不匹配的图片，预测一致性(二分类问题) Masked Language Modeling：预测被遮掩的词 text cls: 预测图文是否一致，二分类\ntext token set： 全局上下文=\u003e预被掩词\ntext token set 和 visual token set：进行对齐Loss\nBEIT-v3 2022 Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks\n统一Vision 和 NLP\n⭐ 核心思想是图像可以被建模为一门外语，这样我们就可以对图像、文本和图文对进行统一的掩码“语言”建模。\n结果非常好\n基础块\n共享注意力矩阵(都是一个物体的不同视角)，但是最后的FFN各个模态专享\n拓展到不同的模态：\n任务：\n图像字幕任务：采用了特殊的自注意力掩模。 图像标记（即图像块）只能在图像序列内双向相互关注。 标题的标记可以关注图像标记、它们的左侧标题标记以及它们本身。 在微调过程中，我们随机屏蔽一定比例的标题标记。 该模型经过训练，可以根据图像的线索及其左侧标题上下文来恢复这些标记。 我们还屏蔽了特殊的边界标记 [SEP]，以帮助模型学习终止生成\n视觉问答： 将任务表述为分类问题。 该模型经过训练，可以从训练集中 3129 个最常见的候选答案中预测答案。我们将给定问题和图像的嵌入连接起来，然后将输入嵌入输入多路转换器以联合编码图像-问题对。 最终的池化输出被输入到分类器层来预测答案。\n**图像文本检索任务：**是测量图像和文本之间的相似度。 根据检索目标的模态，有两个方向：图像到文本检索和文本到图像检索。 双编码器模型分别对图像和文本进行编码以获得它们的表示。 然后我们计算这些表示的余弦相似度分数。\n图像分类： 将该任务制定为图像到文本检索任务。 我们使用类别名称作为文本来构建图像-文本对。 BEIT-3 被训练为双编码器，以找到图像最相关的标签。 在推理过程中，我们首先计算可能的类名的特征嵌入和图像的特征嵌入。 然后计算它们的余弦相似度分数以预测每个图像最可能的标签。\nALBEF - 2021 Align before Fuse: Vision and Language Representation Learning with Momentum Distillation\n现存问题：大多数现有方法采用基于变压器的多模态编码器来联合建模-视觉Token（基于区域的图像特征）和文本Token。 由于视觉标记和单词标记未对齐，因此多模态编码器学习图像文本交互具有挑战性。\n（1）图像特征和文本符号映射仍然停留在他们自己的空间，使得多模态编码器很难学习建模他们之间的交互；\n（2）物体检测器 — 标注费钱，使用费算力 — 在预训练阶段需要标注矩形框，在推理阶段高分辨率图像，如 600*1000，速度较慢；\n（3）广泛使用的 image-text 数据集均是从网上搜集的带有严重噪声的数据，现有的预训练目标，如 MLM 可能过拟合到文本数据，降低了模型的泛化性能。\n框架：\nimage encoder: ViT(ImageNet预训练参数) - CLS token\ntext encoder: Bert(预训练参数) - CLS token\nmultimodal encoder: Bert(预训练参数) + cross-attention\n⭐ **在传入multi-modal encoder前，使用ITC迫使模型进行对齐 ** align before fuse的align\n对齐图像特征和文本特征，使多模态融合编码器更容易执行跨模态学习 改进了单模态编码器，以更好地理解图像和文本的语义 它学习一个共同的低维空间来嵌入图像和文本，这使得图像文本匹配目标能够通过我们的对比硬负挖掘找到更多信息样本。 Image-Text Contrastive Loss：\n正样本对：配对的Image-Text\n负样本对：Queue存储着的样本表示\nImage =\u003e 匹配Text-Queue(Momentum)\nText =\u003e 匹配Image-Queue(Momentum)\n(代码中利用了Momentum Distillation …)\nImage-Text Matching:\n有了multi-modal encoder输出的 embed token (正确样本的表征) =\u003e 即喂入Multi-modal encoder的 text embed = (positive), Image embed = (positive), 拿融合的CLS作为最终表征，过MLP =\u003e 二分类预测是否匹配；这部分Targets=(1, 1, …, 1)\n从同batch中，按相似性大小随机挑选一个(hard)负样本，\n然后，text embed = (positive, …, negetive, …) , Image embed = (negetive, …, positive)\nmulti-modal encoder =\u003e CLS =\u003e MLP =\u003e two probability\nTargets = (0, 0, …, 0)\n// 重新梳理如下：\n1# 图像i-文本j =\u003e 多模态编码器 =\u003e 是否匹配； 2图像 1-文本 1 : 匹配对 3图像 2-文本 2 : 匹配对 4图像 1-文本 2 : 不匹配 // 这里使用余弦相似度选取最困难的样本 5图像 2-文本 1 : 不匹配 // 这里使用余弦相似度选取最困难的样本 Masked Language Modeling:\n屏蔽掉一些词，通过从图片模态信息中预测掉被屏蔽的词(多分类Loss)\n这里也借助了图像的信息去更好的恢复被mask掉的单词\n【这里只对匹配的对计算 掩码Loss】\n目的：缓解noisy web data的不足，真正的label不一定有momentum的好\n真实label不一定比momentum model给出的predict label好，=\u003e 使用KL散度进行约束 一致性\n最大化互信息视角解释：\n在自监督学习中，a 和 b 是同一图像的两个增强。 在视觉语言表示学习中，我们将 a 和 b 视为捕获其语义的图像文本对的不同变体。 我们的目标是学习对观点变化不变的表征。\n最小化Loss =\u003e 最大化互信息的下限(最大化了图像-文本对的**不同“视图”**之间的互信息（MI）的下限)\nInfoNCE Loss\nImage-Text Contrastive Loss\n最大化Text和Image中的互信息， ITC 将两个单独的模态（即 I 和 T）视为图像-文本对的两个视图\nMLM:\nMLM 将图像-文本对的两个视图视为：(1) 随机选择的单词标记，以及 (2) 图像 + 带有该单词屏蔽的上下文文本。\nInstance discrimination - 2018 Unsupervised Feature Learning via Non-Parametric Instance Discrimination\n首次提出个体判别任务！\n观察：\n在监督学习中。在预测’花豹’时，预测概率除了’花豹’，剩余预测得分比较高的是’美洲虎’、‘猎豹’； 最不相似的是’救生艇’、‘购物车’、‘书柜’;\n最高响应的类都是视觉相关的\n⭐ 并不是语义标签，而是数据本身的明显相似性使某些类比其他类更接近；\n❗❗❗ 个体判别：将类监督发挥到了极致，并学习了区分各个实例的特征表示。\n这些观察结果表明，典型的判别学习方法可以自动发现语义类别之间的明显相似性，而无需明确指导这样做。\n我们能否通过纯粹的判别学习来学习反映实例之间明显相似性的有意义的度量？ 图像本身就是独特的，并且每个图像都可能与同一语义类别中的其他图像显着不同。如果我们学会在没有任何语义类别概念的情况下区分各个实例，我们最终可能会得到一个捕获实例之间明显相似性的表示，就像类明智的监督学习如何仍然保留类之间的明显相似性一样。\n目标:\n在没有监督的情况下学习嵌入函数 v = fθ(x)。 fθ 是一个具有参数 θ 的深度神经网络，将图像 x 映射到特征 v。这种嵌入将在图像空间上产生一个度量，对于实例 x 和 y, dθ(x, y) = |fθ(x) − fθ(y)|。 良好的嵌入应该将视觉上相似的图像映射得彼此更接近。 我们新颖的无监督特征学习方法是实例级区分。 我们将每个图像实例视为其自己的不同类，并训练分类器来区分各个实例类。\r方法：\n用一个memory bank存储4096个样本embed feature(128-dimention) 随着网络更新, 目的是让特征在嵌入空间中远离(每一个样本都是一个类)，学习那种有监督时类和类之间相似聚集的现象。\nBYOL - 2020/6 Bootstrap Your Own Latent A New Approach to Self-Supervised Learning\n1无监督学习： { 2 判别式：从增强视图的表示中，他们学会区分同一图像的另一个增强视图的表示和不同图像的增强视图的表示 =\u003e 这种判别方法通常需要将增强视图的每个表示与许多反例进行比较。 3 生成式：通过预测同一图像的不同视图（例如，不同的随机裁剪）来学习表示 =\u003e 图像的增强视图的表示应该能够预测同一图像的另一个增强视图的表示。 4} 方法：\n在线网络θ + 目标网络γ(提供回归目标，γ = α×γ + (1-α)×θ ，指数移动平均 )\nyθ是目标编码器，其余的训练好后丢掉\n⭐一张图片的两个增强表示的相同的语义 =\u003e 在高维的嵌入表示中，应该可以预测对方(相近)\n1yθ：Encoder 2zθ：Projection head 3qθ：Prediction head Loss: $$ 注意图像增强t(x)和t^{}x会对等的传给online\\ net 和 target\\ net\\\\\rLoss: 1/2 × ( || q_{θ}(z_{θ}) - z^{}{ξ}||^{2} + || q{θ}(z_{θ}) - z^{`}_{ξ}||^{2} ) $$ Train: $$ θ：online\\ net\\ parameters\\ ξ：target\\ net\\ parameters\\ θ \u003c- optimizer(θ),\\ \\ \\ ξ \u003c- αξ + (1-α)θ $$\nDINO - 2021 Emerging Properties in Self-Supervised Vision Transformers\nViT最后的CLS注意力图示⭐⭐⭐\n探讨：质疑自监督学习是否为 Vision Transformer (ViT) 提供了比卷积网络 (convnets) 更突出的新属性？\n框架：(借鉴BYOL)\n教师是在训练过程中动态构建的。知识蒸馏就不再被用作自监督预训练的后处理步骤，而是直接作为自监督目标。\n其中学生和教师具有相同的架构并在训练期间使用蒸馏。\n教师在我们工作中用学生的动量平均值进行更新。\n增强策略：\n1. 多裁剪策略构建图像的不同扭曲视图或裁剪。 更准确地说，根据给定的图像，我们生成一组 V 的不同视图。 该集合包含两个全局视图 xg 1 和 xg 2 以及几个分辨率较小的局部视图。\r1. 所有的裁剪都通过学生传递，而只有全局观点通过老师传递，因此鼓励“局部到全局”的对应。\r伪代码：\n防止模型坍塌：\n1. *对动量教师输出进行居中和锐化，以避免模型崩溃。*\r2. **居中（Centering）**：对动量教师的输出进行居中操作是为了减少批次之间的偏差，增加输出的稳定性。具体做法是从每个输出中减去其均值，确保输出围绕零分布，这有助于避免网络输出在特征空间内偏向某一方向，从而降低了模型坍塌的风险。\r3. **锐化（Sharpening）**：锐化是通过增加输出分布的峰值来实现的，目的是使模型的输出更加区分明显，即使不同类别之间的区别更加清晰。这通常通过提高输出概率分布的熵来实现，比如可以采用温度调整（temperature scaling）等方法来调整概率分布，使得主要的概率值更加突出，而其他的概率值则相对降低。\r图示：\n不同颜色是不同的注意力头\n无监督注意力更能学到本质！\nSimSiam - CVPR 2021 Exploring Simple Siamese Representation Learning\n简单设计的Siamese(孪生)网络。 我们的极简主义方法的竞争力表明\n1️⃣ “没有动量编码器的 BYOL”\n2️⃣ “没有负样本的 SimCLR“ + stop-grad(⭐这个非常关键， 这个对防止模型坍塌很关键)\n方法：\n一幅图像的两个增强视图由同一编码器网络 f（主干网络加投影 MLP）处理。 然后在一侧应用预测 MLP-h，在另一侧应用停止梯度操作。 该模型最大化了双方之间的相似性。 它既不使用负对也不使用动量编码器。\n伪代码：\n消融\nLoss:\n负的余弦相似度 和 交叉熵\n相似度：\n交叉熵：\nBatchNorm的影响：\nBatchSize的影响：\n预测头的影响：\nLoss的对称性：\nsym对称；asym非对称；asym. 2×(每个图像采样两对来粗略地补偿对称性)\nSegment Anything 即时分割\n模型组件\n模型框架：\nprompt encoder：\nSparse prompts: point: point =\u003e 256 dimensional vectorial embedding. 这个使用index去索引位置嵌入像Swin-T， foreground or backgroud embedding（自学习）. to add together. box: 左上角位置编码 + 左上角的学习嵌入；左上角位置编码+“右下角”的学习嵌入 text: clip的text encoder. dense prompts: mask: CNN =\u003e 256 特征向量。有则加mask，没有就加可学习的表示无mask的学习嵌入 mask decoder：\nKNN(K-Nearest Neighbors) K-近邻算法\n核心思想：相似的样本具有相似的输出。\n=\u003e KNN通过计算输入样本与训练数据集中所有样本的距离，找到距离最近的K个样本，然后根据这些样本的类别来决定输入样本的类别\n主要步骤:\n选择K值：选择一个正整数K，代表你要比较的邻居数量。\n计算距离：对每个待分类样本，计算它与训练数据集中所有样本的距离。常用的距离度量有欧氏距离、曼哈顿距离和余弦相似度等。\n$$ \\textbf{欧氏距离}: d(x, x_i) = \\sqrt{\\sum_{j=1}^{m}(x_j-x_{ij})^2} $$ 选择最近的K个邻居：根据计算得到的距离，从训练数据集中选择距离待分类样本最近的K个样本\n投票或加权：在分类任务中，K个邻居中最多的类别即为待分类样本的预测类别。在回归任务中，可以对K个邻居的数值进行平均或者加权平均。\n输出结果：输出投票或加权后的结果作为待分类样本的预测结果。\nPatchTST - ICLR 2023 Patchify .\n有监督 =\u003e 可以重叠 自监督 =\u003e 不可以重叠，避免网络可以从重叠区域走捷径学习 多变量独立：\n每个时间序列将有自己的潜在表示，通过共享权重机制交叉学习 ？？？\n共享Encoder权重，不同通道使用相同的模型参数。这种方法允许模型在不同的任务之间共享知识\nOverview\n1x = [batch, channel, length] 2x = [batch, channel, num_token, len_token] 3x = [batch*channel, num_sample, dim_hidden] 4 5=\u003e Transformer Encoder but residual attn # 6 7=\u003e Linear head =\u003e CrossFormer - ICLR 2023 TRANSFORMER UTILIZING CROSSDIMENSION DEPENDENCY FOR MULTIVARIATE TIME SERIES FORECASTING\n创新点：\n显示建模时间依赖关系 + 通道依赖关系 两阶段注意力 （时间：MHSA，通道：Router MHSA） 嵌入方式 and 依据：\n自注意力呈现小局部一致性，一坨而不是一个。\n保持通道独立 ✔️ 注意力优化 ✔️ TwoStageAttention\n1# Step 1. Time Dependency 2x = [batch, channel, length] 3x = [batch, channel, num_patch, dim_patch] # \u003c= DSW (Dimension-Segment-Wise) 4x = [batch*channel, num_patch, dim_patch] 5y = TransformerEncoer(x, x, x) # \u003c= capture time dependency 6 7# Step 2. Channel Dependency 8y = [batch, channel, num_patch, dim_patch] 9y = [batch*num_patch, channel, dim_patch] 10router = [1, num_router, dim_patch] # \u003c= router 11router =\u003e repeat =\u003e [*, num_router, dim_patch] 12z = TransformerEncoder(router, y, y) # \u003c= capture channel dependency 13z = TransformerEncoder(y, router, router) 14 15# save per stage output 16=\u003e Unet Decoder =\u003e to predict ECG与多变量的异同：\n相似点 不同通道贡献不同 =\u003e DSW-patch, 能够更加细粒度编码局部波形 == 多变量(通道) patch化的成功！！！ 不同点 由于是对心脏电活动的同一时间不同角度的观察 =\u003e 病理位置相同 =\u003e是否能够通过共享策略 降低计算成本🤔❓ Informer - AAAI 2021 Best – TopK-Q\n观察：(Q\u0026K 是等价的)\n注意力呈现长尾分布：\nQuery分为活跃于惰性Token\n衡量指标：\n注意力优化：\n1Q, K, V 2# probe 3K = [batch, num_head, len_token, dim_head] 4K_sample = [batch, num_head, random_len, dim_head] 5Q_K_sample = [batch, num_head, len_token, random_len] 6M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K) # 衡量指标 7M_top = M.topk(n_top, sorted=False)[1] 8 9Q_reduce = Q[:, :, M_top, :] 10attn_active = softmax(torch.matmul(Q_reduce, K.transpose(-2, -1))*scale) 11 12contex = V.sum(-2).expand() =\u003e [batch, num_head, len_token, dim_head] # 均匀分布的就直接取V的均值 13contex[:, :, M_top, :] = attn_active@V 结构优化：\n1# Encoder: 2for num_layer ...: 3 x = attn(x) 4 x = conv_layer(x) # \u003c- maxpool(act(norm(conv()))) 5return enc_out 1# Decoder: 2cross = enc_out 3x = # 预测引导 4for num_layer ...: 5 x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask) # Note：Masked MHSA-ProbeAttn 框架逻辑\n1Class Exp_Basic(Object): 2 def __init__(): 3 def _build_model(): 4 def _acquire_device(): 5 def _get_data(): 6 def train(): 7 def vali(): 8 def test(self): 9 10Class Exp_Model(Exp_Basic): 11 def _select_optimizer(): 12 def _select_criterion(): 13 def _process_on_batch(): 1# data_loader.py 2Class Dataset_XXX(Dataset): 3 def __init__(): 4 def __read_data__(self): 5 def __getitem__(self, index): 6 def __len__(self): 1# main.py 2 3parser = argparse.ArgumentParser(description='[Model] Task') 4... 5args = parser.parse_args() 6setting = ...args 7 8exp = Exp_Model(args) 9exp.train(setting) 10exp.test(setting) Adaptive Token Dictionary - CVPR2024 Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary\n扩展局部窗口的限制\nwindow-based self-attention token dictionary cross-attention =\u003e Attention(Q(XW),K(TW), V(TW)) 基于2的Attn，将token map排序分group(类)，进行group内部的Attention Architecture\nPoly Kernel Inception - CVPR 2024 Poly Kernel Inception Network for Remote Sensing Detection\n遥感图像中的目标检测面临着多种挑战，包括目标尺度变化大、测距环境多样等。现有的方法试图通过大核卷积或扩张卷积来扩展脊柱的空间感受野来解决这些挑战。然而，前者通常会引入相当大的背景噪声，而后者则有生成过度稀疏的特征表示的风险。本文提出了一种多核初始化网络（PKINet）来解决上述问题。PKINet采用无膨胀的多尺度卷积核来提取不同尺度的对象特征并捕获局部上下文。此外，一个上下文锚注意（CAA）模块并行引入捕获远程上下文信息。\n不同尺度-局部上下文 并行引入捕获远程上下文信息 1# 十字架型汇聚 =\u003e 近似标准的DWConvKxK =\u003e 降低参数量 2agg = Conv1x1(AvgPool(X)) 3agg = Conv1x1(DWConvKx1(DWConv1xK(agg))) 4attn = Sigmoid(agg) DANet - CVPR 2019 Dual Attention Network for Scene Segmentation\nAt the end of the model, we use the dual attention mechanism to explicitly capture position and channel dependencies.\n1class PAM_Module(Module): 2 \"\"\" Position attention module\"\"\" 3 #Ref from SAGAN 4 def __init__(self, in_dim): 5 super(PAM_Module, self).__init__() 6 self.chanel_in = in_dim 7 8 self.query_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1) 9 self.key_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1) 10 self.value_conv = Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1) 11 self.gamma = Parameter(torch.zeros(1)) 12 13 self.softmax = Softmax(dim=-1) 14 def forward(self, x): 15 \"\"\" 16 inputs : 17 x : input feature maps( B X C X H X W) 18 returns : 19 out : attention value + input feature 20 attention: B X (HxW) X (HxW) 21 \"\"\" 22 m_batchsize, C, height, width = x.size() 23 proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1) 24 proj_key = self.key_conv(x).view(m_batchsize, -1, width*height) 25 energy = torch.bmm(proj_query, proj_key) 26 attention = self.softmax(energy) 27 proj_value = self.value_conv(x).view(m_batchsize, -1, width*height) 28 29 out = torch.bmm(proj_value, attention.permute(0, 2, 1)) 30 out = out.view(m_batchsize, C, height, width) 31 32 out = self.gamma*out + x 33 return out 34 35 36class CAM_Module(Module): 37 \"\"\" Channel attention module\"\"\" 38 def __init__(self, in_dim): 39 super(CAM_Module, self).__init__() 40 self.chanel_in = in_dim 41 42 43 self.gamma = Parameter(torch.zeros(1)) 44 self.softmax = Softmax(dim=-1) 45 def forward(self,x): 46 \"\"\" 47 inputs : 48 x : input feature maps( B X C X H X W) 49 returns : 50 out : attention value + input feature 51 attention: B X C X C 52 \"\"\" 53 m_batchsize, C, height, width = x.size() 54 proj_query = x.view(m_batchsize, C, -1) 55 proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1) 56 energy = torch.bmm(proj_query, proj_key) 57 energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy 58 attention = self.softmax(energy_new) 59 proj_value = x.view(m_batchsize, C, -1) 60 61 out = torch.bmm(attention, proj_value) 62 out = out.view(m_batchsize, C, height, width) 63 64 out = self.gamma*out + x 65 return out MixNet MixConv: Mixed Depthwise Convolutional Kernels\nconvulution =\u003e capture local pattern\nearly stages: edges later stages: objects⭐ 这项研究表明了单个内核大小的局限性：我们既需要大内核来捕获高分辨率模式，也需要小内核来捕获低分辨率模式，以获得更好的模型精度和效率\n在单一机制下实现多种效果，进行增强\nMultimodal Learning Multimodal Learning With Transformers: A Survey\n融合策略\nDual Aggregation Transformer Dual Aggregation Transformer for Image Super-Resolution\nMotivation: 现有方法利用自我注意沿着不同的维度，空间或通道，并取得了令人印象深刻的性能。这启发我们将Transformer中的两个维度结合起来，以获得更强大的表示能力。\nDual Vision Transformer 研究全局语义和更精细的像素级特征之间的依赖关系 =\u003e pixel-level token \u0026 semantic token\n分解和集成的全局语义和本地功能\nFish-Speech Tech-report Text-to-Speech End2End Model\n两阶段训练策略：\nStage 1:\nAudio:Mel Spectrogram =\u003e 【Encoder】 =\u003e Embedding =\u003e Quantize Tokens =\u003e 【⭐Decoder⭐】=\u003e Audio\r**⭐重构目标⭐**\rStage 2:\nText:Quantize Tokens =\u003e 【✨AR Model✨】=\u003e Quantize Tokens ⭐**Text:Audio一致性 + 自回归预测Next**⭐\rInference:\nText:Prompt-Tokens =\u003e 【✨AR Model✨】=\u003e Quantize Tokens =\u003e 【⭐Decoder⭐】=\u003e Audio\rVector Quantize Tech:\nExample：\n1有一组连续的温度数据（如 20.3°C, 21.7°C, 22.5°C, 19.8°C），你想将其离散化为几个类别: 21.低温: 15°C - 20°C 32.中温: 20°C - 25°C 43.高温: 25°C - 30°C 5=\u003e 620.3°C → 中温 721.7°C → 中温 822.5°C → 中温 919.8°C → 低温 10 11假设编码本有 512 个向量 Shape:[512, dim] 12Encoder得到的Embedding Shape:[T, dim] 13对于 Encoder 输出的每个时间步的特征向量，VQ 会找到编码本中与之最接近的向量，并用其索引表示。 14最终输出是一个离散的索引序列，例如 [42, 123, 87, ...]，每个索引对应编码本中的一个向量。 15 16编码本随机初始化，在训练过程中，编码本会通过梯度下降和优化算法（如 Adam）不断更新： 17最近邻搜索 =\u003e 量化误差计算 =\u003e 梯度更新,BP 18编码本的作用 =\u003e 降维与压缩 + 离散化表示 + 提升生成质量(通过离散化减少生成过程中的模糊性，提升生成语音的自然度s) Blip Image-2-Text 任务之一 Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation Architecture\n相同颜色共享参数\nStage 1：\nImage =\u003e 【Image Encoder:ViT】 =\u003e image Embedding\nText =\u003e 【Text Encoder:Bert】 =\u003e text Embedding\n目标：图文一致性, 训练Encoder\nStage 2：\nImage =\u003e 【Image Encoder:ViT:Freeze🥶】 =\u003e image Embedding\nText =\u003e 【Text Encoder:Bert:Freeze🥶 + Cross-Attention:image Embedding🥵】 =\u003e Linear:2class\n目标：图文是否匹配-2分类, 训练Cross-Attention部分\nStage 3:\nImage =\u003e 【Image Encoder:ViT:Freeze🥶】 =\u003e image Embedding\nText =\u003e 【Text Decoder:GPT🥵 + Cross-Attention:image Embedding:Freeze🥶】 =\u003e Linear:multi-class\n目标：Image Embedding + text 自回归预测Next\nInference\nImage =\u003e 【⭐Image Encoder⭐】 =\u003e image Embedding\nPrompt-Text =\u003e 【⭐Text Decoder:GPT⭐ + Cross-Attention:image Embedding:⭐】 =\u003e Linear:multi-class =\u003e Next-Token\n实现Image =\u003e Text\nBEVFormer BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers\n网络架构信息流思路：\n具体：\n一组可学习的BEV Queries，二维网格，模拟鸟瞰图； Spatial Cross Attention，每个视图经过backone提取，拿其中多个层级的输出，拼接为多尺度特征(多个层的特征图，校准通道)。然后每个位置的q，只查询对应几个视图的周边几个k； Temporal Attention，t时刻的BEV中的q，查询t-1时刻，相应位置周边的几个k。 这里的t-1时刻的BEV特征，需要通过一个角度还是啥校准空间对齐； Deformable DETR architecture figure：\nAttn figure：\n1# 伪代码 - 单尺度的 2import torch 3import torch.nn as nn 4import torch.nn.functional as F 5 6class DeformableAttention(nn.Module): 7 def __init__(self, embed_dim, num_heads, num_points): 8 super().__init__() 9 self.embed_dim = embed_dim 10 self.num_heads = num_heads 11 self.num_points = num_points 12 13 # 用于预测采样偏移的线性层 14 self.offset_proj = nn.Linear(embed_dim, num_heads * num_points * 2) 15 16 # 用于计算注意力权重的线性层 17 self.attn_proj = nn.Linear(embed_dim, num_heads * num_points) 18 19 # 输出投影层 20 self.out_proj = nn.Linear(embed_dim, embed_dim) 21\t22 # reference_points, 每个采样点初始，共用同一个reference_point，靠offset进行局部位置偏移 23 def forward(self, query, reference_points, value): 24 B, N, C = query.shape 25 H, W = value.shape[-2:] 26 27 # 预测采样偏移 28 offsets = self.offset_proj(query).view(B, N, self.num_heads, self.num_points, 2) 29 30 # 生成采样点 31 sampling_points = reference_points.unsqueeze(2) + offsets 32 33 # 双线性插值采样特征值 34 sampled_value = F.grid_sample( 35 value, 36 sampling_points.view(B, -1, H, W, 2), 37 mode='bilinear', 38 align_corners=True 39 ).view(B, N, self.num_heads, self.num_points, C // self.num_heads) 40 41 # 计算注意力权重 42 attn_weights = self.attn_proj(query).view(B, N, self.num_heads, self.num_points) 43 attn_weights = F.softmax(attn_weights, dim=-1) 44 45 # 加权求和 46 output = (sampled_value * attn_weights.unsqueeze(-1)).sum(dim=3) 47 output = output.view(B, N, C) 48 49 # 输出投影 50 output = self.out_proj(output) 51 52 return output ",
  "wordCount" : "3881",
  "inLanguage": "en",
  "image": "http://121.40.252.207/papermod-cover.png","datePublished": "2024-04-19T00:00:00Z",
  "dateModified": "2024-04-19T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "LongWei"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://121.40.252.207/posts/learning/paper_reading2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "LongCoding's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "http://121.40.252.207/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://121.40.252.207/" accesskey="h" title="𝓛𝓸𝓷𝓰𝓒𝓸𝓭𝓲𝓷𝓰&#39;𝓼 𝓑𝓵𝓸𝓰 (Alt + H)">
                <img src="http://121.40.252.207/android-icon-48x48.png" alt="" aria-label="logo"
                    height="30">𝓛𝓸𝓷𝓰𝓒𝓸𝓭𝓲𝓷𝓰&#39;𝓼 𝓑𝓵𝓸𝓰</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://121.40.252.207/index.html" title="🏡 Home">
                    <span>🏡 Home</span>
                </a>
            </li>
            <li>
                <a href="http://121.40.252.207/archives/" title="📃 Archive">
                    <span>📃 Archive</span>
                </a>
            </li>
            <li>
                <a href="http://121.40.252.207/tags/" title="📑 Tags">
                    <span>📑 Tags</span>
                </a>
            </li>
            <li>
                <a href="http://121.40.252.207/categories/" title="🗒️ Categories">
                    <span>🗒️ Categories</span>
                </a>
            </li>
            <li>
                <a href="http://121.40.252.207/search/" title="🔍 Search">
                    <span>🔍 Search</span>
                </a>
            </li>
            <li>
                <a href="http://121.40.252.207/about/" title="👨🏻‍🎓 About Me">
                    <span>👨🏻‍🎓 About Me</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://121.40.252.207/">Home</a>&nbsp;»&nbsp;<a href="http://121.40.252.207/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      深度学习论文汇总2
    </h1>
    <div class="post-meta"><span title='2024-04-19 00:00:00 +0000 UTC'>April 19, 2024</span>&nbsp;·&nbsp;19 min&nbsp;·&nbsp;3881 words&nbsp;·&nbsp;LongWei

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#cam----cvpr-2015" aria-label="CAM - CVPR 2015">CAM - CVPR 2015</a></li>
                <li>
                    <a href="#grad-cam----iccv-2017" aria-label="Grad-CAM  - ICCV 2017">Grad-CAM  - ICCV 2017</a></li>
                <li>
                    <a href="#detr----eccv-2020" aria-label="DETR  - ECCV 2020">DETR  - ECCV 2020</a></li>
                <li>
                    <a href="#bert-----2018" aria-label="Bert  -  2018">Bert  -  2018</a></li>
                <li>
                    <a href="#gpt" aria-label="GPT">GPT</a></li>
                <li>
                    <a href="#clip----2021" aria-label="CLIP  - 2021">CLIP  - 2021</a><ul>
                        
                <li>
                    <a href="#%e5%b7%a5%e4%bd%9c%e7%9b%ae%e7%9a%84" aria-label="工作目的">工作目的</a></li>
                <li>
                    <a href="#%e5%af%b9%e6%af%94%e5%ad%a6%e4%b9%a0%e6%96%b9%e6%b3%95-train" aria-label="对比学习方法 （Train）">对比学习方法 （Train）</a></li>
                <li>
                    <a href="#%e9%a2%84%e6%b5%8b" aria-label="预测">预测</a></li>
                <li>
                    <a href="#%e7%bc%ba%e7%82%b9" aria-label="缺点">缺点</a></li></ul>
                </li>
                <li>
                    <a href="#two-stream----2014" aria-label="Two-Stream  - 2014">Two-Stream  - 2014</a></li>
                <li>
                    <a href="#moco----cvpr-2020" aria-label="MOCO  - CVPR 2020">MOCO  - CVPR 2020</a></li>
                <li>
                    <a href="#mae----cvpr2022" aria-label="MAE  - CVPR2022">MAE  - CVPR2022</a></li>
                <li>
                    <a href="#wav2vec-20----neurips-2020" aria-label="wav2vec 2.0  - NeurIPS 2020">wav2vec 2.0  - NeurIPS 2020</a></li>
                <li>
                    <a href="#whisper----2022-openai" aria-label="Whisper - 2022 OpenAI">Whisper - 2022 OpenAI</a></li>
                <li>
                    <a href="#cpc----machine-learning-2018" aria-label="CPC  - Machine Learning 2018">CPC  - Machine Learning 2018</a></li>
                <li>
                    <a href="#dalle-2----20223-openai" aria-label="DALL·E 2 - 2022.3 OpenAI">DALL·E 2 - 2022.3 OpenAI</a></li>
                <li>
                    <a href="#diffusion-model----neurips-2020" aria-label="diffusion model  - NeurIPS 2020">diffusion model  - NeurIPS 2020</a></li>
                <li>
                    <a href="#time-frequency-consistency----neurips-2022" aria-label="Time-Frequency Consistency  - NeurIPS 2022">Time-Frequency Consistency  - NeurIPS 2022</a></li>
                <li>
                    <a href="#comet----neurips--2023" aria-label="COMET  - NeurIPS  2023">COMET  - NeurIPS  2023</a></li>
                <li>
                    <a href="#ts2vec----aaai-22" aria-label="TS2Vec  - AAAI 22">TS2Vec  - AAAI 22</a></li>
                <li>
                    <a href="#timesurl----aaai-2024" aria-label="TimesURL  - AAAI 2024">TimesURL  - AAAI 2024</a></li>
                <li>
                    <a href="#mixup----2017-machine-learning" aria-label="Mixup  - 2017 Machine Learning">Mixup  - 2017 Machine Learning</a></li>
                <li>
                    <a href="#yolo-v1----cvpr-2016" aria-label="YOLO-v1  - CVPR 2016">YOLO-v1  - CVPR 2016</a></li>
                <li>
                    <a href="#semi-supervised-hybrid-loss-----machine-learning-2023" aria-label="Semi-Supervised Hybrid Loss  -  Machine Learning 2023">Semi-Supervised Hybrid Loss  -  Machine Learning 2023</a></li>
                <li>
                    <a href="#simclr----2020" aria-label="SimCLR  - 2020">SimCLR  - 2020</a></li>
                <li>
                    <a href="#vilt----2021" aria-label="ViLT  - 2021">ViLT  - 2021</a></li>
                <li>
                    <a href="#beit-v3-2022" aria-label="BEIT-v3 2022">BEIT-v3 2022</a></li>
                <li>
                    <a href="#albef----2021" aria-label="ALBEF  - 2021">ALBEF  - 2021</a></li>
                <li>
                    <a href="#instance-discrimination----2018" aria-label="Instance discrimination  - 2018">Instance discrimination  - 2018</a></li>
                <li>
                    <a href="#byol----20206" aria-label="BYOL  - 2020/6">BYOL  - 2020/6</a></li>
                <li>
                    <a href="#dino---2021" aria-label="DINO - 2021">DINO - 2021</a></li>
                <li>
                    <a href="#simsiam---cvpr-2021" aria-label="SimSiam - CVPR 2021">SimSiam - CVPR 2021</a></li>
                <li>
                    <a href="#segment-anything" aria-label="Segment Anything">Segment Anything</a></li>
                <li>
                    <a href="#knnk-nearest-neighbors" aria-label="KNN(K-Nearest Neighbors)">KNN(K-Nearest Neighbors)</a></li>
                <li>
                    <a href="#patchtst---iclr-2023" aria-label="PatchTST - ICLR 2023">PatchTST - ICLR 2023</a></li>
                <li>
                    <a href="#crossformer---iclr-2023" aria-label="CrossFormer - ICLR 2023">CrossFormer - ICLR 2023</a></li>
                <li>
                    <a href="#informer---aaai-2021-best" aria-label="Informer - AAAI 2021 Best">Informer - AAAI 2021 Best</a></li>
                <li>
                    <a href="#adaptive-token-dictionary----cvpr2024" aria-label="Adaptive Token Dictionary  - CVPR2024">Adaptive Token Dictionary  - CVPR2024</a></li>
                <li>
                    <a href="#poly-kernel-inception----cvpr-2024" aria-label="Poly Kernel Inception  - CVPR 2024">Poly Kernel Inception  - CVPR 2024</a></li>
                <li>
                    <a href="#danet----cvpr-2019" aria-label="DANet  - CVPR 2019">DANet  - CVPR 2019</a></li>
                <li>
                    <a href="#mixnet" aria-label="MixNet">MixNet</a></li>
                <li>
                    <a href="#multimodal-learning" aria-label="Multimodal Learning">Multimodal Learning</a></li>
                <li>
                    <a href="#dual-aggregation-transformer" aria-label="Dual Aggregation Transformer">Dual Aggregation Transformer</a></li>
                <li>
                    <a href="#dual-vision-transformer" aria-label="Dual Vision Transformer">Dual Vision Transformer</a></li>
                <li>
                    <a href="#fish-speech-tech-report" aria-label="Fish-Speech Tech-report">Fish-Speech Tech-report</a></li>
                <li>
                    <a href="#blip" aria-label="Blip">Blip</a></li>
                <li>
                    <a href="#bevformer" aria-label="BEVFormer">BEVFormer</a></li>
                <li>
                    <a href="#deformable-detr" aria-label="Deformable DETR">Deformable DETR</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h3 id="cam----cvpr-2015"><strong>CAM</strong>  - CVPR 2015<a hidden class="anchor" aria-hidden="true" href="#cam----cvpr-2015">#</a></h3>
<p><em>Learning Deep Features for Discriminative Localization</em></p>
<p><strong>弱监督对象定位</strong>  - 仅提供Image level label</p>
<p>期望：每个单元被其感受野内的某种视觉模式激活。因此 fk （表示空间位置 (x, y) 处最后一个卷积层中单元 k 的激活//输出特征图的一个像素）是该视觉模式存在的地图。类激活图只是这些视觉模式在不同空间位置的存在的加权线性和</p>
<p><strong>计算卷积特征图对于特定输出单元的重要性来实现的</strong></p>
<p>⭐⭐⭐网络可以保留其卓越的定位能力，直到最后一层   =&gt; 深层特征的定位能力</p>
<p>❗❗❗尽管接受了图像级标签的训练，CNN 仍具有出色的对象定位能力</p>
<p><strong>缺陷</strong>：卷积特征图→全局平均池化→softmax层  // 特定网络结构</p>
<p><img alt="image-20240322134603157" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FizSzlSfuX9fZOO_3HduKDclEzcI"></p>
<center style="color: red; font-weight: bold;">做法图示</center>
<p><img alt="image-20240322134441902" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FiBRel9zde6IJ3STCIWgQtDspaZx"></p>
<center style="color: red; font-weight: bold;">数学公式</center>
<p>在卷积特征图上执行全局平均池化，并将它们用作全连接层的特征，产生所需的输出分类;</p>
<p>❗❗❗将输出层的权重投影回卷积特征图来识别图像区域的重要性</p>
<hr>
<h3 id="grad-cam----iccv-2017">Grad-CAM  - ICCV 2017<a hidden class="anchor" aria-hidden="true" href="#grad-cam----iccv-2017">#</a></h3>
<p>适用CNN模型</p>
<p>但论文提到在CNN+LSTM的也能定位有区别的图像区域</p>
<p><img alt="image-20240322141249072" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FjU70VichK5n125Mrm0J9kJnVy43"></p>
<p><img alt="image-20240322141305661" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FpssA7XV30OJ4BKmn1aDL-1GNYSa"></p>
<p>α 捕获<strong>特征图</strong> k 对于目标<strong>类</strong> c 的<strong>重要性</strong>  // 与CAM的分类线性层权重作用一致</p>
<p><img alt="image-20240322141314248" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FoLeKuhJnfw8veLYf18fRWRhEy09"></p>
<p><strong>ReLU</strong>的作用，只对对感兴趣的类别有积极影响的特征感兴趣。负像素可能属于图像中的其他类别</p>
<p><em>上述操作 =&gt; 具有<strong>类别区分性</strong>并且可以很好地定位相关图像区域</em>   - 最后特征图比较小!</p>
<pre><code>				  但缺乏显示细粒度重要性的能力 （能区分猫狗，但对为什么识别为猫，不够精确）
</code></pre>
<p>通过点乘法融合 <strong>引导反向传播</strong> 和 <strong>Grad-CAM</strong> =&gt; 可视化</p>
<p>Grad-CAM : 类别区分性</p>
<p>Guided Backprop： 细节纹理重要程序。 做法：将梯度值小于等于零的部分置为零，保留梯度值大于零的部分 =&gt; 以突出输入图像中对预测结果有积极影响的区域，来实现对神经网络中每个像素对最终预测结果的影响进行可视化和解释</p>
<p><strong>浅层卷积关注纹理特征，深层网络关注本质的那种特征</strong>？</p>
<hr>
<h3 id="detr----eccv-2020">DETR  - ECCV 2020<a hidden class="anchor" aria-hidden="true" href="#detr----eccv-2020">#</a></h3>
<p><em><strong>⭐End-to-End⭐</strong></em> Object Detection with Transformers</p>
<p>传统：设置锚框 + 非极大值抑制(去除多余的框)</p>
<p>创新：集合预测(预测分类 + 锚框)</p>
<p><img alt="image-20240322202435323" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FuvmZgAnU9ncG2a5qW5EUCNP48H1"></p>
<center style="color: red; font-weight: bold;">前向流程</center>
<p><img alt="image-20240322202514460" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FgwVYzzA9oyIF86-03QbkEQU8bms"></p>
<center style="color: red; font-weight: bold;">模型框架</center>
<p>CNN backbone  - local information fusion</p>
<p>Transformer encoder  - global information fusion</p>
<p>object queries  - learnable information vector   作用： =&gt; anchor</p>
<p><em><strong>FFN:  1. classification =&gt; output class vector  2. box =&gt; output 4 number [center_x, center_y, width, hight]</strong></em></p>
<p>object queries: 作用就是锚框，并且一次性生成100个 &raquo; 图片检测的物体数</p>
<p>如何将object queries 与 groundtrue一一对应？</p>
<p>匈牙利算法 寻找最佳匹配</p>
<p>匹配loss = 分类loss(分类正确率) + 框loss(框的重叠度)</p>
<p>=&gt; 匈牙利算法 =&gt; 哪些框与GT最佳匹配(预测框与GT一一对应)</p>
<p>最终回传梯度优化参数LOSS = 分类loss(分类正确率) + [框loss + 与框大小无关的iou loss]</p>
<p>因为框也是生成的，且Transformer容易出大框(全局建模)</p>
<hr>
<h3 id="bert-----2018">Bert  -  2018<a hidden class="anchor" aria-hidden="true" href="#bert-----2018">#</a></h3>
<p><strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong>   <em>- Computation and Language</em></p>
<p>Bidirectional Transformers 意思是</p>
<p>Transformer Encoder中的自注意力计算是全局的，每个Token能观测到其余的Token序列； 而</p>
<p>Transformer Decoder中由于进行的是Masked Multi Head Self Attention，所以序列只能观测到自己与之前的语境；</p>
<p>这对于文本上下文语境建模是有弊端的！</p>
<p>(生成式无监督学习)</p>
<p><strong>总体结构</strong></p>
<p><img alt="image-20240327160149024" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fl4-SrUiEyrZ5ep1hsySNk5hJ6Yz"></p>
<p>[CLS] 分类Token，凝聚全局语义信息</p>
<p>[SEP] 分割符，划分句子范围</p>
<p>用的某个语料库进行词嵌入</p>
<p><img alt="image-20240327202952064" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FtNYoVsEqoeq74FY2SO4dBDE2WyO"></p>
<p>⭐<strong>训练方法</strong></p>
<ul>
<li>
<p>完型填空：使某个词随机被 [Mask] 字符遮挡；为防止模型对[Mask]字符敏感，遮挡时使用概率遮挡 =&gt; 1. 仍替换为[Mask]字符 2.随机替换为其他字符 3. 保持不变 =&gt; 迫使模型学习上下文语境   <strong style='color:blue;'>[句子内信息建模]</strong></p>
</li>
<li>
<p>预测下一句：  <strong style='color:blue;'>[句子间信息建模]</strong></p>
</li>
</ul>
<p><strong>丰富的上下文信息</strong>：通过考虑单词的左右上下文，BERT 能够更好地理解词义和句法结构，这对于理解语言的复杂性至关重要。</p>
<hr>
<h3 id="gpt">GPT<a hidden class="anchor" aria-hidden="true" href="#gpt">#</a></h3>
<p>generative pretrain transformer</p>
<p><em>初略版</em></p>
<p>模型图：(Transformer Decoder -仅Masked Attention版)</p>
<p><img alt="image-20240505163406185" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FkZKmDcVtOr6qyohAj5rVJdX6s7T"></p>
<p>⭐ <em>input:</em>  word =&gt; index(查找词汇表) =&gt; embedding   +  position embedding</p>
<p>[batch, sequence_length, embedding_dim]	 # sequence_length 可由多个句子组成，可以使用&lt;s&gt;标识句子结束，&lt;pad&gt;用来填充，使得sequence_length在batch内长度一致</p>
<p>⭐ <em>train：</em></p>
<p>x1 =&gt; x2</p>
<p>x1, x2 =&gt; x3</p>
<p>x1, x2, x3 =&gt; x4</p>
<p>因为每个词都要预测下一个词，故使用masked attention （mask-softmax），防止答案泄漏</p>
<hr>
<h3 id="clip----2021">CLIP  - 2021<a hidden class="anchor" aria-hidden="true" href="#clip----2021">#</a></h3>
<p><strong>Learning Transferable Visual Models From Natural Language Supervision</strong></p>
<p><strong>实现zero-shot，上游大数据集预训练好，下游任务迁移学习无需样本微调</strong></p>
<p><strong>多模态模型的总体目标就是：训练一个模型，一方面能统一特征表达，另一方面又能让不同模态特征间学到相关性</strong></p>
<p>(判别式无监督学习)</p>
<h4 id="工作目的">工作目的<a hidden class="anchor" aria-hidden="true" href="#工作目的">#</a></h4>
<p>痛点：</p>
<ul>
<li>
<p>在特点数据集上进行标签训练 =&gt; 输入没见过的类别，那么模型就不能输出正确的结果</p>
</li>
<li>
<p>数据出现分布偏移，动物图片与卡通动物图片 =&gt; 识别不出来</p>
</li>
</ul>
<p>图片 - 文字描述 ， <strong>模型学习配对关系</strong></p>
<p><em>一个对象的不同视角表示：图片和文本描述</em></p>
<h4 id="对比学习方法-train"><strong>对比学习方法</strong> <strong>（Train）</strong><a hidden class="anchor" aria-hidden="true" href="#对比学习方法-train">#</a></h4>
<p><img alt="image-20240327200843127" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fu1kXHXwvqyJvl-W0nklKpmttZ44"></p>
<ol>
<li>Text Encoder (resnet/vit) 学习文本描述的 深度特征  - 单模态内特征 // T_i == 一个文本特征</li>
<li>Image Encoder(transformer) 学习图片的 深度特征  - 单模态内特征  // I_i == 一个图像特征</li>
<li>将多模态特征投影到跨模态空间  // 矩阵映射，(特征向量)到同一纬度</li>
<li>计算余弦相似度，很明显，正确配对的位置为对角线</li>
<li>计算Loss： （相似度logit作为预测分数）</li>
<li><strong>按行计算Loss</strong>，在每一行范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一张图片，我们都希望找到和它最相似的文字</li>
<li><strong>按列计算Loss</strong>，在每一列的范围内做softmax，然后计算cross_entropy（蓝色格子部分是真值）。这样计算Loss的意义是：对于每一段文字，我们都希望找到和它最相似的图片</li>
<li><strong>最后将这两个Loss相加取平均</strong>，代表我们在模型优化过程中<strong>考虑了“图片-&gt;文字”和“文字-&gt;图片”的双向关系</strong></li>
</ol>
<p>encoder均从头开始训练</p>
<h4 id="预测">预测<a hidden class="anchor" aria-hidden="true" href="#预测">#</a></h4>
<p><img alt="image-20240327201936442" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FhcxDiV_Ii24QNK9mAIQFKmvrVAK"></p>
<ul>
<li>创建一个标签全集， [f&rsquo;A photo of a {object}&rsquo; for object in dataset_labels]</li>
<li>Text Encoder 学习上述 模板文字描述 的 深度特征</li>
<li>Image Encoder 学习 待预测图片 的 深度特征</li>
<li>计算余弦相似度 =&gt; 取最高分作为预测目标</li>
</ul>
<h4 id="缺点">缺点<a hidden class="anchor" aria-hidden="true" href="#缺点">#</a></h4>
<ul>
<li>每次预测需要构建标签全集描述</li>
<li>对抽象任务，性能较差</li>
</ul>
<hr>
<h3 id="two-stream----2014">Two-Stream  - 2014<a hidden class="anchor" aria-hidden="true" href="#two-stream----2014">#</a></h3>
<p><strong>Two-Stream Convolutional Networks for Action Recognition in Videos</strong></p>
<p>⭐先前工作中通过使用堆叠视频帧作为网络的输入来解决此任务，但结果明显比最好的手工制作的浅层表示差。 =&gt; 这可能表明学习的时空特征不能很好地捕捉运动（简单堆叠 =&gt; 让模型从庞大的数据中学习❌很难）  =&gt; 模型很难识别该类特征 =&gt; <strong>预先处理成模型擅长的数据形式</strong></p>
<p>❗❗❗虽然多帧信息很重要，但以<strong>适当的方式</strong>将其呈现给 ConvNet 也很重要（饱和）</p>
<p>信息显式建模  =&gt; 可以简化学习过程</p>
<p><em>创新点</em></p>
<ul>
<li>空间流从静止视频帧执行动作识别</li>
<li>时间流以识别密集光流形式的运动动作</li>
</ul>
<p>=&gt; 贴合人类视觉：识别物体  +  识别运动  =&gt; ⭐[结果角度]表明两个识别流是互补的</p>
<p>光流：帧帧之间像素变化(描述运动变化的数据形式) | 水平方向和垂直方向  <strong>&lt;= 有用的线索</strong>  (与魔改模型不一样的思路)</p>
<p>⭐⭐⭐此类输入<strong>明确描述</strong>了视频帧之间的<strong>运动</strong> =&gt; 这使得识别更容易 =&gt; 因为网络不需要隐式估计运动  <em><strong>（显示建模）</strong></em></p>
<p>可以从<strong>光流数据</strong> =&gt; <strong>时空局部</strong>特征 || <strong>运动学</strong>特征  || 运动是使用光流位移场<strong>明确表示</strong>的</p>
<p><strong>模型框架</strong>（双模态）</p>
<p><img alt="image-20240328154319036" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fv9O6s0-ISjzOZYNWYnsWeFDMD9H"></p>
<p><strong>提高模型迁移能力</strong>   类似于CLIP的目的</p>
<p>考虑将两个数据集合并为一个，由于类集之间的交叉，这并不简单</p>
<p>=&gt;  多任务学习  =&gt; 最后生成多个分类头，对应两个数据集分类。混合多个数据集进行实验</p>
<hr>
<h3 id="moco----cvpr-2020">MOCO  - CVPR 2020<a hidden class="anchor" aria-hidden="true" href="#moco----cvpr-2020">#</a></h3>
<p><em><strong>Momentum Contrast for Unsupervised Visual Representation Learning</strong></em></p>
<p>(判别式无监督学习)</p>
<p><em><strong>目标：</strong></em></p>
<pre><code>1️⃣ 构建一个足够大的动态词典，包含足够多的负样本，使模型真能够学到判别式的特征; 在海量数据中学到真正的样本分布

2️⃣ 因为词典是动态变化的，为了使词典中的负样本特征尽可能的保持一致性(模型参数不同，时间维度上，得到的特征向量存在不一致性)，提出动量更新 =&gt; 动量模型的缓慢更新确保了字典中的特征相对稳定，从而提供更一致的负样本，提升对比学习的效果。
</code></pre>
<p>$$
θ_{k} ←mθ_{k} + (1 −m)θ_{q}
$$
m ∈ [0, 1) 是动量系数。论文中Query Encoder 和Key Encoder是一样配置架构的编码器。  目的，缓慢的更新Key Encoder，构建一个又大又一致的动态词典。</p>
<p><img alt="image-20240408212751319" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FlwCsI3Y0anvjZAw_Uhq4q_lirYh"></p>
<p><strong>框架伪代码</strong></p>
<p>pretext task ： 实例判别任务。目标拉近正样本对在特征空间的距离，并使负样本对尽可能的远离。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="c1"># f_q, f_k: encoder networks for query and key</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"><span class="c1"># queue: dictionary as a queue of K keys (CxK)</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"><span class="c1"># m: momentum</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"><span class="c1"># t: temperature</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl"><span class="n">f_k</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">f_q</span><span class="o">.</span><span class="n">params</span> <span class="c1"># initialize</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl"><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span> <span class="c1"># load a minibatch x with N samples</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">    <span class="n">x_q</span> <span class="o">=</span> <span class="n">aug</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># a randomly augmented version</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">    <span class="n">x_k</span> <span class="o">=</span> <span class="n">aug</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># another randomly augmented version</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">    
</span></span><span class="line"><span class="ln">10</span><span class="cl">    <span class="n">q</span> <span class="o">=</span> <span class="n">f_q</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x_q</span><span class="p">)</span> <span class="c1"># queries: NxC</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl">    <span class="n">k</span> <span class="o">=</span> <span class="n">f_k</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x_k</span><span class="p">)</span> <span class="c1"># keys: NxC</span>
</span></span><span class="line"><span class="ln">12</span><span class="cl">    <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="c1"># no gradient to keys</span>
</span></span><span class="line"><span class="ln">13</span><span class="cl">    
</span></span><span class="line"><span class="ln">14</span><span class="cl">    <span class="c1"># positive logits: Nx1</span>
</span></span><span class="line"><span class="ln">15</span><span class="cl">    <span class="n">l_pos</span> <span class="o">=</span> <span class="n">bmm</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">C</span><span class="p">),</span> <span class="n">k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="ln">16</span><span class="cl">    
</span></span><span class="line"><span class="ln">17</span><span class="cl">    <span class="c1"># negative logits: NxK</span>
</span></span><span class="line"><span class="ln">18</span><span class="cl">    <span class="n">l_neg</span> <span class="o">=</span> <span class="n">mm</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">C</span><span class="p">),</span> <span class="n">queue</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">C</span><span class="p">,</span><span class="n">K</span><span class="p">))</span>
</span></span><span class="line"><span class="ln">19</span><span class="cl">    
</span></span><span class="line"><span class="ln">20</span><span class="cl">    <span class="c1"># logits: Nx(1+K)</span>
</span></span><span class="line"><span class="ln">21</span><span class="cl">    <span class="n">logits</span> <span class="o">=</span> <span class="n">cat</span><span class="p">([</span><span class="n">l_pos</span><span class="p">,</span> <span class="n">l_neg</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">22</span><span class="cl">    
</span></span><span class="line"><span class="ln">23</span><span class="cl">    <span class="c1"># contrastive loss, Eqn.(1)</span>
</span></span><span class="line"><span class="ln">24</span><span class="cl">    <span class="n">labels</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="c1"># positives are the 0-th</span>
</span></span><span class="line"><span class="ln">25</span><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">logits</span><span class="o">/</span><span class="n">t</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">26</span><span class="cl">    
</span></span><span class="line"><span class="ln">27</span><span class="cl">    <span class="c1"># SGD update: query network</span>
</span></span><span class="line"><span class="ln">28</span><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="ln">29</span><span class="cl">    <span class="n">update</span><span class="p">(</span><span class="n">f_q</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">30</span><span class="cl">    
</span></span><span class="line"><span class="ln">31</span><span class="cl">    <span class="c1"># momentum update: key network</span>
</span></span><span class="line"><span class="ln">32</span><span class="cl">    <span class="n">f_k</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">m</span><span class="o">*</span><span class="n">f_k</span><span class="o">.</span><span class="n">params</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">m</span><span class="p">)</span><span class="o">*</span><span class="n">f_q</span><span class="o">.</span><span class="n">params</span>
</span></span><span class="line"><span class="ln">33</span><span class="cl">    
</span></span><span class="line"><span class="ln">34</span><span class="cl">    <span class="c1"># update dictionary</span>
</span></span><span class="line"><span class="ln">35</span><span class="cl">    <span class="n">enqueue</span><span class="p">(</span><span class="n">queue</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="c1"># enqueue the current minibatch</span>
</span></span><span class="line"><span class="ln">36</span><span class="cl">    <span class="n">dequeue</span><span class="p">(</span><span class="n">queue</span><span class="p">)</span> <span class="c1"># dequeue the earliest minibatch</span>
</span></span></code></pre></div><p>两个不同视角的同一张图片为正样本对，不同图片为负样本对。</p>
<p>抽特征：q, k；   最大化q k相似且与负样本远离</p>
<p>更新Q_Encoder, 使用Q_Encoder参数更新K_Encoder，动量缓慢更新</p>
<p>无监督预训练后好，取Q_Encoder作为抽取特征骨干网络，冻结其参数，微调分类头，在进行泛化测试</p>
<hr>
<h3 id="mae----cvpr2022">MAE  - CVPR2022<a hidden class="anchor" aria-hidden="true" href="#mae----cvpr2022">#</a></h3>
<p><em><strong>Masked Autoencoders Are Scalable Vision Learners</strong></em></p>
<p>非对称掩码自动编码器；Encoder和Decoder架构可以不同</p>
<p>(生成式无监督学习)</p>
<p>CV领域的Bert</p>
<p>⭐NLP与CV的不同：</p>
<ul>
<li>
<p>信息密度不同</p>
<ul>
<li>
<p>NLP：句子信息语义很高，信息密度也高。（人类语言-事先浓缩过的信息）</p>
</li>
<li>
<p>视觉：信息很冗余，也没高级的语义（自然界），像素可以被相邻的重建恢复</p>
</li>
</ul>
</li>
<li>
<p>恢复难度不一致</p>
<ul>
<li>恢复高级语义单词和恢复像素级(低级语义)图片，难度也不一样。</li>
</ul>
</li>
</ul>
<p><em><strong>框架：</strong></em></p>
<p><img alt="image-20240409131436588" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Frag5yZW-mr5iE-Qq9Dg3Mj9TQQ5"></p>
<p>Encoder：位置编码所有Patch都加上。但仅输入未被Mask的Patch。并且Mask比例很高(论文mask75%patch)，迫使模型学到高维的特征，而非捷径。</p>
<p>Decoder：Mask Patch是自学习的向量，并且和Encoded Patch在位置上一致，再次添加位置信息。</p>
<p>简单实现（shuffle，截取前面的作为Encoder输入，后面Patch被Mask；再shuffle逆操作，将Encoded Patch和learnabel patch位置对齐组合好进行Decoder。再重建像素）</p>
<p>通过预测每个屏蔽补丁的像素值来重建输入。解码器的最后一层是线性投影，其输出通道的数量等于补丁中像素值的数量。</p>
<p><strong>损失函数</strong>计算像素空间中重建图像和原始图像之间的<strong>均方误差</strong>。</p>
<p>问题：</p>
<pre><code>预训练的输入中具有很大一部分掩码标记，而这在下游任务未损坏的图像中不存在。这种差距可能会降低部署的准确性。
</code></pre>
<p><strong>自监督学习</strong>方法通常侧重于预训练的不同借口任务。</p>
<p><strong>对比学习</strong>对两个或多个视图之间的图像相似性和相异性进行建模。</p>
<hr>
<h3 id="wav2vec-20----neurips-2020">wav2vec 2.0  - NeurIPS 2020<a hidden class="anchor" aria-hidden="true" href="#wav2vec-20----neurips-2020">#</a></h3>
<p><em><strong>wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</strong></em></p>
<p><strong>时序信号版本的Bert</strong></p>
<p>[自监督]学习通用特征 =&gt; 再微调任务头</p>
<p>(判别式无监督学习)</p>
<p><em><strong>总体框架：</strong></em></p>
<p><img alt="image-20240411195125065" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FqUfVASNdBKkGcmm0F4GiZWIQKjb"></p>
<p>1️⃣ 原始信号[X] =CNN=&gt; 浅在特征表示 [Z]</p>
<p>2️⃣ 浅在特征表示 [Z] =Transformer Encoder=&gt; 全局上下文特征表示 [C]</p>
<p>3️⃣ 浅在特征表示 [Z] =量化器=&gt; 对比目标[Q]</p>
<ul>
<li>把原来连续的特征空间假设是d维，拆分成G个子空间（codebook），每个子空间维度是d/G。</li>
<li>然后分别在每个子空间里面聚类（K-mean什么的），一共获得V个中心和其中心特征。</li>
<li>每个类别的特征用其中心特征代替。</li>
</ul>
<p><img alt="image-20240411204733337" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FmcVQRp_69PpmPRTef5A9RL5WkXm"></p>
<p><img alt="image-20240411204739181" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FjLxT0_K3dukPMzGasHHX8RUkVYI"></p>
<p>量化qt和对应ct</p>
<p>❌ Quantization module 部分不理解</p>
<p><strong>Mask</strong></p>
<p><img alt="image-20240411210825518" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FlRfun5vPoin9xqspVf4f0lzVOZO"></p>
<p>随机起点，遮挡后面t个时间步</p>
<p><img alt="image-20240411210806211" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FnkR4fVzwbvuq6RjaCs_dndb6v5h"></p>
<p><em><strong>对比损失</strong></em></p>
<pre><code>	![image-20240411210912407](http://sthda9dn6.hd-bkt.clouddn.com/FqYwQ0MDlAXtdrK4q6VHMuWRn3km)
</code></pre>
<p>包括 qt 和 K 个干扰项</p>
<p><em><strong>❗❗❗理解不太清晰</strong></em></p>
<hr>
<h3 id="whisper----2022-openai"><strong>Whisper  - 2022 OpenAI</strong><a hidden class="anchor" aria-hidden="true" href="#whisper----2022-openai">#</a></h3>
<p><em><strong>Robust Speech Recognition via Large-Scale Weak Supervision</strong></em></p>
<p>大力出奇迹</p>
<p><em><strong>模型结构</strong></em></p>
<p><img alt="image-20240411211708389" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FrJeqrQ_ZkI5Dq1CvF8QkRfMxcaH"></p>
<p><em><strong>多任务训练</strong></em></p>
<ul>
<li>
<p>[英文口语=&gt; 英文]                            语音识别</p>
</li>
<li>
<p>[多语言口语 =&gt; 英文]                       语音识别 +翻译</p>
</li>
<li>
<p>[多语言口语 =&gt; 对应语言文字]        语音识别</p>
</li>
<li>
<p>识别背景音(无内容声音)</p>
</li>
</ul>
<p>⭐⭐⭐<strong>信号 =&gt; Log-Mel Spectrogram(频谱图)</strong></p>
<p>音素</p>
<p><em><strong>⭐⭐⭐数据集</strong></em></p>
<p>680k小时，超大数据集。 在此基础上预训练，并且0样本迁移(无需特定任务微调)</p>
<hr>
<h3 id="cpc----machine-learning-2018">CPC  - Machine Learning 2018<a hidden class="anchor" aria-hidden="true" href="#cpc----machine-learning-2018">#</a></h3>
<p><em><strong>Representation Learning with Contrastive Predictive Coding</strong></em></p>
<p><em>Contrastive Predictive Coding   - 无监督学习</em></p>
<p><strong>通过预测未来，学习特征表示</strong>  (学习对(高维)信号不同部分之间的底层共享信息进行编码的表示  - 局部平滑度)</p>
<p>不预测原始信号，而是对高维嵌入依赖建模</p>
<p>(判别式无监督学习)</p>
<p><em><strong>CPC框架</strong></em></p>
<p><img alt="image-20240412150754949" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fkg2K2xjA7uBbk6vyr6jA7lWejzG">
$$
g_{enc}:  local\ feature\ learning\
g_{ar}:  global\ context\ learning
$$</p>
<p><em><strong>对比学习</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln">1</span><span class="cl"><span class="c1"># 序列： [x1, x2, x3, x4, x5, x6, x7, x8]</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl">
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="c1"># positive sample</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl"><span class="c1"># x1, x2, x3, x4 =&gt; c &lt;=&gt; x5, x6, x7, x8</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl">
</span></span><span class="line"><span class="ln">6</span><span class="cl"><span class="c1"># negative sample</span>
</span></span><span class="line"><span class="ln">7</span><span class="cl"><span class="c1"># x1, x2, x3, x4 =&gt; c &lt;=&gt; xa, xb, xc, xd (同batch中其他的)</span>
</span></span></code></pre></div><p>迫使模型学习序列间的high level feature，学习这种内部的顺序逻辑关系</p>
<p><strong>互信息公式</strong>  - 衡量两个随机变量之间的相互依赖程度
$$
I(x;c)=\sum_{x,c}p(x,c)\log\frac{p(x|c)}{p(x)}. \
I(x;c): x 与 c 的互信息\
p(x,c): x 和 c 同时发生的联合概率分布\
p(x|c): 给定 c 的条件下，x 发生的条件概率分布\
p(x): x 的边缘概率分布
$$
<strong>InfoNCE Loss</strong></p>
<p><strong>最小化CPC定义的损失 =&gt; 实际上最大化 context c_t 和待预测正样本 X_t 之间的互信息</strong></p>
<p>优化目标：最大化似然概率</p>
<p><em>正相关</em></p>
<p><img alt="image-20240415213652095" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fld0d7f2o5eYFLT27mG-kwpf69p9"></p>
<p><em>近似计算互信息</em></p>
<p><img alt="image-20240415213622018" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FiE2tChXaZTae_EuLA5UxEl-BwbM"></p>
<p><em>Loss</em></p>
<p><img alt="image-20240415213722239" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Frw_sTw_6jzv57J91HJTXwqKjVN-"></p>
<p>X = {x1, x2, &hellip;, xN} N个负样本，从batch中其他数据中采样</p>
<p>E/X  表示似然概率</p>
<p>最大化互信息 =&gt; 最小化Loss  (互信息下界)</p>
<p><img alt="image-20240415213954622" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fmr-VnocMhPJJtXfiZGyk_JrBuo1"></p>
<p><strong>完整InfoNCE-Loss</strong>
$$
\text{InfoNCELoss} = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{\text{exp}(s(x_i, x_i^+))}{\text{exp}(s(x_i, x_i^+)) + \sum_{j=1}^{K} \text{exp}(s(x_i, x_j^-))}
$$</p>
<p><em>不太了解这个最大化互信息的Loss</em></p>
<hr>
<h3 id="dalle-2----20223-openai"><strong>DALL·E 2</strong>  - 2022.3 OpenAI<a hidden class="anchor" aria-hidden="true" href="#dalle-2----20223-openai">#</a></h3>
<p><em>Hierarchical Text-Conditional Image Generation with CLIP Latents</em></p>
<p><em><strong>层级式图生文</strong></em></p>
<p><strong>模型架构</strong></p>
<p><img alt="image-20240416192447568" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fl5NOQ7na3jY5w-4oMbQClRzwjgn"></p>
<p><em><strong>描述：</strong></em></p>
<ul>
<li>虚线上是CLIP架构(文本和图像的联合表示空间)，学习图文对的关联信息</li>
<li>虚线下是生成框架，prior模型根据Text Embedding生成出CLIP对应的Image Embedding， decoder(diffussion model)根据Image Embedding进行重建</li>
</ul>
<p><em>⭐分步训练</em></p>
<p><em><strong>prior</strong></em></p>
<p>生成CLIP image Embedding (diffusion model)</p>
<p><img alt="image-20240416194435205" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FlbLpzwV8mvw-N7XcRUxXpKN_tVX"></p>
<p><em><strong>decoder</strong></em></p>
<p>是根据image Embedding生成图片 &ndash; 并且这个decoder是多个堆叠，先生成低分辨率，再高清化  -  (diffusion model)</p>
<p><em><strong>简洁表示：</strong></em></p>
<p><img alt="image-20240416193905706" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FkLqbQZZdzkbNQsmfckYUFfbO7OI"></p>
<p><em>y - 文字</em></p>
<p><em>x - 图片</em></p>
<p><em>zi - 图片嵌入  （显式生成图像表示可以提高图像多样性，同时将照片真实性和标题相似度的损失降至最低）</em></p>
<hr>
<h3 id="diffusion-model----neurips-2020">diffusion model  - NeurIPS 2020<a hidden class="anchor" aria-hidden="true" href="#diffusion-model----neurips-2020">#</a></h3>
<p><em><strong>Denoising Diffusion Probabilistic Models</strong></em></p>
<p><strong>框架：</strong></p>
<p><em><strong>加噪 + 去噪(还原)</strong></em></p>
<p><img alt="image-20240417165835467" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fq1ieN5eRAPoPKJz9gRb3UUVXuEa"></p>
<p><em><strong>正向过程：</strong></em></p>
<p>1️⃣ Xt 是 前一张图片加噪生成的，Z1是服从正太分布的噪声</p>
<p><img alt="image-20240417170233229" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fu4Ef01QR3UNTEVo22lrQcOAktpQ"></p>
<p><img alt="image-20240417170238304" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fi78LTSVdR9ki7Tkg_P3-MXWSP-U"></p>
<p>βt是超参数=范围为[0.0001,0.02]递增，则αt 是随时间减少， 表示公式一中原图信息越来越少，噪声越来越重</p>
<p>2️⃣ 递推带入一下</p>
<p><img alt="image-20240417170436248" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FiHTiSdVmNrgDsNfKd8o8ajxz5g3"></p>
<p><img alt="image-20240417170509368" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FjGR-X-OPfIzkMdjwnKR3-BwDzMq"></p>
<p>最后可得···</p>
<p><img alt="image-20240417170704121" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FhFVZkEeHGk4XHiNcobYmz_Kyx1p"></p>
<p>Zt_hat 是一个服从正太分布的随机噪声，at_hat = at*at-1*···*a1, <strong>可由X0直接产生任意时间步的加噪图片</strong></p>
<p><strong>反向去噪过程：</strong></p>
<p><em>核心基础</em></p>
<p><img alt="image-20240417171006179" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FuLyZjUK_C7OzaR_Sx5izsv8qVfm"></p>
<p>1️⃣ 用Xt生成Xt-1 ，按贝叶斯公式转换</p>
<p><img alt="image-20240417171023360" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FlPyQq-wecfnCuAKSYOq9yDXtvWy"></p>
<p>2️⃣ q(Xt|Xt-1) == q(Xt|Xt-1, X0)， 而q(Xt-1) == q(Xt-1|X0) <strong>任意步加噪图可由原图直接产生</strong></p>
<p><img alt="image-20240417171223592" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FrLTV45PiHS6n-plx9FCssOtXc8E"></p>
<p>3️⃣ 反解公式7， X0可由Xt进行估计</p>
<p><img alt="image-20240417171845271" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FhkEASm1Lj9tDI7fDu3WnhzbDeTS"></p>
<p>4️⃣ 带入并整理</p>
<p><img alt="image-20240417172918885" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fp8r3kgTbQNu6B5nUdKDOQt0Lzfj"></p>
<p>··· 因此可根据Xt =&gt; Xt-1， 下式为最终的<strong>去噪公式</strong>
$$
x = \frac{1}{\sqrt{\alpha}} \left( x - \frac{1 - \alpha}{\sqrt{1 - \alpha_{\text{hat}}}} \cdot \text{predicted_noise} \right) + \sqrt{\beta} \cdot \text{noise}
$$
β noise 保证多样性</p>
<p><em>Ɛθ表示预测模型</em></p>
<p><img alt="image-20240417175432578" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FiJbpIpLJ6S2emRh_Rh6kg19Dhyp"></p>
<p><em><strong>代码逻辑</strong></em></p>
<p>训练：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">images</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pbar</span><span class="p">):</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl">    <span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl">    <span class="n">t</span> <span class="o">=</span> <span class="n">diffusion</span><span class="o">.</span><span class="n">sample_timesteps</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>	<span class="c1"># 采样几个时间步进行训练</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl">    <span class="n">x_t</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">diffusion</span><span class="o">.</span><span class="n">noise_images</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>	<span class="c1"># @ 公式-7</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">    <span class="n">predicted_noise</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>	<span class="c1"># Unet</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">mse</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl">    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl">
</span></span><span class="line"><span class="ln">12</span><span class="cl"><span class="n">sampled_images</span> <span class="o">=</span> <span class="n">diffusion</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">images</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># @ 去噪公式</span>
</span></span></code></pre></div><p><em>预测模型</em></p>
<p>Unet 带时间点嵌入(用余弦-位置嵌入实现的)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl">    <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl">    <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_dim</span><span class="p">)</span>		<span class="c1"># 嵌入时间步顺序</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl">
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">    <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>	    <span class="c1"># cnn</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl">    <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">down1</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>  <span class="c1"># pooling</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">    <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa1</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>	    <span class="c1"># self attention</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">    <span class="n">x3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">down2</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">    <span class="n">x3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa2</span><span class="p">(</span><span class="n">x3</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl">    <span class="n">x4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">down3</span><span class="p">(</span><span class="n">x3</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl">    <span class="n">x4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa3</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">12</span><span class="cl">
</span></span><span class="line"><span class="ln">13</span><span class="cl">    <span class="n">x4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bot1</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>	
</span></span><span class="line"><span class="ln">14</span><span class="cl">    <span class="n">x4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bot2</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">15</span><span class="cl">    <span class="n">x4</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bot3</span><span class="p">(</span><span class="n">x4</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">16</span><span class="cl">
</span></span><span class="line"><span class="ln">17</span><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up1</span><span class="p">(</span><span class="n">x4</span><span class="p">,</span> <span class="n">x3</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>	<span class="c1"># 插值上采样，再拼接skip connect</span>
</span></span><span class="line"><span class="ln">18</span><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">19</span><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">20</span><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa5</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">21</span><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">22</span><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sa6</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">23</span><span class="cl">    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">outc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">24</span><span class="cl">    <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></div><p>p(Xt-2|Xt-1, t, y)  t是时间步嵌入，y是条件嵌入</p>
<p>最简单的融合方法就是相加</p>
<p>？？？ 疑惑点 - 反向过程求的t时刻的均值方差用在哪了？</p>
<p>2025/1/14 更新：</p>
<p>// X =&gt; X_noise_t  可以一步生成，可以时间步t可以直接生成对应t时间步的噪声图。</p>
<p>// 使用U-net预测时间步t的噪声 使用MSE-Loss进行训练</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="c1"># 训练循环</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl">    <span class="k">for</span> <span class="n">x_0</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>  <span class="c1"># x_0 是原始数据</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl">        <span class="c1"># 1. 随机选择一个时间步 t</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">x_0</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),))</span>  <span class="c1"># 为每个样本随机选择一个时间步</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl">
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">        <span class="c1"># 2. 前向过程：添加噪声</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">        <span class="n">epsilon</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_0</span><span class="p">)</span>  <span class="c1"># 采样噪声</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">        <span class="n">alpha_bar_t_t</span> <span class="o">=</span> <span class="n">alpha_bar_t</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 选择对应时间步的 alpha_bar_t</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl">        <span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alpha_bar_t_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_0</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_bar_t_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">epsilon</span>  <span class="c1"># 添加噪声</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl">
</span></span><span class="line"><span class="ln">12</span><span class="cl">        <span class="c1"># 3. 反向过程：预测噪声</span>
</span></span><span class="line"><span class="ln">13</span><span class="cl">        <span class="n">predicted_epsilon</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>  <span class="c1"># 模型预测噪声</span>
</span></span><span class="line"><span class="ln">14</span><span class="cl">
</span></span><span class="line"><span class="ln">15</span><span class="cl">        <span class="c1"># 4. 计算损失函数</span>
</span></span><span class="line"><span class="ln">16</span><span class="cl">        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">predicted_epsilon</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>  <span class="c1"># 均方误差损失</span>
</span></span><span class="line"><span class="ln">17</span><span class="cl">
</span></span><span class="line"><span class="ln">18</span><span class="cl">        <span class="c1"># 5. 反向传播和优化</span>
</span></span><span class="line"><span class="ln">19</span><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="ln">20</span><span class="cl">        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="ln">21</span><span class="cl">        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span></code></pre></div><p>// 串行，一步一步去噪</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="c1"># 生成过程</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"><span class="k">def</span> <span class="nf">generate_samples</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">alpha_bar_t</span><span class="p">):</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl">    <span class="n">x_T</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>  <span class="c1"># 从标准正态分布中采样初始噪声</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl">    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>  <span class="c1"># 从 T-1 到 1</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">        <span class="c1"># 预测噪声</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl">        <span class="n">epsilon</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_T</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">        
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">        <span class="c1"># 计算去噪后的数据   &lt;= 消除噪声</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">        <span class="n">alpha_t</span> <span class="o">=</span> <span class="n">alpha_bar_t</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">/</span> <span class="n">alpha_bar_t</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl">        <span class="n">x_T</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_T</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alpha_t</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl">    
</span></span><span class="line"><span class="ln">12</span><span class="cl">    <span class="k">return</span> <span class="n">x_T</span>
</span></span></code></pre></div><hr>
<h3 id="time-frequency-consistency----neurips-2022">Time-Frequency Consistency  - NeurIPS 2022<a hidden class="anchor" aria-hidden="true" href="#time-frequency-consistency----neurips-2022">#</a></h3>
<p><em><strong>Self-Supervised Contrastive Pre-Training for Time Series via Time-Frequency Consistency</strong></em></p>
<p>期望同一示例的基于<strong>时间</strong>和基于<strong>频率</strong>的表示在<strong>时频空间中靠近</strong>在一起</p>
<p>(判别式无监督学习)</p>
<p>pretext task：实例判别 (一对正样本，其余负样本)</p>
<p><em><strong>框架</strong></em></p>
<p><img alt="image-20240423132039482" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FiQwWmZiTaA80GpxZ_cP6LoWQBnD"></p>
<p>时域增强：基于时间特性从 xi 扩充，包括抖动、缩放、时移和邻域分段；</p>
<p>频域增强：频谱特征扰动 xFi 的增强，添加或删除频率分量来扰动频谱 (确保扰动时间序列仍然与原始样本在频域和时域仍相似)</p>
<p>Loss：</p>
<p><img alt="image-20240423133548048" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fv5GblVwR6sTQrzRmi3mkPCEiVeA"></p>
<p>余弦相似度：衡量两个向量之间相似性，范围[-1, 1]</p>
<p><strong>// 补充 2025/1/14</strong></p>
<img src="http://sthda9dn6.hd-bkt.clouddn.com/Fhfw9iENvupdYltnGP1TnIoanttM" alt="image-20250114163629763" style="zoom:40%;" />
<p>样本越相似  =&gt; sim(i, j) 越大 =&gt; exp(sim) 越大 =&gt;  exp(sim)/\sum(exp) 越接近于1 =&gt; log(·)越接近于0</p>
<p>-log(·) 将图像倒置，样本越<strong>不</strong>相似  =&gt; exp(sim)/\sum(exp) 越接近于0 =&gt; loss 越大</p>
<hr>
<h3 id="comet----neurips--2023">COMET  - NeurIPS  2023<a hidden class="anchor" aria-hidden="true" href="#comet----neurips--2023">#</a></h3>
<p><em><strong>Contrast Everything A Hierarchical Contrastive Framework for Medical Time-Series</strong></em></p>
<p>(判别式无监督学习)</p>
<ul>
<li>我们的方法旨在弥合标记数据的有限可用性与医疗时间序列分析中稳健且可概括的模型的需求之间的差距</li>
<li>对比表示学习背后的关键思想是通过将相似的数据靠近在一起并将不相似的数据进一步分开来挖掘数据一致性</li>
</ul>
<p>关键是要利用所有可用的信息；除了样本标签之外，数据集是否还有其他信息？(补充信息)</p>
<p><strong>患者间、患者内进行测试(定义打乱规则)</strong></p>
<p><strong>⭐⭐⭐多级信息</strong></p>
<p><img alt="image-20240424172452759" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FszIdIHqJYTLaVVMaDJ3SiLhuUQ4"></p>
<p><strong>多级对比学习框架</strong></p>
<p><img alt="image-20240424172549571" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FgCOlLyo72rghf4TuKsLoY_a4KtI"></p>
<p><em><strong>代码分析总结</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln">1</span><span class="cl"><span class="n">X_train</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="p">[</span><span class="n">Batch</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">segment</span><span class="p">]</span>  <span class="c1"># segment length 330, sampling_rate = 250Hz</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="n">y_train</span>  <span class="c1"># 心肌梗塞二分类标签， patient_id， segment_id</span>
</span></span></code></pre></div><p><em>细分为样本级别(心跳)，故图示Encoder不同级别对比学习可复用</em></p>
<p><strong>关键，计算不同级别的对比Loss</strong></p>
<p><em><strong>Patient-Level Loss</strong></em></p>
<p><img alt="image-20240424205635784" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FmojCp5VGez0QwxsX3dFyUHUCK__"></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">Batch</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">segment</span><span class="p">]</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl">
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"><span class="n">out1</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"><span class="n">out2</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># ? 模拟数据增强后的不同视角或版本，以便在对比学习中生成有效的正例和负例</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">
</span></span><span class="line"><span class="ln"> 6</span><span class="cl"><span class="c1"># 根据y_train 病人id，构建mask矩阵</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl"><span class="n">pid1</span><span class="p">,</span> <span class="n">pid2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">str_pid</span><span class="p">,</span> <span class="n">str_pid</span><span class="p">)</span>	
</span></span><span class="line"><span class="ln"> 8</span><span class="cl"><span class="n">pid_matrix</span> <span class="o">=</span> <span class="n">pid1</span> <span class="o">+</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="n">pid2</span>		<span class="c1">#  每项为 id_x - id_y	</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl"><span class="c1"># 目标位置 id_x - id_x</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl"><span class="n">pids_of_interest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">str_pid</span> <span class="o">+</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="n">str_pid</span><span class="p">)</span>  <span class="c1"># unique combinations of pids of interest i.e. matching </span>
</span></span><span class="line"><span class="ln">11</span><span class="cl"><span class="n">bool_matrix_of_interest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">str_pid</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">str_pid</span><span class="p">)))</span>
</span></span><span class="line"><span class="ln">12</span><span class="cl"><span class="k">for</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">pids_of_interest</span><span class="p">:</span>
</span></span><span class="line"><span class="ln">13</span><span class="cl">    <span class="n">bool_matrix_of_interest</span> <span class="o">+=</span> <span class="n">pid_matrix</span> <span class="o">==</span> <span class="n">pid</span>
</span></span><span class="line"><span class="ln">14</span><span class="cl">
</span></span><span class="line"><span class="ln">15</span><span class="cl"><span class="c1"># 上三角和下三角目标row，col</span>
</span></span><span class="line"><span class="ln">16</span><span class="cl"><span class="n">rows1</span><span class="p">,</span> <span class="n">cols1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">bool_matrix_of_interest</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>  <span class="c1"># upper triangle same patient combs</span>
</span></span><span class="line"><span class="ln">17</span><span class="cl"><span class="n">rows2</span><span class="p">,</span> <span class="n">cols2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">bool_matrix_of_interest</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># down triangle same patient combs</span>
</span></span><span class="line"><span class="ln">18</span><span class="cl">    
</span></span><span class="line"><span class="ln">19</span><span class="cl"><span class="n">out1</span><span class="p">,</span> <span class="n">out2</span>  <span class="o">=&gt;</span> <span class="n">sim_matrix_exp</span>  <span class="c1"># 余弦相似度矩阵</span>
</span></span><span class="line"><span class="ln">20</span><span class="cl">
</span></span><span class="line"><span class="ln">21</span><span class="cl"><span class="c1"># @@ 对比学习：不考虑对角线元素的原因主要是因为对角线元素表示的是特征向量与其自身的相似度</span>
</span></span><span class="line"><span class="ln">22</span><span class="cl"><span class="c1"># @@ 我们关注的是如何使具有相同标签的不同特征向量之间更相近，而使不同标签的特征向量之间更疏远</span>
</span></span><span class="line"><span class="ln">23</span><span class="cl">
</span></span><span class="line"><span class="ln">24</span><span class="cl"><span class="c1"># Loss 分母， 某样本与其他样本相似度之和，  @分上三角和下三角</span>
</span></span><span class="line"><span class="ln">25</span><span class="cl"><span class="n">triu_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sim_matrix_exp</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># add column</span>
</span></span><span class="line"><span class="ln">26</span><span class="cl"><span class="n">tril_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sim_matrix_exp</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># add row</span>
</span></span><span class="line"><span class="ln">27</span><span class="cl">
</span></span><span class="line"><span class="ln">28</span><span class="cl"><span class="n">loss_terms</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 取平均 </span>
</span></span><span class="line"><span class="ln">29</span><span class="cl"><span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">rows1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="ln">30</span><span class="cl">    <span class="n">triu_elements</span> <span class="o">=</span> <span class="n">sim_matrix_exp</span><span class="p">[</span><span class="n">rows1</span><span class="p">,</span> <span class="n">cols1</span><span class="p">]</span>  <span class="c1"># row and column for upper triangle same patient combinations</span>
</span></span><span class="line"><span class="ln">31</span><span class="cl">    <span class="n">loss_triu</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">triu_elements</span> <span class="o">/</span> <span class="n">triu_sum</span><span class="p">[</span><span class="n">rows1</span><span class="p">]))</span>
</span></span><span class="line"><span class="ln">32</span><span class="cl">    <span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_triu</span>  <span class="c1"># technically need to add 1 more term for symmetry</span>
</span></span><span class="line"><span class="ln">33</span><span class="cl">    <span class="n">loss_terms</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="ln">34</span><span class="cl"><span class="o">...</span>
</span></span><span class="line"><span class="ln">35</span><span class="cl">
</span></span><span class="line"><span class="ln">36</span><span class="cl"><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">/</span><span class="n">loss_terms</span>
</span></span></code></pre></div><p><em><strong>Trial-Level Loss</strong></em></p>
<p>同上，只不过patient_id =&gt; segment_id</p>
<p>❌❌❌<em>Sample-Level Loss &amp; Observation-Level Loss</em></p>
<p>看不懂源码~ 😎😭</p>
<hr>
<h3 id="ts2vec----aaai-22">TS2Vec  - AAAI 22<a hidden class="anchor" aria-hidden="true" href="#ts2vec----aaai-22">#</a></h3>
<p><em><strong>TS2Vec: Towards Universal Representation of Time Series</strong></em></p>
<p>(判别式无监督学习)</p>
<p>现存工作局限：</p>
<ul>
<li>它们都没有以不同尺度的时间序列为特征来捕获尺度不变的信息，而这对于时间序列任务的成功至关重要。多尺度特征可以提供不同级别的语义并提高学习表示的泛化能力</li>
<li>粗粒度表示 - 整个时间序列，可能没那么细致</li>
</ul>
<p>之前工作的问题</p>
<p><img alt="image-20240425195237484" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fl7FFxGPcEP-7YFmsNf9PYHpZVDj"></p>
<p>正样本对会误判</p>
<ul>
<li>当存在水平偏移时，子系列一致性很容易受到攻击 (左图)</li>
<li>当出现异常时，时间一致性可能会引入误报对 (右图)</li>
</ul>
<p>创新：</p>
<ul>
<li>TS2Vec 中的对比目标基于<strong>增强上下文视图</strong>，即相同子系列在两个增强上下文中的表示应该是一致的 <strong>(上下文语境下)</strong></li>
<li>层级式对比Loss，由细粒度=&gt;粗粒度，局部到全局  # 学习各种语义级别的任意子系列的上下文表示，灵活且通用的表示。 顶级语义级别的对比使模型能够学习实例(样本)级表示</li>
</ul>
<p><em><strong>框架</strong></em></p>
<p><img alt="image-20240425171329665" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FmF0LTUTFGsYXsZSjO1Es3CsVaFK"></p>
<p>流程：</p>
<ul>
<li>实例(一段信号)，分两个重叠的子序列,  [batch, channel, dim_feature]</li>
<li>投影，[batch, channel, dim_feature] =&gt; [batch, channel, dim_hidden]</li>
<li>随机Mask掉部分信号</li>
<li>CNN-Encoder</li>
</ul>
<p>⭐⭐⭐  Hierarchical Contrasting Loss</p>
<ul>
<li>
<p><em><strong>instance &amp; temporal contrastive loss</strong></em></p>
<pre><code>  ![image-20240425172445164](http://sthda9dn6.hd-bkt.clouddn.com/FjeDH748BsrYQtTOEOmSjYW_SEMr)

  *用 -F.log_softmax(x, dim=-1) 实现*
</code></pre>
</li>
<li>
<p>z1 = F.max_pool1d(z1.transpose(1, 2), kernel_size=2).transpose(1, 2)
z2 = F.max_pool1d(z2.transpose(1, 2), kernel_size=2).transpose(1, 2)</p>
<p>编码特征浓缩，<strong>多尺度</strong>的Contrast loss</p>
</li>
</ul>
<p><em><strong>图示Loss过程</strong></em></p>
<p><img alt="image-20240425193417634" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fu8OeKZvY8TQ-hCVmTs7laVuLNnM"></p>
<p>子序列是从原始信号中裁剪下来的，并且<strong>有重叠部分</strong></p>
<p><img alt="image-20240425175053049" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fk0G8jM8ufq4FezR7WIn1MKEPHow"></p>
<hr>
<h3 id="timesurl----aaai-2024">TimesURL  - AAAI 2024<a hidden class="anchor" aria-hidden="true" href="#timesurl----aaai-2024">#</a></h3>
<p><em><strong>TimesURL: Self-Supervised Contrastive Learning for Universal Time Series Representation Learning</strong></em></p>
<p>代码在TS2Vec上修改</p>
<p>=&gt; 在这里，我们必须提到，重要的时间变化信息，例如趋势和季节，在多次最大池化操作后会丢失，因此顶层对比实际上无法为下游任务捕获足够的实例级信息</p>
<p>=&gt; 掩码重建进行学习实例级信息</p>
<p>(判别式+生成式 混合 无监督学习)</p>
<p><em>观察</em></p>
<p><img alt="image-20240427213825901" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FlxcLu-nrG0hZ5m6BeQMnfJk4BjQ"></p>
<p>**简单负样本：**大多数时间序列片段可以被视为容易负样本。 这些片段往往表现出与锚点的语义差异，并且仅贡献较小的梯度，因此无法提供有用的区分信息</p>
<p><strong>硬负样本：</strong> 硬负样本就是离正样本很近，并且模型很难区别的</p>
<p>正样本-<strong>硬负样本</strong>-负样本：，这些样本如果让其远离正样本可以大大提高模型性能。 其有效性被大量的简单负样本所掩盖。（现存框架没有特点显示的指出）</p>
<p>由于时间序列中的局部平滑性和马尔可夫特性，大多数负样本很容易不足以捕获时间信息，因为它们从根本上缺乏驱动对比学习所需的学习信号。 作为图 2 中真实示例，对于每个正锚点（红色方块），相应的负样本（灰色标记）包含许多简单的负样本和很少的困难负样本，即许多负片太远，无法造成对比损失。</p>
<p>对比学习的一个关键组成部分是选择适当的增强，这些增强可以施加一些先验来构建可行的正样本，以便编码器可以被训练来学习鲁棒和有区别的表示</p>
<p><strong>频率混合</strong>用于通过将通过快速傅里叶变换（FFT）运算计算出的一个训练实例 xi 中的一定比例的频率分量替换为同一批次中的另一个随机训练实例 xk 的相同频率分量来生成新的上下文视图 （保持病理相同）</p>
<p><strong>随机裁剪</strong>。 <em><strong>重叠两个子序列</strong></em> - 随机裁剪是<strong>上下文一致性策略</strong>的关键步骤。 它可以保持时间序列的重要时间关系和语义一致性</p>
<p><em><strong>创新点：</strong></em></p>
<ul>
<li>提出双Universum概念，就是利用Mix-up增强F(x)，追加硬负样本。</li>
<li>对比学习+自监督掩码重建，联合优化，来捕获段级和实例级信息， 实现通用表示</li>
</ul>
<p>⭐ 第一类包括预测、异常检测和插补，它们更多地依赖于在分段级别捕获的细粒度信息，因为这些任务需要推断特定的时间戳或子序列。细粒度(局部)</p>
<p>⭐ 第二类包括分类和聚类优先考虑实例级信息（即粗粒度信息），旨在推断整个系列的目标。粗粒度(全局)</p>
<p><em style="color:red; font-weight:bolder">实现 : 分段(对比学习，学习片段)，整句(掩码重建，学习整体)</em></p>
<p><em><strong>框架：</strong></em></p>
<p><img alt="image-20240427213747206" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FiK4E0Rba4I2lpP2fIpxQ8rFyuFb"></p>
<p><em><strong>AUG：</strong></em></p>
<pre><code>频率增强，**x =&gt; fft() =mask+fusion=&gt; ifft() =&gt; y**，①该篇论文fusion是融合同batch其他信号的某些部分 =&gt; 领域创新 （fusion的信号应该和这个信号相同病理，扩张数据分布）
</code></pre>
<p><em><strong>DualConv：</strong></em></p>
<pre><code>原始信号的两个增强子视图，z1 = Encoder(x1),  	z1' = Encoder(x1'),  =&gt; mix-up option =&gt; **z1_mix = α × z1 + (1-α) × z1[torch.randperm(z1.shape[0])]**，z1'_mix。  

[z1, z1', z1_mix, z1'mix] @ [z1, z1', z1_mix, z1'mix].T   =&gt; sim =&gt; **-F.log_softmax(sim)**

loss:  俩视图对应段为正样本(分子)
</code></pre>
<p><img alt="image-20240427220130014" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FqUjTPyOQ_WkWJxxkJ0GpzCsSAAB"></p>
<p>负样本在母分，x_mix做负样本增强。</p>
<hr>
<h3 id="mixup----2017-machine-learning">Mixup  - 2017 Machine Learning<a hidden class="anchor" aria-hidden="true" href="#mixup----2017-machine-learning">#</a></h3>
<p><em><strong>mixup: BEYOND EMPIRICAL RISK MINIMIZATION</strong></em></p>
<p>一种简单并且不需要专业领域知识的数据增强</p>
<p>现存问题讨论：</p>
<ul>
<li>过拟合(大模型，直接记忆Train data，走捷径)                                          - overfitting</li>
<li>精心设计样本(对抗性例子，人难以察觉，但模型会给出错误的答案)      - generalize</li>
</ul>
<p>👇</p>
<p><strong>ERM</strong>(经验风险最小化原则)问题</p>
<ul>
<li>一方面，即使存在强正则化，ERM 也允许大型神经网络记忆（而不是归纳）训练数据</li>
<li>另一方面，使用 ERM 训练的神经网络在对训练分布之外的示例进行评估时会极大地改变其预测。</li>
</ul>
<p>CV: 图像的邻近区域定义为其水平反射、轻微旋转和轻微缩放的集合</p>
<p>=&gt;  数据增强始终可以提高泛化能力, 但该过程依赖于数据集，因此需要使用专家知识 （不同领域增强不一定通用）</p>
<p>数据增强假设邻近的示例共享同一类，并且不会对不同类的示例之间的邻近关系进行建模。(聚类)</p>
<p>=&gt; 邻近风险最小化 (<strong>VRM</strong>) 原则</p>
<p>⭐=&gt; 从训练样例的邻近分布中提取额外的虚拟样例，以扩大训练分布的支持度</p>
<p><em><strong>方法：</strong></em></p>
<p><img alt="image-20240428175535852" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FqaLaOdoIr-cpS9l6zmtNSzR-Vqq"></p>
<p><strong><em>框架伪代码</em>：</strong></p>
<p><img alt="image-20240428175500700" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FpPy-mOfvDy30xYu8K1sC1Q5zOAp"></p>
<p><em>图例：</em></p>
<p><img alt="image-20240428175951150" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fg1UhIVdFoUyTZUNKCNOy5NVzP-o"></p>
<p>虚拟数据，让数据边界过渡； 当不在Train数据的分布出现时，降低不确定性。 稍微清晰化边界</p>
<hr>
<h3 id="yolo-v1----cvpr-2016">YOLO-v1  - CVPR 2016<a hidden class="anchor" aria-hidden="true" href="#yolo-v1----cvpr-2016">#</a></h3>
<p><em><strong>You Only Look Once: Unified, Real-Time Object Detection</strong></em></p>
<p>⭐将目标检测视作回归问题  =&gt; 预测出来</p>
<p><em><strong>前向推理</strong></em></p>
<p><em>Model:</em></p>
<p><img alt="image-20240429203859825" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FhxSE95ivo_u62hu7k5pAhAamgQZ"></p>
<p><em>input: [3, 448, 448]  =&gt; output: [30, 7, 7]</em></p>
<p><img alt="image-20240429204129869" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FlwDkypmqQ3dpDIVV3Dvc4rqnrcd"></p>
<p><em>置信度(confidence): 这个值代表了模型认为预测的边界框内存在对象的概率</em></p>
<p><em>框的中心坐标 + 宽高</em></p>
<p><em>框中的物体是什么类</em></p>
<p><img alt="image-20240429210124912" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FtrEL1xVvt9z-KAIvxuRVYLmLS_J"></p>
<p><em><strong>非极大值抑制</strong></em>  （最佳：每个类别独立执行非极大值抑制，从而更精确地处理多类别情况）</p>
<ol>
<li><strong>置信度排序</strong>：首先将所有的预测边界框按照它们的置信度（confidence scores）进行降序排序。</li>
<li><strong>选择最高置信度边界框</strong>：从排序后的列表中选择置信度最高的边界框作为参考框（reference box）。</li>
<li><strong>计算IOU</strong>：计算选中的参考框与列表中其他所有边界框的交并比（IOU）。交并比是两个边界框的交集面积与它们的并集面积的比值。</li>
<li><strong>抑制</strong>：如果参考框与任何其他边界框的IOU超过预先设定的阈值（通常设置为0.5），那么这些边界框会被认为是多余的，并从列表中删除。</li>
<li><strong>重复步骤</strong>：从剩余的边界框列表中再次选择置信度最高的边界框，重复上述过程，直到所有的边界框都被处理完毕。</li>
<li><strong>最终结果</strong>：经过非极大值抑制后，剩余的边界框被认为是对目标位置的最佳预测，它们将被用于最终的目标检测输出。</li>
</ol>
<p><em><strong>训练：</strong></em></p>
<p><img alt="image-20240429210707202" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FoYarO_WymLCMWJzy9ShWN6l4FAd"></p>
<p>λcoord = 5,   λnoobj = 0.5,  调整各个部分的重要性
$$
1_{ij}^{obj}: 表示第ij个格子有对象 \
1_{ij}^{noobj}: 表示第ij个格子没有对象\
S^{2}: 图片划分格子 \
B: 每个格子预测多少个框
$$
<em>bounding box loss : 中心点 + 框宽高</em></p>
<p><em>confidence: 格子是否有对象</em></p>
<p><em>classes：格子分类是否正确</em></p>
<hr>
<h3 id="semi-supervised-hybrid-loss-----machine-learning-2023">Semi-Supervised Hybrid Loss  -  Machine Learning 2023<a hidden class="anchor" aria-hidden="true" href="#semi-supervised-hybrid-loss-----machine-learning-2023">#</a></h3>
<p><em><strong>Semi-Supervised End-To-End Contrastive Learning For Time Series Classification</strong></em></p>
<p>1️⃣无标签数据对比学习(增强视图一致性)，2️⃣有标签对比学习(相同种类一致性)，3️⃣有标签分类监督学习</p>
<p>(判别式无监督学习 + 有监督学习)</p>
<p><strong>框架对比</strong></p>
<p><img alt="image-20240504141408686" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FvRDJcCTiZLB2TSscRE047d6YZYv"></p>
<p>⭐ <em><strong>End to End</strong></em></p>
<p><strong>框架</strong></p>
<p><img alt="image-20240504141522823" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FntDu8y-SPWRFtWIEWiahhJImAax"></p>
<p>Unlabeled Sample : 使用两个增强视图作为positive pair，与其他sample为negative pair (标准的对比学习)</p>
<p>Labeled Sample：1️⃣ 同类型的sample为positive pair，不同类型的sample为negative pair.  2️⃣过分类头，计算分类Loss</p>
<p>❤️ 混合上述三个Loss，联合优化<strong>Encoder</strong></p>
<hr>
<h3 id="simclr----2020">SimCLR  - 2020<a hidden class="anchor" aria-hidden="true" href="#simclr----2020">#</a></h3>
<p><em><strong>A Simple Framework for Contrastive Learning of Visual Representations</strong></em></p>
<p><em>贡献：</em></p>
<ul>
<li>
<p><strong>数据增强</strong>对于对比学习至关重要 (裁剪缩放，翻转，颜色紊乱，旋转， 掩盖， 高斯噪声， 高斯模糊，Sobel 滤波)  - 裁剪缩放+颜色紊乱 比较好</p>
</li>
<li>
<p>在经过Resnet编码器后，追加MLP能增强模型性能</p>
</li>
<li>
<p>样本x，增强视图xi和xj(正样本)，batch size =N，一共2N的增强视图，对于某个样本x，xi和xj为正样本，和batch中剩余的样本的增强为负样本</p>
<p>(大batchsize, 性能更好， 全局BN)</p>
</li>
</ul>
<p><em><strong>样本自成一类</strong></em>，来尽可能地让编码器找到图像中最重要的特征</p>
<p><em>框架</em></p>
<p><img alt="image-20240504192858744" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FtXMJSSYglENTuecQD7bhMyV1xVo"></p>
<p>共享参数 shared weight</p>
<p><em>Loss</em></p>
<p><img alt="image-20240504192936596" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FnYcuZnuiDcvvR3ajLOAq4DShL2R"></p>
<p>上面是正样本对，下面是负样本对
-log_softmax() =&gt; 挑选出需要的值</p>
<p><em>算法</em></p>
<p><img alt="image-20240504193222346" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FtXUBR7a_alxoHOMOf_A6CuFi92w"></p>
<hr>
<h3 id="vilt----2021">ViLT  - 2021<a hidden class="anchor" aria-hidden="true" href="#vilt----2021">#</a></h3>
<p><em><strong>ViLT Vision-and-Language Transformer Without Convolution or Region Supervision</strong></em></p>
<p>极简结构的图片文多模态融合</p>
<p>速度限制-问题分析</p>
<p><img alt="image-20240506202622982" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FophrEolg_pY22h3f06TApCOn7vU"></p>
<p>归纳总结：</p>
<p><img alt="image-20240506201201840" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Ft9OjzVqwvKABcvbEdV0-NdHWT3y"></p>
<p>(a) vision embedding 参数量 &gt; Text Embedding &gt; Modality Interaction  # 缺点，视觉嵌入太重(比重太大)，并且融合非常简单即点乘算相似度</p>
<p>(b) vision embedding和Text embedding 占比差不多  &gt; Modality Interaction   # 模态融合之前，工作太繁杂，而且前抽取特征不好，限制后面融合，并且不重视后面的模态融合操作。</p>
<p>(c) 重视visual  Embed和后期的modality interaction，# text 和 vision不均等，重要性不平衡</p>
<p>=&gt; 简单框架，Text词嵌入(bert中的BertEmbeddings加载训练后的权重)，vision用patch projection，都很快</p>
<p>⭐ 1. 图像和文本前期嵌入应该有相似均匀的表达能力 2. 这两种模态是否在深层网络中相互作用。</p>
<p>模型框架：</p>
<p><img alt="image-20240506202930648" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fk4qZVe2Ed-ezQCm2RoOZuE3J0Ha"></p>
<p><em>初始化参数-ViT，而不是bert</em></p>
<p>优化目标(主要)：</p>
<ul>
<li>Image Text Matching：0.5概率将图片替换为与文本不匹配的图片，预测一致性(二分类问题)</li>
<li>Masked Language Modeling：预测被遮掩的词</li>
</ul>
<p>text cls: 预测图文是否一致，二分类</p>
<p>text token set： 全局上下文=&gt;预被掩词</p>
<p>text token set 和 visual token set：进行对齐Loss</p>
<hr>
<h3 id="beit-v3-2022">BEIT-v3 2022<a hidden class="anchor" aria-hidden="true" href="#beit-v3-2022">#</a></h3>
<p><em><strong>Image as a Foreign Language: BEIT Pretraining for All Vision and Vision-Language Tasks</strong></em></p>
<p>统一Vision 和 NLP</p>
<p>⭐ 核心思想是图像可以被建模为一门外语，这样我们就可以对图像、文本和图文对进行统一的掩码“语言”建模。</p>
<p><strong>结果非常好</strong></p>
<p><img alt="image-20240506221847766" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FvdT24-GqGPwRnm3KHtiyZK8rTtO"></p>
<p><em>基础块</em></p>
<p><img alt="image-20240506221730469" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FlsZ16a9K6-0aI8vE7dcRGRmqc4V"></p>
<p>共享注意力矩阵(都是一个物体的不同视角)，但是最后的FFN各个模态专享</p>
<p><em>拓展到不同的模态</em>：</p>
<p><img alt="image-20240506221937618" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FvuKOj42D1UKUyfgc9DhAB_kHSUm"></p>
<p><em>任务</em>：</p>
<p><strong>图像字幕任务</strong>：采用了特殊的自注意力掩模。 <em><strong>图像标记（即图像块）只能在图像序列内双向相互关注</strong></em>。 <em><strong>标题的标记可以关注图像标记、它们的左侧标题标记以及它们本身</strong></em>。 在微调过程中，我们随机屏蔽一定比例的标题标记。 该模型经过训练，可以根据图像的线索及其左侧标题上下文来恢复这些标记。 我们还屏蔽了特殊的边界标记 [SEP]，以帮助模型学习终止生成</p>
<p><strong>视觉问答：</strong> 将任务表述为分类问题。 该模型经过训练，可以从训练集中 3129 个最常见的候选答案中预测答案。我们将给定问题和图像的嵌入连接起来，然后将输入嵌入输入多路转换器以联合编码图像-问题对。 最终的池化输出被输入到分类器层来预测答案。</p>
<p>**图像文本检索任务：**是测量图像和文本之间的相似度。 根据检索目标的模态，有两个方向：图像到文本检索和文本到图像检索。 <em>双编码器模型</em>分别对图像和文本进行编码以获得它们的表示。 然后我们计算这些表示的余弦相似度分数。</p>
<p><strong>图像分类：</strong> 将该任务制定为图像到文本检索任务。 我们使用类别名称作为文本来构建图像-文本对。 BEIT-3 被训练为双编码器，以找到图像最相关的标签。 在推理过程中，我们首先计算可能的类名的特征嵌入和图像的特征嵌入。 然后计算它们的余弦相似度分数以预测每个图像最可能的标签。</p>
<hr>
<h3 id="albef----2021">ALBEF  - 2021<a hidden class="anchor" aria-hidden="true" href="#albef----2021">#</a></h3>
<p><em><strong>Align before Fuse</strong>: Vision and Language Representation Learning with Momentum Distillation</em></p>
<p>现存问题：大多数现有方法采用基于变压器的多模态编码器来联合建模-视觉Token（基于区域的图像特征）和文本Token。 <strong>由于视觉标记和单词标记未对齐</strong>，因此多模态编码器学习图像文本交互具有挑战性。</p>
<p>（1）图像特征和文本符号映射仍然停留在他们自己的空间，使得多模态编码器很难学习建模他们之间的交互；</p>
<p>（2）物体检测器 &mdash; 标注费钱，使用费算力 &mdash; 在预训练阶段需要标注矩形框，在推理阶段高分辨率图像，如 600*1000，速度较慢；</p>
<p>（3）广泛使用的 image-text 数据集均是从网上搜集的带有严重噪声的数据，现有的预训练目标，如 MLM 可能过拟合到文本数据，降低了模型的泛化性能。</p>
<p><em>框架：</em></p>
<p><img alt="image-20240507195518743" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fm8rrHfMoDMFD44WkrXWbLfBZWsO"></p>
<p>image encoder: ViT(ImageNet预训练参数)  - CLS token</p>
<p>text encoder: Bert(预训练参数)  - CLS token</p>
<p>multimodal encoder: Bert(预训练参数) + cross-attention</p>
<p>⭐ **在传入multi-modal encoder前，使用ITC迫使模型进行对齐 ** <em>align before fuse的align</em></p>
<ul>
<li>对齐图像特征和文本特征，使多模态融合编码器更容易执行跨模态学习</li>
<li>改进了单模态编码器，以更好地理解图像和文本的语义</li>
<li>它学习一个共同的低维空间来嵌入图像和文本，这使得图像文本匹配目标能够通过我们的对比硬负挖掘找到更多信息样本。</li>
</ul>
<p><strong>Image-Text Contrastive Loss：</strong></p>
<p>正样本对：配对的Image-Text</p>
<p>负样本对：Queue存储着的样本表示</p>
<p>Image =&gt; 匹配Text-Queue(Momentum)</p>
<p>Text =&gt; 匹配Image-Queue(Momentum)</p>
<p>(代码中利用了Momentum Distillation &hellip;)</p>
<p><strong>Image-Text Matching:</strong></p>
<p>有了multi-modal encoder输出的 embed token (正确样本的表征) =&gt; 即喂入Multi-modal encoder的 text embed = (positive), Image embed = (positive), 拿<strong>融合的CLS</strong>作为最终表征，过MLP =&gt; 二分类预测是否匹配；这部分Targets=(1, 1, &hellip;, 1)</p>
<p>从同batch中，按相似性大小随机挑选一个(hard)负样本，</p>
<p>然后，text embed = (positive, &hellip;, negetive, &hellip;) , Image embed = (negetive, &hellip;, positive)</p>
<p>multi-modal encoder =&gt; CLS =&gt; MLP =&gt; two probability</p>
<p>Targets = (0, 0, &hellip;, 0)</p>
<p>// 重新梳理如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln">1</span><span class="cl"><span class="c1"># 图像i-文本j =&gt; 多模态编码器 =&gt; 是否匹配；</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="n">图像</span> <span class="mi">1</span><span class="o">-</span><span class="n">文本</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">匹配对</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="n">图像</span> <span class="mi">2</span><span class="o">-</span><span class="n">文本</span> <span class="mi">2</span> <span class="p">:</span> <span class="n">匹配对</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl"><span class="n">图像</span> <span class="mi">1</span><span class="o">-</span><span class="n">文本</span> <span class="mi">2</span> <span class="p">:</span> <span class="n">不匹配</span>  <span class="o">//</span> <span class="n">这里使用余弦相似度选取最困难的样本</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl"><span class="n">图像</span> <span class="mi">2</span><span class="o">-</span><span class="n">文本</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">不匹配</span>  <span class="o">//</span> <span class="n">这里使用余弦相似度选取最困难的样本</span>
</span></span></code></pre></div><p><strong>Masked Language Modeling</strong>:</p>
<p>屏蔽掉一些词，通过从图片模态信息中预测掉被屏蔽的词(多分类Loss)</p>
<p>这里也借助了图像的信息去更好的恢复被mask掉的单词</p>
<p>【这里只对匹配的对计算 掩码Loss】</p>
<p><img alt="image-20240507203423762" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fo5QVCCNEHnTtRVHiD9VTTtG2NhB"></p>
<p>目的：缓解noisy web data的不足，真正的label不一定有momentum的好</p>
<p><img alt="image-20250115224719284" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FlfwuaqgM0F6HxRI8NLy9yX0qlBH"></p>
<p>真实label不一定比momentum model给出的predict label好，=&gt; 使用KL散度进行约束 一致性</p>
<p><strong>最大化互信息视角解释：</strong></p>
<p>在自监督学习中，a 和 b 是同一图像的两个增强。 在视觉语言表示学习中，我们将 a 和 b 视为捕获其语义的图像文本对的不同变体。 我们的目标是学习对观点变化不变的表征。</p>
<p>最小化Loss =&gt; 最大化互信息的下限(最大化了图像-文本对的**不同“视图”**之间的互信息（MI）的下限)</p>
<p>InfoNCE Loss</p>
<p><img alt="image-20240507201135974" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fn6k4nYEeCC98gdG9m20Ny9q11Kt"></p>
<p><em><strong>Image-Text Contrastive Loss</strong></em></p>
<p><img alt="image-20240507201202181" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Foi0D0DMVUtxzsHTL76RXZKMh3jo"></p>
<p>最大化Text和Image中的互信息， ITC 将两个单独的模态（即 I 和 T）视为图像-文本对的两个视图</p>
<p><em><strong>MLM:</strong></em></p>
<p><img alt="image-20240507201419353" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FhfVA9APbraPAhcN-2iDpbkpMlSA"></p>
<p>MLM 将图像-文本对的两个视图视为：(1) 随机选择的单词标记，以及 (2) 图像 + 带有该单词屏蔽的上下文文本。</p>
<hr>
<h3 id="instance-discrimination----2018">Instance discrimination  - 2018<a hidden class="anchor" aria-hidden="true" href="#instance-discrimination----2018">#</a></h3>
<p><em><strong>Unsupervised Feature Learning via Non-Parametric Instance Discrimination</strong></em></p>
<p>首次提出个体判别任务！</p>
<p>观察：</p>
<p><img alt="image-20240508125442186" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FvT1Zpo3nQTgy2ZEOa9t67cXuVDQ"></p>
<p>在监督学习中。在预测&rsquo;花豹&rsquo;时，预测概率除了&rsquo;花豹&rsquo;，<strong>剩余</strong>预测得分比较高的是&rsquo;美洲虎&rsquo;、&lsquo;猎豹&rsquo;； <strong>最不相似</strong>的是&rsquo;救生艇&rsquo;、&lsquo;购物车&rsquo;、&lsquo;书柜&rsquo;;</p>
<p><em><strong>最高响应的类都是视觉相关的</strong></em></p>
<p>⭐ 并不是语义标签，<strong>而是数据本身的明显相似性使某些类比其他类更接近</strong>；</p>
<p>❗❗❗ <strong>个体判别：将类监督发挥到了极致，并学习了区分各个实例的特征表示。</strong></p>
<p>这些观察结果表明，典型的判别学习方法可以自动<strong>发现语义类别之间的明显相似性，而无需明确指导这样做</strong>。</p>
<p>我们能否通过纯粹的判别学习来学习反映实例之间明显相似性的有意义的度量？ <strong>图像本身就是独特的，并且每个图像都可能与同一语义类别中的其他图像显着不同</strong>。如果我们学会<strong>在没有任何语义类别概念的情况下区分各个实例，我们最终可能会得到一个捕获实例之间明显相似性的表示，就像类明智的监督学习如何仍然保留类之间的明显相似性一样</strong>。</p>
<p><strong>目标</strong>:</p>
<pre><code>在没有监督的情况下学习嵌入函数 v = fθ(x)。 fθ 是一个具有参数 θ 的深度神经网络，将图像 x 映射到特征 v。这种嵌入将在图像空间上产生一个度量，对于实例 x 和 y, dθ(x, y) = |fθ(x) − fθ(y)|。 良好的嵌入应该将视觉上相似的图像映射得彼此更接近。 我们新颖的无监督特征学习方法是实例级区分。 我们将每个图像实例视为其自己的不同类，并训练分类器来区分各个实例类。
</code></pre>
<p><strong>方法：</strong></p>
<p><img alt="image-20240508130437824" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FgTn1HJ9-x4YmL1t8ikulVrn2ZpY"></p>
<p>用一个memory bank存储4096个样本embed feature(128-dimention) 随着网络更新, 目的是让特征在嵌入空间中远离(每一个样本都是一个类)，学习那种有监督时类和类之间相似聚集的现象。</p>
<hr>
<h3 id="byol----20206">BYOL  - 2020/6<a hidden class="anchor" aria-hidden="true" href="#byol----20206">#</a></h3>
<p><em><strong>Bootstrap Your Own Latent A New Approach to Self-Supervised Learning</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln">1</span><span class="cl"><span class="n">无监督学习</span><span class="err">：</span> <span class="p">{</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl">    <span class="n">判别式</span><span class="err">：</span><span class="n">从增强视图的表示中</span><span class="err">，</span><span class="n">他们学会区分同一图像的另一个增强视图的表示和不同图像的增强视图的表示</span> <span class="o">=&gt;</span> <span class="n">这种判别方法通常需要将增强视图的每个表示与许多反例进行比较</span><span class="err">。</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl">    <span class="n">生成式</span><span class="err">：</span><span class="n">通过预测同一图像的不同视图</span><span class="err">（</span><span class="n">例如</span><span class="err">，</span><span class="n">不同的随机裁剪</span><span class="err">）</span><span class="n">来学习表示</span> <span class="o">=&gt;</span> <span class="n">图像的增强视图的表示应该能够预测同一图像的另一个增强视图的表示</span><span class="err">。</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p><em><strong>方法：</strong></em></p>
<p><img alt="image-20240508172636506" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fgnq_P3obfLx4Ys4EVFMcEvtdv14"></p>
<p>在线网络θ + 目标网络γ(提供回归目标，γ = α×γ + (1-α)×θ ，指数移动平均 )</p>
<p><strong>yθ</strong>是目标编码器，其余的训练好后丢掉</p>
<p>⭐一张图片的两个增强表示的相同的语义 =&gt; 在高维的嵌入表示中，应该可以预测对方(相近)</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln">1</span><span class="cl"><span class="n">yθ</span><span class="err">：</span><span class="n">Encoder</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="n">zθ</span><span class="err">：</span><span class="n">Projection</span> <span class="n">head</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="n">qθ</span><span class="err">：</span><span class="n">Prediction</span> <span class="n">head</span>
</span></span></code></pre></div><p><em><strong>Loss:</strong></em>
$$
注意图像增强t(x)和t^{<code>}x会对等的传给online\ net 和 target\ net\\ Loss: 1/2 × ( || q_{θ}(z_{θ}) - z^{</code>}<em>{ξ}||^{2}  + || q</em>{θ}(z_{θ}) - z^{`}_{ξ}||^{2}  )
$$
<em><strong>Train:</strong></em>
$$
θ：online\ net\ parameters\
ξ：target\ net\ parameters\
θ &lt;- optimizer(θ),\ \ \ ξ &lt;- αξ + (1-α)θ
$$</p>
<hr>
<h3 id="dino---2021">DINO - 2021<a hidden class="anchor" aria-hidden="true" href="#dino---2021">#</a></h3>
<p><em><strong>Emerging Properties in Self-Supervised Vision Transformers</strong></em></p>
<p><img alt="image-20240508213644391" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FgmNTLztdwi5Fz9jueQRIveYY2PT"></p>
<p>ViT最后的CLS注意力图示⭐⭐⭐</p>
<p>探讨：<em>质疑自监督学习是否为 Vision Transformer (ViT) 提供了比卷积网络 (convnets) 更突出的新属性？</em></p>
<p>框架：(借鉴BYOL)</p>
<p><img alt="image-20240508213533118" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FoMugd-rgr0cSAJqhDni4V7Xx0mV"></p>
<p>教师是在训练过程中动态构建的。知识蒸馏就不再被用作自监督预训练的后处理步骤，而是直接作为自监督目标。</p>
<p>其中学生和教师具有相同的架构并在训练期间使用蒸馏。</p>
<p>教师在我们工作中用学生的动量平均值进行更新。</p>
<p>增强策略：</p>
<pre><code>1. 多裁剪策略构建图像的不同扭曲视图或裁剪。 更准确地说，根据给定的图像，我们生成一组 V 的不同视图。 该集合包含两个全局视图 xg 1 和 xg 2 以及几个分辨率较小的局部视图。
1.  所有的裁剪都通过学生传递，而只有全局观点通过老师传递，因此鼓励“局部到全局”的对应。
</code></pre>
<p>伪代码：</p>
<p><img alt="image-20240508213511774" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FpwbfqXQctKIAjN6nzEdScMiH1Tv"></p>
<p>防止模型坍塌：</p>
<pre><code>1. *对动量教师输出进行居中和锐化，以避免模型崩溃。*
2. **居中（Centering）**：对动量教师的输出进行居中操作是为了减少批次之间的偏差，增加输出的稳定性。具体做法是从每个输出中减去其均值，确保输出围绕零分布，这有助于避免网络输出在特征空间内偏向某一方向，从而降低了模型坍塌的风险。
3. **锐化（Sharpening）**：锐化是通过增加输出分布的峰值来实现的，目的是使模型的输出更加区分明显，即使不同类别之间的区别更加清晰。这通常通过提高输出概率分布的熵来实现，比如可以采用温度调整（temperature scaling）等方法来调整概率分布，使得主要的概率值更加突出，而其他的概率值则相对降低。
</code></pre>
<p>图示：</p>
<p><img alt="image-20240508215946714" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fm-JcH7RKbGtr4JmV6THi9pBZpzJ"></p>
<p><em>不同颜色是不同的注意力头</em></p>
<p><img alt="image-20240508220018510" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FpiUx2RoGk6ES3YqU1cacRQmhQug"></p>
<p>无监督注意力更能学到本质！</p>
<hr>
<h3 id="simsiam---cvpr-2021">SimSiam - CVPR 2021<a hidden class="anchor" aria-hidden="true" href="#simsiam---cvpr-2021">#</a></h3>
<p><em><strong>Exploring Simple Siamese Representation Learning</strong></em></p>
<p>简单设计的Siamese(孪生)网络。 我们的极简主义方法的竞争力表明</p>
<p><img alt="image-20240509130843870" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fuh4i3VZlmz43UK5fCBeVdASTTui"></p>
<p>1️⃣ “没有动量编码器的 BYOL”</p>
<p>2️⃣ “没有负样本的 SimCLR“ + stop-grad(⭐这个非常关键， 这个对防止模型坍塌很关键)</p>
<p><em>方法：</em></p>
<p><img alt="image-20240509131933070" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fjpwyna05L85B-6wGDYZmz_Qw0MA"></p>
<p>一幅图像的两个增强视图由同一编码器网络 f（主干网络加投影 MLP）处理。 然后在一侧应用预测 MLP-h，在另一侧应用停止梯度操作。 该模型最大化了双方之间的相似性。 它既不使用负对也不使用动量编码器。</p>
<p><em>伪代码：</em></p>
<p><img alt="image-20240509132032286" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fn0nqir1DgxbNEJEZxTwwyAticNl"></p>
<p><em style="font-weight:bold; font-size: 24px">消融</em></p>
<p><em><strong>Loss:</strong></em></p>
<p><em>负的余弦相似度 和 交叉熵</em></p>
<p>相似度：<img alt="image-20240509132907059" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fvq2llOYgXiD8my85xmHV4Qfciy2"></p>
<p>交叉熵：<img alt="image-20240509132841007" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fo-Wyh7xSpg3IJLeTKwtQKrrnJg0"></p>
<p><img alt="image-20240509132922553" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fi1FIOIJnexoaKUFVFunkW1ACLD0"></p>
<p><img alt="image-20240509132830913" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FqfhYzetpsM5M-7uRQ67CWI2eeol"></p>
<p><em><strong>BatchNorm的影响：</strong></em></p>
<p><img alt="image-20240509132146667" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fovin_S7qt5lFb027We7eEvPRQPd"></p>
<p><em><strong>BatchSize的影响：</strong></em></p>
<p><img alt="image-20240509132226252" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FohojIt8WTiIjRV1Pg1dgQVdX22J"></p>
<p><em><strong>预测头的影响：</strong></em></p>
<p><img alt="image-20240509132304829" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fr8ykXKTMCL8sJr1AioPfrb10cgi"></p>
<p><em><strong>Loss的对称性：</strong></em></p>
<p><img alt="image-20240509132550886" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fn0qB0BgCd8qAYGtw508QP7TYPGY"></p>
<p>sym对称；asym非对称；asym. 2×(每个图像采样两对来粗略地补偿对称性)</p>
<hr>
<hr>
<h3 id="segment-anything">Segment Anything<a hidden class="anchor" aria-hidden="true" href="#segment-anything">#</a></h3>
<p><em>即时分割</em></p>
<p><em>模型组件</em></p>
<p><img alt="image-20240615220259186" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fo1ZZ7uXEYg4FF6zhXP22Wb00GRI"></p>
<p>模型框架：</p>
<p><img alt="image-20240615220352268" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fr4-lV5r1l6wvGHaV5X7ttwK-v-B"></p>
<p><em>prompt encoder：</em></p>
<ul>
<li>Sparse prompts:</li>
<li>point: point =&gt; 256 dimensional vectorial embedding. 这个使用index去索引位置嵌入像Swin-T， foreground or backgroud embedding（自学习）. to add together.</li>
<li>box: 左上角位置编码 + 左上角的学习嵌入；左上角位置编码+“右下角”的学习嵌入</li>
<li>text: clip的text encoder.</li>
<li>dense prompts:
<ul>
<li>mask: CNN =&gt; 256 特征向量。有则加mask，没有就加可学习的表示无mask的学习嵌入</li>
</ul>
</li>
</ul>
<p><em>mask decoder：</em></p>
<p><img alt="image-20240615220440257" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FjIJbCLSSY9RpRdpfUBwRHKDZ11H"></p>
<hr>
<h3 id="knnk-nearest-neighbors">KNN(K-Nearest Neighbors)<a hidden class="anchor" aria-hidden="true" href="#knnk-nearest-neighbors">#</a></h3>
<p><em>K-近邻算法</em></p>
<p>核心思想：<strong>相似的样本具有相似的输出</strong>。</p>
<p>=&gt; KNN通过计算输入样本与训练数据集中所有样本的距离，找到距离最近的K个样本，然后根据这些样本的类别来决定输入样本的类别</p>
<p>主要步骤:</p>
<ul>
<li>
<p><strong>选择K值</strong>：选择一个正整数K，代表你要比较的邻居数量。</p>
</li>
<li>
<p><strong>计算距离</strong>：对每个待分类样本，计算它与训练数据集中所有样本的距离。常用的距离度量有欧氏距离、曼哈顿距离和余弦相似度等。</p>
<ul>
<li>$$
\textbf{欧氏距离}: d(x, x_i) = \sqrt{\sum_{j=1}^{m}(x_j-x_{ij})^2}
$$</li>
</ul>
</li>
<li>
<p><strong>选择最近的K个邻居</strong>：根据计算得到的距离，从训练数据集中选择距离待分类样本最近的K个样本</p>
</li>
<li>
<p><strong>投票或加权</strong>：在分类任务中，K个邻居中最多的类别即为待分类样本的预测类别。在回归任务中，可以对K个邻居的数值进行平均或者加权平均。</p>
</li>
<li>
<p><strong>输出结果</strong>：输出投票或加权后的结果作为待分类样本的预测结果。</p>
</li>
</ul>
<hr>
<h3 id="patchtst---iclr-2023">PatchTST - ICLR 2023<a hidden class="anchor" aria-hidden="true" href="#patchtst---iclr-2023">#</a></h3>
<ol>
<li>
<p>Patchify .</p>
<ol>
<li>有监督 =&gt; 可以重叠</li>
<li>自监督 =&gt; 不可以重叠，避免网络可以从重叠区域走捷径学习</li>
</ol>
</li>
<li>
<p>多变量独立：</p>
<ol>
<li>
<p>每个时间序列将有自己的潜在表示，通过共享权重机制交叉学习 ？？？</p>
<p>共享Encoder权重，不同通道使用相同的模型参数。这种方法允许模型在不同的任务之间共享知识</p>
</li>
</ol>
</li>
</ol>
<p><strong>Overview</strong></p>
<p><img alt="image-20240801210057622" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fq0nT1xy9-5ka1qxWndHierUZxTr"></p>
<p><img alt="image-20240801205504650" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FoubmWQSlQXo0ibNAHq6LdsmBlGT"></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln">1</span><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">length</span><span class="p">]</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">num_token</span><span class="p">,</span> <span class="n">len_token</span><span class="p">]</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="o">*</span><span class="n">channel</span><span class="p">,</span> <span class="n">num_sample</span><span class="p">,</span> <span class="n">dim_hidden</span><span class="p">]</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl">
</span></span><span class="line"><span class="ln">5</span><span class="cl"><span class="o">=&gt;</span> <span class="n">Transformer</span> <span class="n">Encoder</span> <span class="n">but</span> <span class="n">residual</span> <span class="n">attn</span> <span class="c1"># </span>
</span></span><span class="line"><span class="ln">6</span><span class="cl">
</span></span><span class="line"><span class="ln">7</span><span class="cl"><span class="o">=&gt;</span> <span class="n">Linear</span> <span class="n">head</span> <span class="o">=&gt;</span>
</span></span></code></pre></div><hr>
<h3 id="crossformer---iclr-2023">CrossFormer - ICLR 2023<a hidden class="anchor" aria-hidden="true" href="#crossformer---iclr-2023">#</a></h3>
<p>TRANSFORMER UTILIZING CROSSDIMENSION DEPENDENCY FOR MULTIVARIATE TIME SERIES FORECASTING</p>
<p>创新点：</p>
<ul>
<li>显示建模时间依赖关系 + 通道依赖关系</li>
<li>两阶段注意力 （时间：MHSA，通道：Router MHSA）</li>
</ul>
<p>嵌入方式 and 依据：</p>
<p><img alt="image-20240802201801644" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FmcGiYT9kC7WvvSf5RRFZuugHNPj"></p>
<p>自注意力呈现小局部一致性，一坨而不是一个。</p>
<ul>
<li>保持通道独立 ✔️</li>
<li>注意力优化     ✔️</li>
</ul>
<p><em><strong>TwoStageAttention</strong></em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="c1"># Step 1. Time Dependency</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">length</span><span class="p">]</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">num_patch</span><span class="p">,</span> <span class="n">dim_patch</span><span class="p">]</span> <span class="c1"># &lt;= DSW (Dimension-Segment-Wise)</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="o">*</span><span class="n">channel</span><span class="p">,</span> <span class="n">num_patch</span><span class="p">,</span> <span class="n">dim_patch</span><span class="p">]</span>  
</span></span><span class="line"><span class="ln"> 5</span><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">TransformerEncoer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># &lt;= capture time dependency</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl">
</span></span><span class="line"><span class="ln"> 7</span><span class="cl"><span class="c1"># Step 2. Channel Dependency</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">num_patch</span><span class="p">,</span> <span class="n">dim_patch</span><span class="p">]</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="o">*</span><span class="n">num_patch</span><span class="p">,</span> <span class="n">channel</span><span class="p">,</span> <span class="n">dim_patch</span><span class="p">]</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl"><span class="n">router</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_router</span><span class="p">,</span> <span class="n">dim_patch</span><span class="p">]</span> <span class="c1"># &lt;= router</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl"><span class="n">router</span> <span class="o">=&gt;</span> <span class="n">repeat</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="o">*</span><span class="p">,</span> <span class="n">num_router</span><span class="p">,</span> <span class="n">dim_patch</span><span class="p">]</span>
</span></span><span class="line"><span class="ln">12</span><span class="cl"><span class="n">z</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">router</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># &lt;= capture channel dependency   </span>
</span></span><span class="line"><span class="ln">13</span><span class="cl"><span class="n">z</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">router</span><span class="p">,</span> <span class="n">router</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">14</span><span class="cl">
</span></span><span class="line"><span class="ln">15</span><span class="cl"><span class="c1"># save per stage output</span>
</span></span><span class="line"><span class="ln">16</span><span class="cl"><span class="o">=&gt;</span> <span class="n">Unet</span> <span class="n">Decoder</span> <span class="o">=&gt;</span> <span class="n">to</span> <span class="n">predict</span>
</span></span></code></pre></div><p>ECG与多变量的异同：</p>
<ul>
<li>相似点
<ul>
<li>不同通道贡献不同 =&gt; DSW-patch, 能够更加细粒度编码局部波形  == 多变量(通道)</li>
<li>patch化的成功！！！</li>
</ul>
</li>
<li>不同点
<ul>
<li>由于是对心脏电活动的同一时间不同角度的观察 =&gt; 病理位置相同 =&gt;是否能够通过<strong>共享</strong>策略 降低计算成本🤔❓</li>
</ul>
</li>
</ul>
<hr>
<h3 id="informer---aaai-2021-best">Informer - AAAI 2021 Best<a hidden class="anchor" aria-hidden="true" href="#informer---aaai-2021-best">#</a></h3>
<p>&ndash; TopK-Q</p>
<p>观察：(Q&amp;K 是等价的)</p>
<p><img alt="image-20240803141041758" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FsuQLyF7-jRZhDX_VBbBjyVuOJYE"></p>
<p><img alt="image-20240803141402493" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FrWzYw4K5yiCpTHywdjy8lbdsTJc"></p>
<p>注意力呈现长尾分布：</p>
<p>Query分为活跃于惰性Token</p>
<p>衡量指标：</p>
<p><img alt="image-20240803141306326" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FnyVbctz6tBWyu_6v-9eFxtgAmsL"></p>
<p>注意力优化：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"><span class="c1"># probe</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"><span class="n">K</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_head</span><span class="p">,</span> <span class="n">len_token</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">]</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"><span class="n">K_sample</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_head</span><span class="p">,</span> <span class="n">random_len</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">]</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl"><span class="n">Q_K_sample</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_head</span><span class="p">,</span> <span class="n">len_token</span><span class="p">,</span> <span class="n">random_len</span><span class="p">]</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl"><span class="n">M</span> <span class="o">=</span> <span class="n">Q_K_sample</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">Q_K_sample</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">L_K</span><span class="p">)</span>  <span class="c1"># 衡量指标</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl"><span class="n">M_top</span> <span class="o">=</span> <span class="n">M</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">n_top</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">
</span></span><span class="line"><span class="ln"> 9</span><span class="cl"><span class="n">Q_reduce</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">M_top</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl"><span class="n">attn_active</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q_reduce</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">*</span><span class="n">scale</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl">
</span></span><span class="line"><span class="ln">12</span><span class="cl"><span class="n">contex</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">()</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="n">num_head</span><span class="p">,</span> <span class="n">len_token</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">]</span>  <span class="c1"># 均匀分布的就直接取V的均值</span>
</span></span><span class="line"><span class="ln">13</span><span class="cl"><span class="n">contex</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">M_top</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">attn_active</span><span class="nd">@V</span>
</span></span></code></pre></div><p>结构优化：</p>
<p><img alt="image-20240803142458572" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FjJea7jjH25S7tZe7BywKypEuXlv"></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln">1</span><span class="cl"><span class="c1"># Encoder:</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="k">for</span> <span class="n">num_layer</span> <span class="o">...</span><span class="p">:</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># &lt;- maxpool(act(norm(conv())))</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl"><span class="k">return</span> <span class="n">enc_out</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln">1</span><span class="cl"><span class="c1"># Decoder:</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="n">cross</span> <span class="o">=</span> <span class="n">enc_out</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="c1"># 预测引导</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl"><span class="k">for</span> <span class="n">num_layer</span> <span class="o">...</span><span class="p">:</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cross</span><span class="p">,</span> <span class="n">x_mask</span><span class="o">=</span><span class="n">x_mask</span><span class="p">,</span> <span class="n">cross_mask</span><span class="o">=</span><span class="n">cross_mask</span><span class="p">)</span>  <span class="c1"># Note：Masked MHSA-ProbeAttn</span>
</span></span></code></pre></div><p>框架逻辑</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="n">Class</span> <span class="n">Exp_Basic</span><span class="p">(</span><span class="n">Object</span><span class="p">):</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">():</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl">    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">():</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl">    <span class="k">def</span> <span class="nf">_acquire_device</span><span class="p">():</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">    <span class="k">def</span> <span class="nf">_get_data</span><span class="p">():</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl">    <span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">    <span class="k">def</span> <span class="nf">vali</span><span class="p">():</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">    <span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">        
</span></span><span class="line"><span class="ln">10</span><span class="cl"><span class="n">Class</span> <span class="n">Exp_Model</span><span class="p">(</span><span class="n">Exp_Basic</span><span class="p">):</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl">    <span class="k">def</span> <span class="nf">_select_optimizer</span><span class="p">():</span>
</span></span><span class="line"><span class="ln">12</span><span class="cl">    <span class="k">def</span> <span class="nf">_select_criterion</span><span class="p">():</span>
</span></span><span class="line"><span class="ln">13</span><span class="cl">    <span class="k">def</span> <span class="nf">_process_on_batch</span><span class="p">():</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln">1</span><span class="cl"><span class="c1"># data_loader.py</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="n">Class</span> <span class="n">Dataset_XXX</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">():</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl">    <span class="k">def</span> <span class="nf">__read_data__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="ln">5</span><span class="cl">    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
</span></span><span class="line"><span class="ln">6</span><span class="cl">    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="c1"># main.py</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl">
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"><span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;[Model] Task&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"><span class="o">...</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl"><span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl"><span class="n">setting</span> <span class="o">=</span> <span class="o">...</span><span class="n">args</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">
</span></span><span class="line"><span class="ln"> 8</span><span class="cl"><span class="n">exp</span> <span class="o">=</span> <span class="n">Exp_Model</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl"><span class="n">exp</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">setting</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl"><span class="n">exp</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">setting</span><span class="p">)</span>
</span></span></code></pre></div><img src="http://sthda9dn6.hd-bkt.clouddn.com/Fr4gDWDQ5ks4NCuXHRCQ-UOBW3f6" alt="image-20240817204741361" style="zoom: 50%;" />
<hr>
<h3 id="adaptive-token-dictionary----cvpr2024">Adaptive Token Dictionary  - CVPR2024<a hidden class="anchor" aria-hidden="true" href="#adaptive-token-dictionary----cvpr2024">#</a></h3>
<p>Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary</p>
<p>扩展局部窗口的限制</p>
<p><img alt="image-20240919153745713" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FvL2v53zt2HNJxfaf6DoqKEZxUQv"></p>
<ol>
<li>window-based self-attention</li>
<li>token dictionary cross-attention =&gt; Attention(Q(XW),K(TW), V(TW))</li>
<li>基于2的Attn，将token map排序分group(类)，进行group内部的Attention</li>
</ol>
<p><img alt="image-20240919154126696" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fm6YWCM2vD3r3UYGENn7woDqo_2r"></p>
<p><strong>Architecture</strong></p>
<p><img alt="image-20240919154258809" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FsliGkn-Ia9KlymRQrFUZZaff_H5"></p>
<hr>
<h3 id="poly-kernel-inception----cvpr-2024">Poly Kernel Inception  - CVPR 2024<a hidden class="anchor" aria-hidden="true" href="#poly-kernel-inception----cvpr-2024">#</a></h3>
<p>Poly Kernel Inception Network for Remote Sensing Detection</p>
<p>遥感图像中的目标检测面临着多种挑战，包括<strong>目标尺度变化大</strong>、测距环境多样等。现有的方法试图通过大核卷积或扩张卷积来扩展脊柱的空间感受野来解决这些挑战。然而，前者通常会<strong>引入相当大的背景噪声</strong>，而后者则有生成<strong>过度稀疏的特征表示</strong>的风险。本文提出了一种多核初始化网络（PKINet）来解决上述问题。PKINet采用无膨胀的多尺度卷积核来提取<strong>不同尺度</strong>的对象特征并捕获<strong>局部上下文</strong>。此外，一个上下文锚注意（CAA）模块并行引入捕获远程上下文信息。</p>
<ol>
<li><strong>不同尺度-局部上下文</strong></li>
<li><strong>并行引入捕获远程上下文信息</strong></li>
</ol>
<p><img alt="image-20240919154538153" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FjXAvi5BOhnK1vn-qVoFbI57Ajvo"></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln">1</span><span class="cl"><span class="c1"># 十字架型汇聚 =&gt; 近似标准的DWConvKxK =&gt; 降低参数量</span>
</span></span><span class="line"><span class="ln">2</span><span class="cl"><span class="n">agg</span> <span class="o">=</span> <span class="n">Conv1x1</span><span class="p">(</span><span class="n">AvgPool</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
</span></span><span class="line"><span class="ln">3</span><span class="cl"><span class="n">agg</span> <span class="o">=</span> <span class="n">Conv1x1</span><span class="p">(</span><span class="n">DWConvKx1</span><span class="p">(</span><span class="n">DWConv1xK</span><span class="p">(</span><span class="n">agg</span><span class="p">)))</span>
</span></span><span class="line"><span class="ln">4</span><span class="cl"><span class="n">attn</span> <span class="o">=</span> <span class="n">Sigmoid</span><span class="p">(</span><span class="n">agg</span><span class="p">)</span> 
</span></span></code></pre></div><hr>
<h3 id="danet----cvpr-2019">DANet  - CVPR 2019<a hidden class="anchor" aria-hidden="true" href="#danet----cvpr-2019">#</a></h3>
<p>Dual Attention Network for Scene Segmentation</p>
<p><img alt="image-20240925140803581" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fv2w3idnvTYq2ouUvrQMtxqlfucH"></p>
<p>At the end of the model, we use the dual attention mechanism to explicitly capture position and channel dependencies.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="k">class</span> <span class="nc">PAM_Module</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl">    <span class="s2">&#34;&#34;&#34; Position attention module&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl">    <span class="c1">#Ref from SAGAN</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">):</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">PAM_Module</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">chanel_in</span> <span class="o">=</span> <span class="n">in_dim</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">query_conv</span> <span class="o">=</span> <span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_dim</span><span class="o">//</span><span class="mi">8</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">key_conv</span> <span class="o">=</span> <span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_dim</span><span class="o">//</span><span class="mi">8</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">value_conv</span> <span class="o">=</span> <span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="o">=</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="ln">12</span><span class="cl">
</span></span><span class="line"><span class="ln">13</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">14</span><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="ln">15</span><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="ln">16</span><span class="cl"><span class="s2">            inputs :
</span></span></span><span class="line"><span class="ln">17</span><span class="cl"><span class="s2">                x : input feature maps( B X C X H X W)
</span></span></span><span class="line"><span class="ln">18</span><span class="cl"><span class="s2">            returns :
</span></span></span><span class="line"><span class="ln">19</span><span class="cl"><span class="s2">                out : attention value + input feature
</span></span></span><span class="line"><span class="ln">20</span><span class="cl"><span class="s2">                attention: B X (HxW) X (HxW)
</span></span></span><span class="line"><span class="ln">21</span><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="ln">22</span><span class="cl">        <span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="ln">23</span><span class="cl">        <span class="n">proj_query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span><span class="o">*</span><span class="n">height</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">24</span><span class="cl">        <span class="n">proj_key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">key_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span><span class="o">*</span><span class="n">height</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">25</span><span class="cl">        <span class="n">energy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">proj_query</span><span class="p">,</span> <span class="n">proj_key</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">26</span><span class="cl">        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">energy</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">27</span><span class="cl">        <span class="n">proj_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">value_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">width</span><span class="o">*</span><span class="n">height</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">28</span><span class="cl">
</span></span><span class="line"><span class="ln">29</span><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">proj_value</span><span class="p">,</span> <span class="n">attention</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="ln">30</span><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">31</span><span class="cl">
</span></span><span class="line"><span class="ln">32</span><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">out</span> <span class="o">+</span> <span class="n">x</span>
</span></span><span class="line"><span class="ln">33</span><span class="cl">        <span class="k">return</span> <span class="n">out</span>
</span></span><span class="line"><span class="ln">34</span><span class="cl">
</span></span><span class="line"><span class="ln">35</span><span class="cl">
</span></span><span class="line"><span class="ln">36</span><span class="cl"><span class="k">class</span> <span class="nc">CAM_Module</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="ln">37</span><span class="cl">    <span class="s2">&#34;&#34;&#34; Channel attention module&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="ln">38</span><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">):</span>
</span></span><span class="line"><span class="ln">39</span><span class="cl">        <span class="nb">super</span><span class="p">(</span><span class="n">CAM_Module</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="ln">40</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">chanel_in</span> <span class="o">=</span> <span class="n">in_dim</span>
</span></span><span class="line"><span class="ln">41</span><span class="cl">
</span></span><span class="line"><span class="ln">42</span><span class="cl">
</span></span><span class="line"><span class="ln">43</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="ln">44</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span>  <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">45</span><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="ln">46</span><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="ln">47</span><span class="cl"><span class="s2">            inputs :
</span></span></span><span class="line"><span class="ln">48</span><span class="cl"><span class="s2">                x : input feature maps( B X C X H X W)
</span></span></span><span class="line"><span class="ln">49</span><span class="cl"><span class="s2">            returns :
</span></span></span><span class="line"><span class="ln">50</span><span class="cl"><span class="s2">                out : attention value + input feature
</span></span></span><span class="line"><span class="ln">51</span><span class="cl"><span class="s2">                attention: B X C X C
</span></span></span><span class="line"><span class="ln">52</span><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="ln">53</span><span class="cl">        <span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="ln">54</span><span class="cl">        <span class="n">proj_query</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">55</span><span class="cl">        <span class="n">proj_key</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">56</span><span class="cl">        <span class="n">energy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">proj_query</span><span class="p">,</span> <span class="n">proj_key</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">57</span><span class="cl">        <span class="n">energy_new</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">energy</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">energy</span><span class="p">)</span><span class="o">-</span><span class="n">energy</span>
</span></span><span class="line"><span class="ln">58</span><span class="cl">        <span class="n">attention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">energy_new</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">59</span><span class="cl">        <span class="n">proj_value</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">60</span><span class="cl">
</span></span><span class="line"><span class="ln">61</span><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attention</span><span class="p">,</span> <span class="n">proj_value</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">62</span><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">m_batchsize</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">63</span><span class="cl">
</span></span><span class="line"><span class="ln">64</span><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">*</span><span class="n">out</span> <span class="o">+</span> <span class="n">x</span>
</span></span><span class="line"><span class="ln">65</span><span class="cl">        <span class="k">return</span> <span class="n">out</span>
</span></span></code></pre></div><hr>
<h3 id="mixnet">MixNet<a hidden class="anchor" aria-hidden="true" href="#mixnet">#</a></h3>
<p><em>MixConv: Mixed Depthwise Convolutional Kernels</em></p>
<p><img alt="image-20240925145109345" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FtRY7ZtihlPgnE0gXKTYRzWioi9l"></p>
<p>convulution =&gt; capture local pattern</p>
<ul>
<li>early stages: edges</li>
<li>later stages: objects⭐</li>
</ul>
<p>这项研究表明了单个内核大小的局限性：我们既需要大内核来捕获高分辨率模式，也需要小内核来捕获低分辨率模式，以获得更好的模型精度和效率</p>
<p>在单一机制下实现多种效果，进行增强</p>
<hr>
<h3 id="multimodal-learning">Multimodal Learning<a hidden class="anchor" aria-hidden="true" href="#multimodal-learning">#</a></h3>
<p>Multimodal Learning With Transformers: A Survey</p>
<p><strong>融合策略</strong></p>
<p><img alt="image-20240925151957579" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fpkl7hX0gQn4hTn-9pCn7v0eSDsx"></p>
<hr>
<h3 id="dual-aggregation-transformer">Dual Aggregation Transformer<a hidden class="anchor" aria-hidden="true" href="#dual-aggregation-transformer">#</a></h3>
<p>Dual Aggregation Transformer for Image Super-Resolution</p>
<p>Motivation: 现有方法利用自我注意沿着不同的维度，空间或通道，并取得了令人印象深刻的性能。这启发我们将Transformer中的两个维度结合起来，以获得更强大的表示能力。</p>
<p><img alt="image-20241004172905131" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FkgqHcOiU-RhuZ2OuevAk2UbTecu"></p>
<p><img alt="image-20241004172825801" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/Fr9yfe5d0MlT-gv6julT2CWe0wme"></p>
<hr>
<h3 id="dual-vision-transformer">Dual Vision Transformer<a hidden class="anchor" aria-hidden="true" href="#dual-vision-transformer">#</a></h3>
<p>研究全局语义和更精细的像素级特征之间的依赖关系 =&gt; pixel-level token  &amp;  semantic token</p>
<p>分解和集成的全局语义和本地功能</p>
<p><img alt="image-20250114144508261" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FioBD2_JVUt1b_3OHzNhyt1y4Qyx"></p>
<p><img alt="image-20241030220700886" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FuAjYWuMPbxeo1UKHwCopbn3dyf8"></p>
<hr>
<h3 id="fish-speech-tech-report">Fish-Speech Tech-report<a hidden class="anchor" aria-hidden="true" href="#fish-speech-tech-report">#</a></h3>
<p>Text-to-Speech End2End Model</p>
<p><strong>两阶段训练策略：</strong></p>
<img src="http://sthda9dn6.hd-bkt.clouddn.com/Fl0WmE6EneKMjE67yM20TWC7qsrz" alt="image-20250215131815505" style="zoom: 50%;" />
<p><em>Stage 1:</em></p>
<pre><code>Audio:Mel Spectrogram =&gt; 【Encoder】 =&gt; Embedding =&gt; Quantize Tokens =&gt; 【⭐Decoder⭐】=&gt; Audio

**⭐重构目标⭐**
</code></pre>
<p><em>Stage 2:</em></p>
<pre><code>Text:Quantize Tokens =&gt; 【✨AR Model✨】=&gt; Quantize Tokens  

⭐**Text:Audio一致性 + 自回归预测Next**⭐
</code></pre>
<p><em>Inference:</em></p>
<pre><code>Text:Prompt-Tokens =&gt; 【✨AR Model✨】=&gt; Quantize Tokens  =&gt; 【⭐Decoder⭐】=&gt; Audio
</code></pre>
<p><em>Vector Quantize Tech:</em></p>
<p>Example：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="n">有一组连续的温度数据</span><span class="err">（</span><span class="n">如</span> <span class="mf">20.3</span><span class="err">°</span><span class="n">C</span><span class="p">,</span> <span class="mf">21.7</span><span class="err">°</span><span class="n">C</span><span class="p">,</span> <span class="mf">22.5</span><span class="err">°</span><span class="n">C</span><span class="p">,</span> <span class="mf">19.8</span><span class="err">°</span><span class="n">C</span><span class="err">），</span><span class="n">你想将其离散化为几个类别</span><span class="p">:</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"><span class="mf">1.</span><span class="n">低温</span><span class="p">:</span> <span class="mi">15</span><span class="err">°</span><span class="n">C</span> <span class="o">-</span> <span class="mi">20</span><span class="err">°</span><span class="n">C</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"><span class="mf">2.</span><span class="n">中温</span><span class="p">:</span> <span class="mi">20</span><span class="err">°</span><span class="n">C</span> <span class="o">-</span> <span class="mi">25</span><span class="err">°</span><span class="n">C</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"><span class="mf">3.</span><span class="n">高温</span><span class="p">:</span> <span class="mi">25</span><span class="err">°</span><span class="n">C</span> <span class="o">-</span> <span class="mi">30</span><span class="err">°</span><span class="n">C</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl"><span class="o">=&gt;</span>
</span></span><span class="line"><span class="ln"> 6</span><span class="cl"><span class="mf">20.3</span><span class="err">°</span><span class="n">C</span> <span class="err">→</span> <span class="n">中温</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl"><span class="mf">21.7</span><span class="err">°</span><span class="n">C</span> <span class="err">→</span> <span class="n">中温</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl"><span class="mf">22.5</span><span class="err">°</span><span class="n">C</span> <span class="err">→</span> <span class="n">中温</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl"><span class="mf">19.8</span><span class="err">°</span><span class="n">C</span> <span class="err">→</span> <span class="n">低温</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl">
</span></span><span class="line"><span class="ln">11</span><span class="cl"><span class="n">假设编码本有</span> <span class="mi">512</span> <span class="n">个向量</span> <span class="n">Shape</span><span class="p">:[</span><span class="mi">512</span><span class="p">,</span> <span class="n">dim</span><span class="p">]</span>
</span></span><span class="line"><span class="ln">12</span><span class="cl"><span class="n">Encoder得到的Embedding</span> <span class="n">Shape</span><span class="p">:[</span><span class="n">T</span><span class="p">,</span> <span class="n">dim</span><span class="p">]</span>
</span></span><span class="line"><span class="ln">13</span><span class="cl"><span class="n">对于</span> <span class="n">Encoder</span> <span class="n">输出的每个时间步的特征向量</span><span class="err">，</span><span class="n">VQ</span> <span class="n">会找到编码本中与之最接近的向量</span><span class="err">，</span><span class="n">并用其索引表示</span><span class="err">。</span>
</span></span><span class="line"><span class="ln">14</span><span class="cl"><span class="n">最终输出是一个离散的索引序列</span><span class="err">，</span><span class="n">例如</span> <span class="p">[</span><span class="mi">42</span><span class="p">,</span> <span class="mi">123</span><span class="p">,</span> <span class="mi">87</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span><span class="err">，</span><span class="n">每个索引对应编码本中的一个向量</span><span class="err">。</span>
</span></span><span class="line"><span class="ln">15</span><span class="cl">
</span></span><span class="line"><span class="ln">16</span><span class="cl"><span class="n">编码本随机初始化</span><span class="err">，</span><span class="n">在训练过程中</span><span class="err">，</span><span class="n">编码本会通过梯度下降和优化算法</span><span class="err">（</span><span class="n">如</span> <span class="n">Adam</span><span class="err">）</span><span class="n">不断更新</span><span class="err">：</span>
</span></span><span class="line"><span class="ln">17</span><span class="cl"><span class="n">最近邻搜索</span> <span class="o">=&gt;</span> <span class="n">量化误差计算</span> <span class="o">=&gt;</span> <span class="n">梯度更新</span><span class="p">,</span><span class="n">BP</span>
</span></span><span class="line"><span class="ln">18</span><span class="cl"><span class="n">编码本的作用</span> <span class="o">=&gt;</span> <span class="n">降维与压缩</span> <span class="o">+</span> <span class="n">离散化表示</span> <span class="o">+</span> <span class="n">提升生成质量</span><span class="p">(</span><span class="n">通过离散化减少生成过程中的模糊性</span><span class="err">，</span><span class="n">提升生成语音的自然度s</span><span class="p">)</span>
</span></span></code></pre></div><hr>
<h3 id="blip">Blip<a hidden class="anchor" aria-hidden="true" href="#blip">#</a></h3>
<ul>
<li>Image-2-Text 任务之一</li>
</ul>
<p>Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation Architecture</p>
<img src="http://sthda9dn6.hd-bkt.clouddn.com/FpPP6kQ3PE7BEtpByp9YYxMRML6v" alt="image-20250215133226745" style="zoom:50%;" />
<p>相同颜色共享参数</p>
<p>Stage 1：</p>
<p>Image =&gt; 【Image Encoder:ViT】 =&gt; image Embedding</p>
<p>Text =&gt; 【Text Encoder:Bert】 =&gt; text Embedding</p>
<p>目标：图文一致性, 训练Encoder</p>
<p>Stage 2：</p>
<p>Image =&gt; 【Image Encoder:ViT:Freeze🥶】 =&gt; image Embedding</p>
<p>Text =&gt; 【Text Encoder:Bert:Freeze🥶 + Cross-Attention:image Embedding🥵】 =&gt; Linear:2class</p>
<p>目标：图文是否匹配-2分类, 训练Cross-Attention部分</p>
<p>Stage 3:</p>
<p>Image =&gt; 【Image Encoder:ViT:Freeze🥶】 =&gt; image Embedding</p>
<p>Text =&gt; 【Text Decoder:GPT🥵 + Cross-Attention:image Embedding:Freeze🥶】 =&gt; Linear:multi-class</p>
<p>目标：Image Embedding + text 自回归预测Next</p>
<p>Inference</p>
<p>Image =&gt; 【⭐Image Encoder⭐】 =&gt; image Embedding</p>
<p>Prompt-Text =&gt; 【⭐Text Decoder:GPT⭐ + Cross-Attention:image Embedding:⭐】 =&gt; Linear:multi-class  =&gt; Next-Token</p>
<p>实现Image =&gt; Text</p>
<hr>
<h3 id="bevformer">BEVFormer<a hidden class="anchor" aria-hidden="true" href="#bevformer">#</a></h3>
<p>BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</p>
<p>网络架构信息流思路：</p>
<p><img alt="image-20250310223908226" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FhrytoVkAZHPxv74ssV8XYRfxwe-"></p>
<p>具体：</p>
<p><img alt="image-20250310223507001" loading="lazy" src="http://sthda9dn6.hd-bkt.clouddn.com/FiHzv4f6t8ftO97JsQ2A8Kqg4ad_"></p>
<ul>
<li>一组可学习的BEV Queries，二维网格，模拟鸟瞰图；</li>
<li>Spatial Cross Attention，每个视图经过backone提取，拿其中多个层级的输出，拼接为多尺度特征(多个层的特征图，校准通道)。然后每个位置的q，只查询对应几个视图的周边几个k；</li>
<li>Temporal Attention，t时刻的BEV中的q，查询t-1时刻，相应位置周边的几个k。 这里的t-1时刻的BEV特征，需要通过一个角度还是啥校准空间对齐；</li>
</ul>
<hr>
<h3 id="deformable-detr">Deformable DETR<a hidden class="anchor" aria-hidden="true" href="#deformable-detr">#</a></h3>
<p>architecture figure：</p>
<img src="http://sthda9dn6.hd-bkt.clouddn.com/FsCz6WFOAG-lyH--baQzssLcXHQl" alt="image-20250310230354425" style="zoom:50%;" />
<p>Attn figure：</p>
<img src="http://sthda9dn6.hd-bkt.clouddn.com/Fj3SPOZCjcvybFaLv1hoUNeGlail" alt="image-20250310230305133" style="zoom: 50%;" />
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="ln"> 1</span><span class="cl"><span class="c1"># 伪代码 - 单尺度的</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl">
</span></span><span class="line"><span class="ln"> 6</span><span class="cl"><span class="k">class</span> <span class="nc">DeformableAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_points</span><span class="p">):</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="ln"> 9</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span></span><span class="line"><span class="ln">11</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_points</span> <span class="o">=</span> <span class="n">num_points</span>
</span></span><span class="line"><span class="ln">12</span><span class="cl">
</span></span><span class="line"><span class="ln">13</span><span class="cl">        <span class="c1"># 用于预测采样偏移的线性层</span>
</span></span><span class="line"><span class="ln">14</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">offset_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">num_points</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">15</span><span class="cl">
</span></span><span class="line"><span class="ln">16</span><span class="cl">        <span class="c1"># 用于计算注意力权重的线性层</span>
</span></span><span class="line"><span class="ln">17</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">attn_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">num_points</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">18</span><span class="cl">
</span></span><span class="line"><span class="ln">19</span><span class="cl">        <span class="c1"># 输出投影层</span>
</span></span><span class="line"><span class="ln">20</span><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">21</span><span class="cl">	
</span></span><span class="line"><span class="ln">22</span><span class="cl">    <span class="c1"># reference_points, 每个采样点初始，共用同一个reference_point，靠offset进行局部位置偏移</span>
</span></span><span class="line"><span class="ln">23</span><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">reference_points</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
</span></span><span class="line"><span class="ln">24</span><span class="cl">        <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="ln">25</span><span class="cl">        <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
</span></span><span class="line"><span class="ln">26</span><span class="cl">
</span></span><span class="line"><span class="ln">27</span><span class="cl">        <span class="c1"># 预测采样偏移</span>
</span></span><span class="line"><span class="ln">28</span><span class="cl">        <span class="n">offsets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">offset_proj</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">29</span><span class="cl">
</span></span><span class="line"><span class="ln">30</span><span class="cl">        <span class="c1"># 生成采样点</span>
</span></span><span class="line"><span class="ln">31</span><span class="cl">        <span class="n">sampling_points</span> <span class="o">=</span> <span class="n">reference_points</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">offsets</span>
</span></span><span class="line"><span class="ln">32</span><span class="cl">
</span></span><span class="line"><span class="ln">33</span><span class="cl">        <span class="c1"># 双线性插值采样特征值</span>
</span></span><span class="line"><span class="ln">34</span><span class="cl">        <span class="n">sampled_value</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">grid_sample</span><span class="p">(</span>
</span></span><span class="line"><span class="ln">35</span><span class="cl">            <span class="n">value</span><span class="p">,</span>
</span></span><span class="line"><span class="ln">36</span><span class="cl">            <span class="n">sampling_points</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
</span></span><span class="line"><span class="ln">37</span><span class="cl">            <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="ln">38</span><span class="cl">            <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span>
</span></span><span class="line"><span class="ln">39</span><span class="cl">        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">40</span><span class="cl">
</span></span><span class="line"><span class="ln">41</span><span class="cl">        <span class="c1"># 计算注意力权重</span>
</span></span><span class="line"><span class="ln">42</span><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_proj</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_points</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">43</span><span class="cl">        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">44</span><span class="cl">
</span></span><span class="line"><span class="ln">45</span><span class="cl">        <span class="c1"># 加权求和</span>
</span></span><span class="line"><span class="ln">46</span><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">sampled_value</span> <span class="o">*</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">47</span><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">48</span><span class="cl">
</span></span><span class="line"><span class="ln">49</span><span class="cl">        <span class="c1"># 输出投影</span>
</span></span><span class="line"><span class="ln">50</span><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">51</span><span class="cl">
</span></span><span class="line"><span class="ln">52</span><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://121.40.252.207/tags/dl/">DL</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://121.40.252.207/posts/learning/paper_reading1/">
    <span class="title">« Prev</span>
    <br>
    <span>深度学习论文汇总1</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://121.40.252.207/">LongCoding&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
